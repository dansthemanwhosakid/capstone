{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim = NLP\n",
    "\n",
    "processes texts, work with word vector models (Word2Vec, FastText), and topic models\n",
    "\n",
    "topic modeling - techinque to extract topics from text\n",
    "\n",
    "provides LDA and LSI\n",
    "\n",
    "```\n",
    "Token\n",
    "- word\n",
    "\n",
    "Document \n",
    "- sentence or paragraph\n",
    "\n",
    "Corpus\n",
    "- collection of documents as a bag of words\n",
    "- contains each word's id and its frequency count in that document (If countvec)\n",
    "\n",
    "Dictionary\n",
    "- maps each word to a unique id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Saudis',\n",
       " 'are',\n",
       " 'preparing',\n",
       " 'a',\n",
       " 'report',\n",
       " 'that',\n",
       " 'will',\n",
       " 'acknowledge',\n",
       " 'that']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basically \n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim will use this dictionary to create a bag-of-words corpus where the words in the documents are replaced with its respective id provided by this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Saudis': 0,\n",
       " 'The': 1,\n",
       " 'a': 2,\n",
       " 'acknowledge': 3,\n",
       " 'are': 4,\n",
       " 'preparing': 5,\n",
       " 'report': 6,\n",
       " 'that': 7,\n",
       " 'will': 8,\n",
       " 'Jamal': 9,\n",
       " \"Khashoggi's\": 10,\n",
       " 'Saudi': 11,\n",
       " 'an': 12,\n",
       " 'death': 13,\n",
       " 'journalist': 14,\n",
       " 'of': 15,\n",
       " 'result': 16,\n",
       " 'the': 17,\n",
       " 'was': 18,\n",
       " 'intended': 19,\n",
       " 'interrogation': 20,\n",
       " 'lead': 21,\n",
       " 'one': 22,\n",
       " 'to': 23,\n",
       " 'went': 24,\n",
       " 'wrong,': 25,\n",
       " 'Turkey,': 26,\n",
       " 'abduction': 27,\n",
       " 'according': 28,\n",
       " 'from': 29,\n",
       " 'his': 30,\n",
       " 'sources.': 31,\n",
       " 'two': 32}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id\n",
    "# basically each unique word has an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A CORPUS (\"BAG OF WORDS\")\n",
    "# is a corpus object that contains the word id and its frequency in each document. \n",
    "# You can think of it as gensimâ€™s equivalent of a Document-Term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1)],\n",
      " [(9, 1),\n",
      "  (10, 1),\n",
      "  (11, 1),\n",
      "  (12, 1),\n",
      "  (13, 1),\n",
      "  (14, 1),\n",
      "  (15, 1),\n",
      "  (16, 1),\n",
      "  (17, 1),\n",
      "  (18, 1)],\n",
      " [(7, 2),\n",
      "  (18, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (21, 1),\n",
      "  (22, 1),\n",
      "  (23, 1),\n",
      "  (24, 1),\n",
      "  (25, 1)],\n",
      " [(23, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create the Corpus\n",
    "\n",
    "\n",
    "mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]\n",
    "pprint(mycorpus)\n",
    "\n",
    "# interpretation:\n",
    "# (0,1) means word with id = 0 appears once in the first doucment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)\n",
    "\n",
    "# The (0, 1) in line 1 means, the word with id=0 appears once in the 1st document.\n",
    "# Likewise, the (4, 4) in the second list item means the word with id 4 appears 4 times in the second document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dogs': 0, 'let': 1, 'out': 2, 'the': 3, 'who': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]\n"
     ]
    }
   ],
   "source": [
    "# frequency of words in each line\n",
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
    "pprint(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['first', 1], ['is', 1], ['line', 1], ['the', 1], ['this', 1]]\n",
      "[['is', 1], ['the', 1], ['this', 1], ['second', 1], ['sentence', 1]]\n",
      "[['this', 1], ['document', 1], ['third', 1]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"This is the first line\",\n",
    "             \"This is the second sentence\",\n",
    "             \"This third document\"]\n",
    "\n",
    "# Create the Dictionary and Corpus\n",
    "mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
    "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]\n",
    "\n",
    "# Show the Word Weights in Corpus\n",
    "for doc in corpus:\n",
    "    print([[mydict[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(models.TfidfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['first', 0.66], ['is', 0.24], ['line', 0.66], ['the', 0.24]]\n",
      "[['is', 0.24], ['the', 0.24], ['second', 0.66], ['sentence', 0.66]]\n",
      "[['document', 0.71], ['third', 0.71]]\n"
     ]
    }
   ],
   "source": [
    "# Create the TF-IDF model\n",
    "# SMART information retreival system\n",
    "# https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System\n",
    "# ntc \n",
    "# n = raw term frequency\n",
    "# t = inverse collection frequency\n",
    "# c = cosine normalization\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "# [['first', 0.66], ['is', 0.24], ['line', 0.66], ['the', 0.24]]\n",
    "# [['is', 0.24], ['the', 0.24], ['second', 0.66], ['sentence', 0.66]]\n",
    "# [['document', 0.71], ['third', 0.71]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING GENSIM downloader to load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('red', 0.8901657462120056),\n",
       " ('black', 0.8648406863212585),\n",
       " ('pink', 0.845291793346405),\n",
       " ('green', 0.8346816301345825),\n",
       " ('yellow', 0.8320707082748413),\n",
       " ('purple', 0.8293111324310303),\n",
       " ('white', 0.8225342035293579),\n",
       " ('orange', 0.8114302158355713),\n",
       " ('bright', 0.799933910369873),\n",
       " ('colored', 0.7876655459403992)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Get information about the model or dataset\n",
    "api.info('glove-wiki-gigaword-50')\n",
    "# {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
    "#  'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
    "#  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
    "#  'file_name': 'glove-wiki-gigaword-50.gz',\n",
    "#  'file_size': 69182535,\n",
    "#  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
    "#  (... truncated...)\n",
    "\n",
    "# Download\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "w2v_model.most_similar('blue')\n",
    "# [('red', 0.8901656866073608),\n",
    "#  ('black', 0.8648407459259033),\n",
    "#  ('pink', 0.8452916741371155),\n",
    "#  ('green', 0.8346816301345825),\n",
    "#  ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eating', 0.8706035614013672),\n",
       " ('ate', 0.8597103357315063),\n",
       " ('eaten', 0.8458368182182312),\n",
       " ('eats', 0.7801411747932434),\n",
       " ('meal', 0.7678447961807251),\n",
       " ('chicken', 0.7672050595283508),\n",
       " ('meat', 0.7620248198509216),\n",
       " ('cooked', 0.7545220851898193),\n",
       " ('fish', 0.7533289790153503),\n",
       " ('drink', 0.7511290907859802)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('eat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW TO CREATE TOPIC MODELS WITH LDA???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of topic models is to extract the underlying topics from a given collection of text documents. Each document in the text is considered as a combination of topics and each topic is considered as a combination of related words.\n",
    "\n",
    "\n",
    "Topic modeling can be done by algorithms like `Latent Dirichlet Allocation (LDA)` and `Latent Semantic Indexing (LSI)`\n",
    "\n",
    "\n",
    "In both cases you need to provide the number of topics as input. The topic model, in turn, will provide the topic keywords for each topic and the percentage contribution of topics in each document.\n",
    "\n",
    "\n",
    "The quality of topics is highly dependent on the quality of text processing and the number of topics you provide to the algorithm. The earlier post on how to build best topic models explains the procedure in more detail. However, I recommend understanding the basic steps involved and the interpretation in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Import packages and stopwords\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "#logging.root.setLevel(level=logging.INFO)\n",
    "stop_wordss = stopwords.words('english')\n",
    "#stop_words = stop_words + ['com', 'edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the dataset and get the text and real topic of each news article\n",
    "#dataset = api.load(\"text8\")\n",
    "\n",
    "# list of lists\n",
    "#data = [d for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined = pd.read_pickle(\"./data/combined_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = list(combined.token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing words that have a length of 1 (ie letters)\n",
    "\n",
    "tokenized_docs = [[word for word in doc if len(word) != 1] \n",
    "                  for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(89562 unique tokens: ['about', 'act', 'actually', 'advice', 'again']...)\n",
      "Dictionary(32320 unique tokens: ['about', 'act', 'actually', 'advice', 'again']...)\n"
     ]
    }
   ],
   "source": [
    "# 89562 unique tokens\n",
    "print(dictionary)\n",
    "\n",
    "\n",
    "# putting a filter on the dictionary\n",
    "# keep tokens that are at least in 5 documents\n",
    "# Keep tokens which are contained in no more than 90% of documents\n",
    "\n",
    "# lowered to 32320 tokens\n",
    "dictionary.filter_extremes(no_below = 5, no_above = 0.9)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red'>THIS IS BAG OF WORDS... BUT WE MIGHT DO doc2VEC???!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert document into the bag-of-words (BoW) format = list of (token_id, token_count) tuples.\n",
    "\n",
    "\n",
    "**Distributed Bag of Words (DBOW)**\n",
    "\n",
    "DBOW is the doc2vec model analogous to Skip-gram model in word2vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "\n",
    "\n",
    "Doc2Vec Model\n",
    "Doc to vec preverves word order in the embeddings, so \"I hate rose\" and \"I love rose\" will be treated differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the corpus: \n",
    "# The corpus is the collection of all synopses \n",
    "# pre-processed and transformed using the dictionary.\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 1), (16, 1), (21, 1), (52, 1), (69, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[1][:5])\n",
    "# vector (9,1) means that the word with id 9 appears 1 times\n",
    "# in the second character's movie lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the word with the id 9 is...\n",
    "dictionary[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf transformation - importance of word count\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 0.016118962531409778), (16, 0.12066094665980344), (21, 0.041074717287898806), (52, 0.04454961567656583), (69, 0.16023906583393127)]\n"
     ]
    }
   ],
   "source": [
    "# similar to above but now it shows each word and their\n",
    "# respective tfidf ratings\n",
    "print(corpus_tfidf[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_tfidf) =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32320"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus (iterable of iterable of (int, number)) â€“ Input corpus in the Gensim bag-of-words format.\n",
    "# num_terms (int) â€“ Number of terms in the dictionary. X-axis of the resulting matrix.\n",
    "# num_docs (int, optional) â€“ Number of documents in the corpus. If provided, a slightly more memory-efficient code path is taken. Y-axis of the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell runs forever\n",
    "numpy_matrix = gensim.matutils.corpus2dense(corpus, \n",
    "                                            num_terms= len(dictionary), \n",
    "                                            num_docs = len(corpus)\n",
    "                                           )\n",
    "\n",
    "s = np.linalg.svd(numpy_matrix, full_matrices=False, compute_uv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], tags=['0']),\n",
       " TaggedDocument(words=['i', 'love', 'coding', 'in', 'python'], tags=['1']),\n",
       " TaggedDocument(words=['i', 'love', 'building', 'chatbots'], tags=['2']),\n",
       " TaggedDocument(words=['they', 'chat', 'amagingly', 'well'], tags=['3'])]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "data = [\"I love machine learning. Its awesome.\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"they chat amagingly well\"]\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2019-05-12 17:46:09,842 : INFO : collecting all words and their counts\n",
      "2019-05-12 17:46:09,846 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-05-12 17:46:09,849 : INFO : collected 16 word types and 4 unique tags from a corpus of 4 examples and 21 words\n",
      "2019-05-12 17:46:09,853 : INFO : Loading a fresh vocabulary\n",
      "2019-05-12 17:46:09,856 : INFO : effective_min_count=1 retains 16 unique words (100% of original 16, drops 0)\n",
      "2019-05-12 17:46:09,857 : INFO : effective_min_count=1 leaves 21 word corpus (100% of original 21, drops 0)\n",
      "2019-05-12 17:46:09,874 : INFO : deleting the raw counts dictionary of 16 items\n",
      "2019-05-12 17:46:09,875 : INFO : sample=0.001 downsamples 16 most-common words\n",
      "2019-05-12 17:46:09,877 : INFO : downsampling leaves estimated 2 word corpus (13.9% of prior 21)\n",
      "2019-05-12 17:46:09,890 : INFO : estimated required memory for 16 words and 20 dimensions: 11680 bytes\n",
      "2019-05-12 17:46:09,894 : INFO : resetting layer weights\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "2019-05-12 17:46:09,920 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:09,976 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:09,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:09,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:09,984 : INFO : EPOCH - 1 : training on 21 raw words (9 effective words) took 0.0s, 739 effective words/s\n",
      "2019-05-12 17:46:10,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,009 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,010 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 1426 effective words/s\n",
      "2019-05-12 17:46:10,025 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,029 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1491 effective words/s\n",
      "2019-05-12 17:46:10,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,050 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 952 effective words/s\n",
      "2019-05-12 17:46:10,061 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,065 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,067 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 922 effective words/s\n",
      "2019-05-12 17:46:10,070 : INFO : training on a 105 raw words (37 effective words) took 0.1s, 250 effective words/s\n",
      "2019-05-12 17:46:10,071 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,073 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,090 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,096 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1041 effective words/s\n",
      "2019-05-12 17:46:10,108 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,111 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,112 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 2112 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:10,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,125 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,127 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 2173 effective words/s\n",
      "2019-05-12 17:46:10,137 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,138 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,142 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,143 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 1374 effective words/s\n",
      "2019-05-12 17:46:10,146 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,148 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,149 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1984 effective words/s\n",
      "2019-05-12 17:46:10,151 : INFO : training on a 105 raw words (37 effective words) took 0.1s, 489 effective words/s\n",
      "2019-05-12 17:46:10,153 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,154 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,162 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,164 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,166 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,167 : INFO : EPOCH - 1 : training on 21 raw words (9 effective words) took 0.0s, 1671 effective words/s\n",
      "2019-05-12 17:46:10,172 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,174 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,177 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1407 effective words/s\n",
      "2019-05-12 17:46:10,183 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,187 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1424 effective words/s\n",
      "2019-05-12 17:46:10,193 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,195 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,195 : INFO : EPOCH - 4 : training on 21 raw words (9 effective words) took 0.0s, 3663 effective words/s\n",
      "2019-05-12 17:46:10,200 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,203 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2934 effective words/s\n",
      "2019-05-12 17:46:10,203 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 764 effective words/s\n",
      "2019-05-12 17:46:10,205 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,205 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,209 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,210 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,211 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,212 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 2844 effective words/s\n",
      "2019-05-12 17:46:10,216 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,217 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,218 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2480 effective words/s\n",
      "2019-05-12 17:46:10,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,224 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,225 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,226 : INFO : EPOCH - 3 : training on 21 raw words (10 effective words) took 0.0s, 4304 effective words/s\n",
      "2019-05-12 17:46:10,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,230 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,231 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,232 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2433 effective words/s\n",
      "2019-05-12 17:46:10,236 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,237 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,238 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,239 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1995 effective words/s\n",
      "2019-05-12 17:46:10,240 : INFO : training on a 105 raw words (38 effective words) took 0.0s, 1124 effective words/s\n",
      "2019-05-12 17:46:10,241 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,241 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,249 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 1810 effective words/s\n",
      "2019-05-12 17:46:10,258 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,260 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,260 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,261 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 2125 effective words/s\n",
      "2019-05-12 17:46:10,265 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,266 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,269 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 2281 effective words/s\n",
      "2019-05-12 17:46:10,277 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,280 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1576 effective words/s\n",
      "2019-05-12 17:46:10,285 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,287 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,288 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,288 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 2942 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:10,290 : INFO : training on a 105 raw words (39 effective words) took 0.0s, 810 effective words/s\n",
      "2019-05-12 17:46:10,291 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,292 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,299 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,301 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,302 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2179 effective words/s\n",
      "2019-05-12 17:46:10,308 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,311 : INFO : EPOCH - 2 : training on 21 raw words (4 effective words) took 0.0s, 1346 effective words/s\n",
      "2019-05-12 17:46:10,319 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,320 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,321 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 3151 effective words/s\n",
      "2019-05-12 17:46:10,327 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,328 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,329 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,329 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2060 effective words/s\n",
      "2019-05-12 17:46:10,335 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,336 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,337 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,337 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 3491 effective words/s\n",
      "2019-05-12 17:46:10,338 : INFO : training on a 105 raw words (33 effective words) took 0.0s, 723 effective words/s\n",
      "2019-05-12 17:46:10,340 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,340 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,344 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,347 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 2429 effective words/s\n",
      "2019-05-12 17:46:10,350 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,351 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,352 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,353 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2257 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:10,356 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,358 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 3128 effective words/s\n",
      "2019-05-12 17:46:10,362 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,362 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,363 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,365 : INFO : EPOCH - 4 : training on 21 raw words (9 effective words) took 0.0s, 2678 effective words/s\n",
      "2019-05-12 17:46:10,368 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,370 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,371 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 2410 effective words/s\n",
      "2019-05-12 17:46:10,371 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 1217 effective words/s\n",
      "2019-05-12 17:46:10,372 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,373 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,377 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,379 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,379 : INFO : EPOCH - 1 : training on 21 raw words (11 effective words) took 0.0s, 5215 effective words/s\n",
      "2019-05-12 17:46:10,382 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,384 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 3164 effective words/s\n",
      "2019-05-12 17:46:10,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,389 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,390 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3011 effective words/s\n",
      "2019-05-12 17:46:10,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,395 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,396 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,396 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 3357 effective words/s\n",
      "2019-05-12 17:46:10,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,402 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,402 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,403 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 2403 effective words/s\n",
      "2019-05-12 17:46:10,403 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 1246 effective words/s\n",
      "2019-05-12 17:46:10,404 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,405 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,408 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,410 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 2480 effective words/s\n",
      "2019-05-12 17:46:10,413 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,414 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,415 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2765 effective words/s\n",
      "2019-05-12 17:46:10,420 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,421 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,423 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3142 effective words/s\n",
      "2019-05-12 17:46:10,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,427 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 3189 effective words/s\n",
      "2019-05-12 17:46:10,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,430 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,432 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 3510 effective words/s\n",
      "2019-05-12 17:46:10,432 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 1338 effective words/s\n",
      "2019-05-12 17:46:10,433 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,433 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,437 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,439 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,439 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 2498 effective words/s\n",
      "2019-05-12 17:46:10,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,443 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,443 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,444 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 3456 effective words/s\n",
      "2019-05-12 17:46:10,448 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,449 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,450 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2914 effective words/s\n",
      "2019-05-12 17:46:10,453 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,454 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,455 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,456 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2466 effective words/s\n",
      "2019-05-12 17:46:10,459 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:10,461 : INFO : EPOCH - 5 : training on 21 raw words (11 effective words) took 0.0s, 5045 effective words/s\n",
      "2019-05-12 17:46:10,462 : INFO : training on a 105 raw words (39 effective words) took 0.0s, 1375 effective words/s\n",
      "2019-05-12 17:46:10,463 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,464 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,468 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,469 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,470 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2653 effective words/s\n",
      "2019-05-12 17:46:10,474 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,476 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,476 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 3087 effective words/s\n",
      "2019-05-12 17:46:10,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,481 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,482 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,483 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2393 effective words/s\n",
      "2019-05-12 17:46:10,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,486 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,487 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,487 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 3068 effective words/s\n",
      "2019-05-12 17:46:10,490 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,491 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,492 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,493 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 3359 effective words/s\n",
      "2019-05-12 17:46:10,493 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 1275 effective words/s\n",
      "2019-05-12 17:46:10,494 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,495 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,503 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,506 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 2444 effective words/s\n",
      "2019-05-12 17:46:10,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,514 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,515 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 1982 effective words/s\n",
      "2019-05-12 17:46:10,523 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,524 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,525 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2330 effective words/s\n",
      "2019-05-12 17:46:10,530 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,531 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,533 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 2120 effective words/s\n",
      "2019-05-12 17:46:10,536 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,538 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,539 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2470 effective words/s\n",
      "2019-05-12 17:46:10,540 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 776 effective words/s\n",
      "2019-05-12 17:46:10,541 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,542 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,546 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,550 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1634 effective words/s\n",
      "2019-05-12 17:46:10,554 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,556 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,557 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2115 effective words/s\n",
      "2019-05-12 17:46:10,560 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,561 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,562 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,562 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 3022 effective words/s\n",
      "2019-05-12 17:46:10,569 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,570 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,571 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1800 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:10,576 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,578 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,579 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1635 effective words/s\n",
      "2019-05-12 17:46:10,580 : INFO : training on a 105 raw words (31 effective words) took 0.0s, 832 effective words/s\n",
      "2019-05-12 17:46:10,582 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,583 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,610 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,611 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,613 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,615 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1060 effective words/s\n",
      "2019-05-12 17:46:10,621 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,622 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,623 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,624 : INFO : EPOCH - 2 : training on 21 raw words (4 effective words) took 0.0s, 1014 effective words/s\n",
      "2019-05-12 17:46:10,634 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,635 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,639 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1505 effective words/s\n",
      "2019-05-12 17:46:10,645 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,650 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,650 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 995 effective words/s\n",
      "2019-05-12 17:46:10,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,659 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,661 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,662 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1582 effective words/s\n",
      "2019-05-12 17:46:10,663 : INFO : training on a 105 raw words (28 effective words) took 0.1s, 501 effective words/s\n",
      "2019-05-12 17:46:10,663 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,664 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,674 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,675 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,676 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,677 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2758 effective words/s\n",
      "2019-05-12 17:46:10,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,686 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,688 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 3241 effective words/s\n",
      "2019-05-12 17:46:10,696 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,700 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,702 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 865 effective words/s\n",
      "2019-05-12 17:46:10,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,711 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,713 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1717 effective words/s\n",
      "2019-05-12 17:46:10,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,728 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,731 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,732 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 1257 effective words/s\n",
      "2019-05-12 17:46:10,733 : INFO : training on a 105 raw words (35 effective words) took 0.1s, 537 effective words/s\n",
      "2019-05-12 17:46:10,735 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,739 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,756 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,759 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 1493 effective words/s\n",
      "2019-05-12 17:46:10,773 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,783 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 654 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:10,806 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,813 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,817 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 637 effective words/s\n",
      "2019-05-12 17:46:10,828 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,831 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,835 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,838 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 517 effective words/s\n",
      "2019-05-12 17:46:10,842 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,844 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,845 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,846 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 2452 effective words/s\n",
      "2019-05-12 17:46:10,846 : INFO : training on a 105 raw words (35 effective words) took 0.1s, 331 effective words/s\n",
      "2019-05-12 17:46:10,849 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,850 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,857 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,858 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,858 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,859 : INFO : EPOCH - 1 : training on 21 raw words (10 effective words) took 0.0s, 5524 effective words/s\n",
      "2019-05-12 17:46:10,863 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,866 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,867 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,868 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1206 effective words/s\n",
      "2019-05-12 17:46:10,873 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,875 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,875 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2865 effective words/s\n",
      "2019-05-12 17:46:10,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,879 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,880 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 2235 effective words/s\n",
      "2019-05-12 17:46:10,885 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,888 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,888 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2071 effective words/s\n",
      "2019-05-12 17:46:10,889 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 921 effective words/s\n",
      "2019-05-12 17:46:10,890 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,890 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,899 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,899 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2947 effective words/s\n",
      "2019-05-12 17:46:10,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,904 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,905 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2739 effective words/s\n",
      "2019-05-12 17:46:10,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,911 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2783 effective words/s\n",
      "2019-05-12 17:46:10,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,916 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,916 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,917 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1851 effective words/s\n",
      "2019-05-12 17:46:10,921 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,924 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,925 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,925 : INFO : EPOCH - 5 : training on 21 raw words (4 effective words) took 0.0s, 880 effective words/s\n",
      "2019-05-12 17:46:10,926 : INFO : training on a 105 raw words (31 effective words) took 0.0s, 886 effective words/s\n",
      "2019-05-12 17:46:10,927 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,928 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,931 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,932 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,933 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2994 effective words/s\n",
      "2019-05-12 17:46:10,939 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,941 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,942 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,943 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1304 effective words/s\n",
      "2019-05-12 17:46:10,946 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,947 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,948 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,949 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2621 effective words/s\n",
      "2019-05-12 17:46:10,952 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,953 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,954 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2678 effective words/s\n",
      "2019-05-12 17:46:10,958 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,959 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,959 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 2197 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:10,960 : INFO : training on a 105 raw words (30 effective words) took 0.0s, 960 effective words/s\n",
      "2019-05-12 17:46:10,961 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,961 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,964 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,964 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,966 : INFO : EPOCH - 1 : training on 21 raw words (4 effective words) took 0.0s, 1392 effective words/s\n",
      "2019-05-12 17:46:10,969 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,969 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,970 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,971 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 3873 effective words/s\n",
      "2019-05-12 17:46:10,974 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,975 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,976 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,977 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 1641 effective words/s\n",
      "2019-05-12 17:46:10,980 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,981 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,982 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,983 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2269 effective words/s\n",
      "2019-05-12 17:46:10,986 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,990 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 1165 effective words/s\n",
      "2019-05-12 17:46:10,991 : INFO : training on a 105 raw words (28 effective words) took 0.0s, 970 effective words/s\n",
      "2019-05-12 17:46:10,992 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:10,992 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:10,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:10,998 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:10,998 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:10,999 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 2526 effective words/s\n",
      "2019-05-12 17:46:11,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,006 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,006 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2430 effective words/s\n",
      "2019-05-12 17:46:11,011 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,013 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2872 effective words/s\n",
      "2019-05-12 17:46:11,017 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,018 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,019 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2109 effective words/s\n",
      "2019-05-12 17:46:11,022 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,022 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,024 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 2228 effective words/s\n",
      "2019-05-12 17:46:11,024 : INFO : training on a 105 raw words (32 effective words) took 0.0s, 1027 effective words/s\n",
      "2019-05-12 17:46:11,025 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,025 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,029 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,031 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2298 effective words/s\n",
      "2019-05-12 17:46:11,033 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,034 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,036 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 2862 effective words/s\n",
      "2019-05-12 17:46:11,039 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,040 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,040 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,041 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 2195 effective words/s\n",
      "2019-05-12 17:46:11,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,045 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,046 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,047 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 2897 effective words/s\n",
      "2019-05-12 17:46:11,049 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,050 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,051 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:11,052 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 1824 effective words/s\n",
      "2019-05-12 17:46:11,053 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 1236 effective words/s\n",
      "2019-05-12 17:46:11,054 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,055 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,057 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,059 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,060 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 1992 effective words/s\n",
      "2019-05-12 17:46:11,063 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,064 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,065 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2591 effective words/s\n",
      "2019-05-12 17:46:11,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,072 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,073 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,074 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3370 effective words/s\n",
      "2019-05-12 17:46:11,077 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,079 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2999 effective words/s\n",
      "2019-05-12 17:46:11,081 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,084 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,085 : INFO : EPOCH - 5 : training on 21 raw words (11 effective words) took 0.0s, 3154 effective words/s\n",
      "2019-05-12 17:46:11,085 : INFO : training on a 105 raw words (39 effective words) took 0.0s, 1297 effective words/s\n",
      "2019-05-12 17:46:11,086 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,087 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,090 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,092 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,092 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 2096 effective words/s\n",
      "2019-05-12 17:46:11,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,098 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,099 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2493 effective words/s\n",
      "2019-05-12 17:46:11,102 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,103 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,103 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,104 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 4546 effective words/s\n",
      "2019-05-12 17:46:11,107 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,107 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,108 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,109 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 2462 effective words/s\n",
      "2019-05-12 17:46:11,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,120 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,121 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,122 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2389 effective words/s\n",
      "2019-05-12 17:46:11,123 : INFO : training on a 105 raw words (33 effective words) took 0.0s, 922 effective words/s\n",
      "2019-05-12 17:46:11,124 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,130 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,137 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,138 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,141 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1194 effective words/s\n",
      "2019-05-12 17:46:11,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,147 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,148 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 3126 effective words/s\n",
      "2019-05-12 17:46:11,150 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,153 : INFO : EPOCH - 3 : training on 21 raw words (10 effective words) took 0.0s, 3409 effective words/s\n",
      "2019-05-12 17:46:11,155 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,157 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,159 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1857 effective words/s\n",
      "2019-05-12 17:46:11,163 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,165 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,166 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,167 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1975 effective words/s\n",
      "2019-05-12 17:46:11,169 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 980 effective words/s\n",
      "2019-05-12 17:46:11,170 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,172 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,177 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,180 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,180 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1656 effective words/s\n",
      "2019-05-12 17:46:11,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,185 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:11,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,188 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1236 effective words/s\n",
      "2019-05-12 17:46:11,191 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,192 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,194 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 1356 effective words/s\n",
      "2019-05-12 17:46:11,196 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,196 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,197 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,198 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 3633 effective words/s\n",
      "2019-05-12 17:46:11,200 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,203 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 2231 effective words/s\n",
      "2019-05-12 17:46:11,204 : INFO : training on a 105 raw words (29 effective words) took 0.0s, 949 effective words/s\n",
      "2019-05-12 17:46:11,204 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,205 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,208 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,211 : INFO : EPOCH - 1 : training on 21 raw words (4 effective words) took 0.0s, 1161 effective words/s\n",
      "2019-05-12 17:46:11,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,214 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,215 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,215 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1820 effective words/s\n",
      "2019-05-12 17:46:11,220 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,221 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,222 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,223 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2584 effective words/s\n",
      "2019-05-12 17:46:11,225 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,227 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 2029 effective words/s\n",
      "2019-05-12 17:46:11,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,230 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,231 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,232 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 2121 effective words/s\n",
      "2019-05-12 17:46:11,233 : INFO : training on a 105 raw words (27 effective words) took 0.0s, 992 effective words/s\n",
      "2019-05-12 17:46:11,234 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,235 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,238 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,239 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,239 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,240 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2649 effective words/s\n",
      "2019-05-12 17:46:11,243 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,244 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,245 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 3639 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:11,260 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,261 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,262 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,263 : INFO : EPOCH - 3 : training on 21 raw words (10 effective words) took 0.0s, 3052 effective words/s\n",
      "2019-05-12 17:46:11,267 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,268 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,269 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,270 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1545 effective words/s\n",
      "2019-05-12 17:46:11,278 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,279 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,280 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,281 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 2697 effective words/s\n",
      "2019-05-12 17:46:11,282 : INFO : training on a 105 raw words (38 effective words) took 0.0s, 826 effective words/s\n",
      "2019-05-12 17:46:11,283 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,285 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,288 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,289 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,290 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,291 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 2279 effective words/s\n",
      "2019-05-12 17:46:11,293 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,294 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,296 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 2415 effective words/s\n",
      "2019-05-12 17:46:11,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,303 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,304 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2385 effective words/s\n",
      "2019-05-12 17:46:11,307 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,311 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1954 effective words/s\n",
      "2019-05-12 17:46:11,313 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,317 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1632 effective words/s\n",
      "2019-05-12 17:46:11,319 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 1033 effective words/s\n",
      "2019-05-12 17:46:11,320 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,321 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,326 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,327 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,328 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 2325 effective words/s\n",
      "2019-05-12 17:46:11,332 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,333 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,334 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,335 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2205 effective words/s\n",
      "2019-05-12 17:46:11,337 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,339 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,340 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3007 effective words/s\n",
      "2019-05-12 17:46:11,342 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,345 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,346 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1703 effective words/s\n",
      "2019-05-12 17:46:11,349 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,351 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 2402 effective words/s\n",
      "2019-05-12 17:46:11,352 : INFO : training on a 105 raw words (32 effective words) took 0.0s, 1053 effective words/s\n",
      "2019-05-12 17:46:11,353 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,354 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,357 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,358 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,359 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,360 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 2540 effective words/s\n",
      "2019-05-12 17:46:11,367 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,372 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1211 effective words/s\n",
      "2019-05-12 17:46:11,380 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,382 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,382 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,384 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 2793 effective words/s\n",
      "2019-05-12 17:46:11,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,391 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,392 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,394 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1649 effective words/s\n",
      "2019-05-12 17:46:11,404 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,407 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,412 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 759 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:11,414 : INFO : training on a 105 raw words (35 effective words) took 0.1s, 593 effective words/s\n",
      "2019-05-12 17:46:11,444 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,446 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,450 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,452 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,452 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,453 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 3094 effective words/s\n",
      "2019-05-12 17:46:11,456 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,456 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,458 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,458 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 2793 effective words/s\n",
      "2019-05-12 17:46:11,462 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,464 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,466 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,467 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1618 effective words/s\n",
      "2019-05-12 17:46:11,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,471 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,472 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,473 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1714 effective words/s\n",
      "2019-05-12 17:46:11,477 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,480 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1835 effective words/s\n",
      "2019-05-12 17:46:11,482 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 1045 effective words/s\n",
      "2019-05-12 17:46:11,483 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,485 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:11,489 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,493 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,495 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,496 : INFO : EPOCH - 1 : training on 21 raw words (4 effective words) took 0.0s, 624 effective words/s\n",
      "2019-05-12 17:46:11,506 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,509 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,513 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 829 effective words/s\n",
      "2019-05-12 17:46:11,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,525 : INFO : EPOCH - 3 : training on 21 raw words (10 effective words) took 0.0s, 1887 effective words/s\n",
      "2019-05-12 17:46:11,530 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,531 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,533 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,536 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 952 effective words/s\n",
      "2019-05-12 17:46:11,539 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,542 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,543 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,544 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1432 effective words/s\n",
      "2019-05-12 17:46:11,545 : INFO : training on a 105 raw words (33 effective words) took 0.1s, 568 effective words/s\n",
      "2019-05-12 17:46:11,546 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,548 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,560 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 1561 effective words/s\n",
      "2019-05-12 17:46:11,565 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,567 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,569 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,570 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1000 effective words/s\n",
      "2019-05-12 17:46:11,575 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,595 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,616 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 146 effective words/s\n",
      "2019-05-12 17:46:11,624 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,633 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 870 effective words/s\n",
      "2019-05-12 17:46:11,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,642 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,643 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 905 effective words/s\n",
      "2019-05-12 17:46:11,644 : INFO : training on a 105 raw words (31 effective words) took 0.1s, 332 effective words/s\n",
      "2019-05-12 17:46:11,645 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,647 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,654 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,656 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,658 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,659 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1079 effective words/s\n",
      "2019-05-12 17:46:11,668 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,669 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,673 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1137 effective words/s\n",
      "2019-05-12 17:46:11,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,687 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 950 effective words/s\n",
      "2019-05-12 17:46:11,695 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,696 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,698 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,699 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1080 effective words/s\n",
      "2019-05-12 17:46:11,706 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,716 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 871 effective words/s\n",
      "2019-05-12 17:46:11,717 : INFO : training on a 105 raw words (31 effective words) took 0.1s, 456 effective words/s\n",
      "2019-05-12 17:46:11,718 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,719 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,723 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,729 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,731 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 873 effective words/s\n",
      "2019-05-12 17:46:11,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,739 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,740 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 1786 effective words/s\n",
      "2019-05-12 17:46:11,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,745 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:11,750 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 1447 effective words/s\n",
      "2019-05-12 17:46:11,759 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,761 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,765 : INFO : EPOCH - 4 : training on 21 raw words (4 effective words) took 0.0s, 733 effective words/s\n",
      "2019-05-12 17:46:11,770 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,772 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,773 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,774 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1661 effective words/s\n",
      "2019-05-12 17:46:11,775 : INFO : training on a 105 raw words (34 effective words) took 0.1s, 622 effective words/s\n",
      "2019-05-12 17:46:11,777 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,781 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,793 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,795 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,796 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,797 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1171 effective words/s\n",
      "2019-05-12 17:46:11,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,810 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 817 effective words/s\n",
      "2019-05-12 17:46:11,815 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,823 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 862 effective words/s\n",
      "2019-05-12 17:46:11,830 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,832 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,833 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,833 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1842 effective words/s\n",
      "2019-05-12 17:46:11,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,842 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,843 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,846 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1182 effective words/s\n",
      "2019-05-12 17:46:11,847 : INFO : training on a 105 raw words (31 effective words) took 0.1s, 527 effective words/s\n",
      "2019-05-12 17:46:11,849 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,850 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,855 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,856 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,857 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,858 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1677 effective words/s\n",
      "2019-05-12 17:46:11,863 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,864 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,865 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,866 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1700 effective words/s\n",
      "2019-05-12 17:46:11,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,873 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,874 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,876 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1109 effective words/s\n",
      "2019-05-12 17:46:11,879 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,880 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,882 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2416 effective words/s\n",
      "2019-05-12 17:46:11,889 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,890 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,892 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 2001 effective words/s\n",
      "2019-05-12 17:46:11,893 : INFO : training on a 105 raw words (30 effective words) took 0.0s, 708 effective words/s\n",
      "2019-05-12 17:46:11,894 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,896 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,900 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,903 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,904 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,905 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 1265 effective words/s\n",
      "2019-05-12 17:46:11,918 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,919 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,921 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 2700 effective words/s\n",
      "2019-05-12 17:46:11,925 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,928 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,929 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 1836 effective words/s\n",
      "2019-05-12 17:46:11,931 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,932 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,936 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1130 effective words/s\n",
      "2019-05-12 17:46:11,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,944 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,946 : INFO : EPOCH - 5 : training on 21 raw words (10 effective words) took 0.0s, 3022 effective words/s\n",
      "2019-05-12 17:46:11,948 : INFO : training on a 105 raw words (40 effective words) took 0.1s, 790 effective words/s\n",
      "2019-05-12 17:46:11,950 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,954 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:11,961 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,964 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,966 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 1548 effective words/s\n",
      "2019-05-12 17:46:11,969 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,970 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,971 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,972 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2062 effective words/s\n",
      "2019-05-12 17:46:11,978 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,980 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:11,983 : INFO : EPOCH - 3 : training on 21 raw words (4 effective words) took 0.0s, 826 effective words/s\n",
      "2019-05-12 17:46:11,986 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,989 : INFO : EPOCH - 4 : training on 21 raw words (9 effective words) took 0.0s, 3521 effective words/s\n",
      "2019-05-12 17:46:11,991 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:11,992 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:11,993 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:11,994 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 3283 effective words/s\n",
      "2019-05-12 17:46:11,995 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 872 effective words/s\n",
      "2019-05-12 17:46:11,996 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:11,997 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,004 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,006 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 2055 effective words/s\n",
      "2019-05-12 17:46:12,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,010 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,011 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2384 effective words/s\n",
      "2019-05-12 17:46:12,014 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,014 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,016 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 3769 effective words/s\n",
      "2019-05-12 17:46:12,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,020 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,022 : INFO : EPOCH - 4 : training on 21 raw words (10 effective words) took 0.0s, 4554 effective words/s\n",
      "2019-05-12 17:46:12,029 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,030 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,033 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 1868 effective words/s\n",
      "2019-05-12 17:46:12,034 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 1011 effective words/s\n",
      "2019-05-12 17:46:12,035 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,036 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,041 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,042 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,042 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,046 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 1561 effective words/s\n",
      "2019-05-12 17:46:12,048 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,050 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,052 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1114 effective words/s\n",
      "2019-05-12 17:46:12,055 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,056 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,057 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,057 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2950 effective words/s\n",
      "2019-05-12 17:46:12,060 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,061 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,062 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,062 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2793 effective words/s\n",
      "2019-05-12 17:46:12,065 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,066 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,067 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,067 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 3122 effective words/s\n",
      "2019-05-12 17:46:12,068 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 1089 effective words/s\n",
      "2019-05-12 17:46:12,069 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,071 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,074 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,075 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,077 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1692 effective words/s\n",
      "2019-05-12 17:46:12,080 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,082 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,082 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 2847 effective words/s\n",
      "2019-05-12 17:46:12,085 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,086 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,087 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,088 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2198 effective words/s\n",
      "2019-05-12 17:46:12,090 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,091 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,092 : INFO : EPOCH - 4 : training on 21 raw words (9 effective words) took 0.0s, 3853 effective words/s\n",
      "2019-05-12 17:46:12,095 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,096 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,097 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,098 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2392 effective words/s\n",
      "2019-05-12 17:46:12,100 : INFO : training on a 105 raw words (33 effective words) took 0.0s, 1171 effective words/s\n",
      "2019-05-12 17:46:12,101 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:12,102 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,105 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,105 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,106 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,106 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 2737 effective words/s\n",
      "2019-05-12 17:46:12,109 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,110 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,111 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,111 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 2983 effective words/s\n",
      "2019-05-12 17:46:12,132 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,133 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,134 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,136 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1563 effective words/s\n",
      "2019-05-12 17:46:12,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,150 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,151 : INFO : EPOCH - 4 : training on 21 raw words (4 effective words) took 0.0s, 665 effective words/s\n",
      "2019-05-12 17:46:12,156 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,157 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,158 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,159 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 2626 effective words/s\n",
      "2019-05-12 17:46:12,161 : INFO : training on a 105 raw words (33 effective words) took 0.1s, 569 effective words/s\n",
      "2019-05-12 17:46:12,162 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,163 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,171 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,173 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,174 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 2267 effective words/s\n",
      "2019-05-12 17:46:12,178 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,179 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,181 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,182 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 1692 effective words/s\n",
      "2019-05-12 17:46:12,188 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,190 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,193 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 1532 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:12,198 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,199 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,203 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1160 effective words/s\n",
      "2019-05-12 17:46:12,204 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,207 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2375 effective words/s\n",
      "2019-05-12 17:46:12,208 : INFO : training on a 105 raw words (36 effective words) took 0.0s, 877 effective words/s\n",
      "2019-05-12 17:46:12,209 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,210 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,214 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,218 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,221 : INFO : EPOCH - 1 : training on 21 raw words (9 effective words) took 0.0s, 1116 effective words/s\n",
      "2019-05-12 17:46:12,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,229 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1683 effective words/s\n",
      "2019-05-12 17:46:12,235 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,238 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,240 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 1011 effective words/s\n",
      "2019-05-12 17:46:12,243 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,247 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 1992 effective words/s\n",
      "2019-05-12 17:46:12,252 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,253 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,253 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,256 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 2236 effective words/s\n",
      "2019-05-12 17:46:12,257 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 796 effective words/s\n",
      "2019-05-12 17:46:12,258 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,259 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,265 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,267 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1557 effective words/s\n",
      "2019-05-12 17:46:12,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,273 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,274 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,276 : INFO : EPOCH - 2 : training on 21 raw words (10 effective words) took 0.0s, 1645 effective words/s\n",
      "2019-05-12 17:46:12,280 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,281 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,284 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1394 effective words/s\n",
      "2019-05-12 17:46:12,287 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,289 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,291 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,291 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1859 effective words/s\n",
      "2019-05-12 17:46:12,297 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,299 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,300 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,303 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 1445 effective words/s\n",
      "2019-05-12 17:46:12,304 : INFO : training on a 105 raw words (36 effective words) took 0.0s, 837 effective words/s\n",
      "2019-05-12 17:46:12,305 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,306 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,312 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,312 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,313 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,314 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 3334 effective words/s\n",
      "2019-05-12 17:46:12,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,321 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,323 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 997 effective words/s\n",
      "2019-05-12 17:46:12,328 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,329 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,333 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1569 effective words/s\n",
      "2019-05-12 17:46:12,337 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,338 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,339 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2746 effective words/s\n",
      "2019-05-12 17:46:12,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,347 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1348 effective words/s\n",
      "2019-05-12 17:46:12,348 : INFO : training on a 105 raw words (31 effective words) took 0.0s, 752 effective words/s\n",
      "2019-05-12 17:46:12,352 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,354 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:12,360 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,361 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,362 : INFO : EPOCH - 1 : training on 21 raw words (10 effective words) took 0.0s, 3542 effective words/s\n",
      "2019-05-12 17:46:12,371 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,372 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,375 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 1747 effective words/s\n",
      "2019-05-12 17:46:12,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,385 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,386 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 2465 effective words/s\n",
      "2019-05-12 17:46:12,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,390 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,392 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2281 effective words/s\n",
      "2019-05-12 17:46:12,397 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,398 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,398 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,399 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 2654 effective words/s\n",
      "2019-05-12 17:46:12,400 : INFO : training on a 105 raw words (40 effective words) took 0.0s, 899 effective words/s\n",
      "2019-05-12 17:46:12,400 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,401 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,404 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,405 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,406 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,406 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 2887 effective words/s\n",
      "2019-05-12 17:46:12,409 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:12,411 : INFO : EPOCH - 2 : training on 21 raw words (10 effective words) took 0.0s, 4658 effective words/s\n",
      "2019-05-12 17:46:12,414 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,415 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,417 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2273 effective words/s\n",
      "2019-05-12 17:46:12,421 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,423 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,423 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2984 effective words/s\n",
      "2019-05-12 17:46:12,426 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,428 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 3679 effective words/s\n",
      "2019-05-12 17:46:12,428 : INFO : training on a 105 raw words (40 effective words) took 0.0s, 1492 effective words/s\n",
      "2019-05-12 17:46:12,429 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,430 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,433 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,434 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,434 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,435 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2913 effective words/s\n",
      "2019-05-12 17:46:12,437 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,439 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,440 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 3290 effective words/s\n",
      "2019-05-12 17:46:12,443 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,443 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,444 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,445 : INFO : EPOCH - 3 : training on 21 raw words (11 effective words) took 0.0s, 3571 effective words/s\n",
      "2019-05-12 17:46:12,448 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,449 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,450 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2450 effective words/s\n",
      "2019-05-12 17:46:12,453 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,454 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,454 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,455 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 2445 effective words/s\n",
      "2019-05-12 17:46:12,455 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 1509 effective words/s\n",
      "2019-05-12 17:46:12,456 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,457 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,459 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,461 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 3364 effective words/s\n",
      "2019-05-12 17:46:12,464 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,465 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,465 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,466 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2217 effective words/s\n",
      "2019-05-12 17:46:12,469 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,470 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,470 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,471 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3269 effective words/s\n",
      "2019-05-12 17:46:12,474 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,476 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,476 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1741 effective words/s\n",
      "2019-05-12 17:46:12,479 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,480 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,480 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 2637 effective words/s\n",
      "2019-05-12 17:46:12,481 : INFO : training on a 105 raw words (33 effective words) took 0.0s, 1391 effective words/s\n",
      "2019-05-12 17:46:12,482 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,483 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,486 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,487 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,488 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,488 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1919 effective words/s\n",
      "2019-05-12 17:46:12,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,493 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,493 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,494 : INFO : EPOCH - 2 : training on 21 raw words (10 effective words) took 0.0s, 3788 effective words/s\n",
      "2019-05-12 17:46:12,497 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,498 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,498 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,499 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3364 effective words/s\n",
      "2019-05-12 17:46:12,503 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,505 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,506 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,507 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 2047 effective words/s\n",
      "2019-05-12 17:46:12,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,512 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:12,513 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,514 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 2218 effective words/s\n",
      "2019-05-12 17:46:12,515 : INFO : training on a 105 raw words (40 effective words) took 0.0s, 1296 effective words/s\n",
      "2019-05-12 17:46:12,515 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,516 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,521 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,525 : INFO : EPOCH - 1 : training on 21 raw words (10 effective words) took 0.0s, 2552 effective words/s\n",
      "2019-05-12 17:46:12,528 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,534 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,551 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 309 effective words/s\n",
      "2019-05-12 17:46:12,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,558 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,559 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2402 effective words/s\n",
      "2019-05-12 17:46:12,562 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,563 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,564 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,565 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1948 effective words/s\n",
      "2019-05-12 17:46:12,569 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,571 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,575 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1284 effective words/s\n",
      "2019-05-12 17:46:12,576 : INFO : training on a 105 raw words (36 effective words) took 0.1s, 613 effective words/s\n",
      "2019-05-12 17:46:12,576 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,577 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,579 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,580 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,612 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,614 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 202 effective words/s\n",
      "2019-05-12 17:46:12,619 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,620 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,621 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,621 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2575 effective words/s\n",
      "2019-05-12 17:46:12,623 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,625 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,626 : INFO : EPOCH - 3 : training on 21 raw words (10 effective words) took 0.0s, 3780 effective words/s\n",
      "2019-05-12 17:46:12,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,628 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,629 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,630 : INFO : EPOCH - 4 : training on 21 raw words (9 effective words) took 0.0s, 4083 effective words/s\n",
      "2019-05-12 17:46:12,632 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,633 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:12,636 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,639 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1031 effective words/s\n",
      "2019-05-12 17:46:12,641 : INFO : training on a 105 raw words (40 effective words) took 0.1s, 635 effective words/s\n",
      "2019-05-12 17:46:12,643 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,648 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,661 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,663 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,664 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,665 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 1989 effective words/s\n",
      "2019-05-12 17:46:12,669 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,670 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,671 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,672 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2537 effective words/s\n",
      "2019-05-12 17:46:12,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,678 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2744 effective words/s\n",
      "2019-05-12 17:46:12,679 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,680 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,681 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,682 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 2797 effective words/s\n",
      "2019-05-12 17:46:12,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,687 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,689 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,689 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1774 effective words/s\n",
      "2019-05-12 17:46:12,690 : INFO : training on a 105 raw words (36 effective words) took 0.0s, 866 effective words/s\n",
      "2019-05-12 17:46:12,691 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,691 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,694 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,695 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,697 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 3327 effective words/s\n",
      "2019-05-12 17:46:12,702 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,703 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,704 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2353 effective words/s\n",
      "2019-05-12 17:46:12,706 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,709 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,710 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 2163 effective words/s\n",
      "2019-05-12 17:46:12,714 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,715 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,717 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1811 effective words/s\n",
      "2019-05-12 17:46:12,720 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,724 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,726 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 1423 effective words/s\n",
      "2019-05-12 17:46:12,727 : INFO : training on a 105 raw words (39 effective words) took 0.0s, 1123 effective words/s\n",
      "2019-05-12 17:46:12,728 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,729 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,732 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,733 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,734 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2255 effective words/s\n",
      "2019-05-12 17:46:12,739 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,742 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,745 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 895 effective words/s\n",
      "2019-05-12 17:46:12,749 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,750 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,751 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,752 : INFO : EPOCH - 3 : training on 21 raw words (12 effective words) took 0.0s, 5109 effective words/s\n",
      "2019-05-12 17:46:12,758 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,759 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,761 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,761 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1391 effective words/s\n",
      "2019-05-12 17:46:12,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,769 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,771 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,772 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1303 effective words/s\n",
      "2019-05-12 17:46:12,774 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 790 effective words/s\n",
      "2019-05-12 17:46:12,775 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,776 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,788 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,793 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,794 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,796 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 886 effective words/s\n",
      "2019-05-12 17:46:12,798 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:12,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,802 : INFO : EPOCH - 2 : training on 21 raw words (10 effective words) took 0.0s, 2518 effective words/s\n",
      "2019-05-12 17:46:12,809 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,814 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 1095 effective words/s\n",
      "2019-05-12 17:46:12,821 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,828 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,829 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 688 effective words/s\n",
      "2019-05-12 17:46:12,835 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,836 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,837 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,839 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1516 effective words/s\n",
      "2019-05-12 17:46:12,840 : INFO : training on a 105 raw words (34 effective words) took 0.1s, 544 effective words/s\n",
      "2019-05-12 17:46:12,841 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,842 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,845 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,847 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:12,849 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,852 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 799 effective words/s\n",
      "2019-05-12 17:46:12,859 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,861 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,862 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,863 : INFO : EPOCH - 2 : training on 21 raw words (10 effective words) took 0.0s, 2823 effective words/s\n",
      "2019-05-12 17:46:12,871 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,873 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,877 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 940 effective words/s\n",
      "2019-05-12 17:46:12,883 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,885 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,885 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,886 : INFO : EPOCH - 4 : training on 21 raw words (4 effective words) took 0.0s, 1293 effective words/s\n",
      "2019-05-12 17:46:12,893 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,894 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,895 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2893 effective words/s\n",
      "2019-05-12 17:46:12,898 : INFO : training on a 105 raw words (32 effective words) took 0.1s, 580 effective words/s\n",
      "2019-05-12 17:46:12,899 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,902 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,915 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,918 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 937 effective words/s\n",
      "2019-05-12 17:46:12,925 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,927 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,928 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,929 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 2164 effective words/s\n",
      "2019-05-12 17:46:12,935 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,936 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,937 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,938 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1818 effective words/s\n",
      "2019-05-12 17:46:12,944 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,945 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,946 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,947 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2470 effective words/s\n",
      "2019-05-12 17:46:12,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,959 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,959 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 1418 effective words/s\n",
      "2019-05-12 17:46:12,960 : INFO : training on a 105 raw words (35 effective words) took 0.1s, 616 effective words/s\n",
      "2019-05-12 17:46:12,961 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:12,962 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:12,964 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,965 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,967 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,969 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 1889 effective words/s\n",
      "2019-05-12 17:46:12,974 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,977 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1643 effective words/s\n",
      "2019-05-12 17:46:12,979 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,980 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,982 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 2586 effective words/s\n",
      "2019-05-12 17:46:12,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,990 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,992 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1350 effective words/s\n",
      "2019-05-12 17:46:12,994 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:12,995 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:12,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:12,996 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 2475 effective words/s\n",
      "2019-05-12 17:46:12,997 : INFO : training on a 105 raw words (32 effective words) took 0.0s, 923 effective words/s\n",
      "2019-05-12 17:46:13,000 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,001 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,005 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,007 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,008 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2302 effective words/s\n",
      "2019-05-12 17:46:13,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,011 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,012 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,013 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2579 effective words/s\n",
      "2019-05-12 17:46:13,018 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,023 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1299 effective words/s\n",
      "2019-05-12 17:46:13,026 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,028 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,029 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2338 effective words/s\n",
      "2019-05-12 17:46:13,031 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,033 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,041 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 838 effective words/s\n",
      "2019-05-12 17:46:13,045 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 816 effective words/s\n",
      "2019-05-12 17:46:13,046 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,048 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,055 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,055 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,057 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,058 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1910 effective words/s\n",
      "2019-05-12 17:46:13,065 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,068 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,069 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 1877 effective words/s\n",
      "2019-05-12 17:46:13,074 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,075 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,077 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 2036 effective words/s\n",
      "2019-05-12 17:46:13,080 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,083 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,086 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1122 effective words/s\n",
      "2019-05-12 17:46:13,091 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,092 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,093 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,095 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 998 effective words/s\n",
      "2019-05-12 17:46:13,097 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 732 effective words/s\n",
      "2019-05-12 17:46:13,098 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,099 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:13,105 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,106 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,108 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,108 : INFO : EPOCH - 1 : training on 21 raw words (10 effective words) took 0.0s, 2596 effective words/s\n",
      "2019-05-12 17:46:13,112 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,113 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,113 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,115 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1781 effective words/s\n",
      "2019-05-12 17:46:13,124 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,126 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3836 effective words/s\n",
      "2019-05-12 17:46:13,130 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,132 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,133 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,134 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1241 effective words/s\n",
      "2019-05-12 17:46:13,136 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,137 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,138 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,139 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 3043 effective words/s\n",
      "2019-05-12 17:46:13,140 : INFO : training on a 105 raw words (40 effective words) took 0.0s, 1032 effective words/s\n",
      "2019-05-12 17:46:13,140 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,141 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,148 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,149 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1068 effective words/s\n",
      "2019-05-12 17:46:13,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,155 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2040 effective words/s\n",
      "2019-05-12 17:46:13,159 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,162 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,165 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1116 effective words/s\n",
      "2019-05-12 17:46:13,170 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,174 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,175 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,176 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 1320 effective words/s\n",
      "2019-05-12 17:46:13,183 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,185 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,188 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1386 effective words/s\n",
      "2019-05-12 17:46:13,188 : INFO : training on a 105 raw words (32 effective words) took 0.0s, 694 effective words/s\n",
      "2019-05-12 17:46:13,190 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,191 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,198 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,200 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,203 : INFO : EPOCH - 1 : training on 21 raw words (9 effective words) took 0.0s, 1714 effective words/s\n",
      "2019-05-12 17:46:13,207 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,208 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,209 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,210 : INFO : EPOCH - 2 : training on 21 raw words (11 effective words) took 0.0s, 3469 effective words/s\n",
      "2019-05-12 17:46:13,212 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,213 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,216 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,218 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 886 effective words/s\n",
      "2019-05-12 17:46:13,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,222 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,223 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,225 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 2624 effective words/s\n",
      "2019-05-12 17:46:13,228 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,229 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,231 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,232 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2241 effective words/s\n",
      "2019-05-12 17:46:13,233 : INFO : training on a 105 raw words (40 effective words) took 0.0s, 977 effective words/s\n",
      "2019-05-12 17:46:13,234 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,235 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,240 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,243 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2128 effective words/s\n",
      "2019-05-12 17:46:13,254 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,255 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,256 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,257 : INFO : EPOCH - 2 : training on 21 raw words (4 effective words) took 0.0s, 1122 effective words/s\n",
      "2019-05-12 17:46:13,260 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,260 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,262 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2401 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:13,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,271 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,273 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 1740 effective words/s\n",
      "2019-05-12 17:46:13,276 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,278 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 3103 effective words/s\n",
      "2019-05-12 17:46:13,279 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 814 effective words/s\n",
      "2019-05-12 17:46:13,280 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,286 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,292 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,295 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2209 effective words/s\n",
      "2019-05-12 17:46:13,299 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,303 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,303 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1148 effective words/s\n",
      "2019-05-12 17:46:13,308 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,311 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3182 effective words/s\n",
      "2019-05-12 17:46:13,313 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,314 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,317 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1401 effective words/s\n",
      "2019-05-12 17:46:13,324 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,325 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,325 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,326 : INFO : EPOCH - 5 : training on 21 raw words (12 effective words) took 0.0s, 5242 effective words/s\n",
      "2019-05-12 17:46:13,327 : INFO : training on a 105 raw words (38 effective words) took 0.0s, 958 effective words/s\n",
      "2019-05-12 17:46:13,328 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,328 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,333 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,334 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,336 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,338 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 1331 effective words/s\n",
      "2019-05-12 17:46:13,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:13,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,350 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 911 effective words/s\n",
      "2019-05-12 17:46:13,356 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,361 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 1201 effective words/s\n",
      "2019-05-12 17:46:13,365 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,366 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,367 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,368 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1833 effective words/s\n",
      "2019-05-12 17:46:13,379 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,380 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,382 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,384 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 1750 effective words/s\n",
      "2019-05-12 17:46:13,385 : INFO : training on a 105 raw words (38 effective words) took 0.1s, 678 effective words/s\n",
      "2019-05-12 17:46:13,386 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,389 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,395 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,396 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,397 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1713 effective words/s\n",
      "2019-05-12 17:46:13,402 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,404 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,405 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,407 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 1854 effective words/s\n",
      "2019-05-12 17:46:13,412 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,413 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 3880 effective words/s\n",
      "2019-05-12 17:46:13,417 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,418 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,419 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,422 : INFO : EPOCH - 4 : training on 21 raw words (11 effective words) took 0.0s, 2121 effective words/s\n",
      "2019-05-12 17:46:13,426 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,428 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,428 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2736 effective words/s\n",
      "2019-05-12 17:46:13,429 : INFO : training on a 105 raw words (41 effective words) took 0.0s, 1039 effective words/s\n",
      "2019-05-12 17:46:13,429 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,430 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,439 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,440 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1421 effective words/s\n",
      "2019-05-12 17:46:13,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,443 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,444 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,444 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 3324 effective words/s\n",
      "2019-05-12 17:46:13,446 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,447 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,448 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,449 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2095 effective words/s\n",
      "2019-05-12 17:46:13,454 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,455 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,460 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 790 effective words/s\n",
      "2019-05-12 17:46:13,464 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,470 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,471 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,472 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 606 effective words/s\n",
      "2019-05-12 17:46:13,473 : INFO : training on a 105 raw words (29 effective words) took 0.0s, 693 effective words/s\n",
      "2019-05-12 17:46:13,474 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,475 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,483 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,484 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,485 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,486 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1397 effective words/s\n",
      "2019-05-12 17:46:13,494 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,496 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,497 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1680 effective words/s\n",
      "2019-05-12 17:46:13,506 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,509 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2188 effective words/s\n",
      "2019-05-12 17:46:13,517 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,518 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,519 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,521 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1384 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:13,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,525 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,526 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2905 effective words/s\n",
      "2019-05-12 17:46:13,527 : INFO : training on a 105 raw words (30 effective words) took 0.0s, 641 effective words/s\n",
      "2019-05-12 17:46:13,527 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,529 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,535 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,538 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,539 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1197 effective words/s\n",
      "2019-05-12 17:46:13,546 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,550 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1537 effective words/s\n",
      "2019-05-12 17:46:13,554 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,557 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,558 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1884 effective words/s\n",
      "2019-05-12 17:46:13,563 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,564 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,568 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1132 effective words/s\n",
      "2019-05-12 17:46:13,572 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,573 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,574 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,575 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 1273 effective words/s\n",
      "2019-05-12 17:46:13,576 : INFO : training on a 105 raw words (29 effective words) took 0.0s, 617 effective words/s\n",
      "2019-05-12 17:46:13,577 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,578 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,582 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:13,589 : INFO : EPOCH - 1 : training on 21 raw words (9 effective words) took 0.0s, 1030 effective words/s\n",
      "2019-05-12 17:46:13,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,614 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,614 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 2033 effective words/s\n",
      "2019-05-12 17:46:13,617 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,618 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,619 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,620 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2492 effective words/s\n",
      "2019-05-12 17:46:13,623 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,625 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,625 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2379 effective words/s\n",
      "2019-05-12 17:46:13,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,629 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,631 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 1392 effective words/s\n",
      "2019-05-12 17:46:13,632 : INFO : training on a 105 raw words (32 effective words) took 0.1s, 594 effective words/s\n",
      "2019-05-12 17:46:13,633 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,634 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,639 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,645 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 857 effective words/s\n",
      "2019-05-12 17:46:13,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,652 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,652 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,655 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 1845 effective words/s\n",
      "2019-05-12 17:46:13,659 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,660 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,660 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,662 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1838 effective words/s\n",
      "2019-05-12 17:46:13,668 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,670 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,672 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,673 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1262 effective words/s\n",
      "2019-05-12 17:46:13,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,678 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,679 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 1991 effective words/s\n",
      "2019-05-12 17:46:13,680 : INFO : training on a 105 raw words (36 effective words) took 0.0s, 789 effective words/s\n",
      "2019-05-12 17:46:13,682 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,683 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,688 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,689 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,691 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1491 effective words/s\n",
      "2019-05-12 17:46:13,699 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,704 : INFO : EPOCH - 2 : training on 21 raw words (4 effective words) took 0.0s, 781 effective words/s\n",
      "2019-05-12 17:46:13,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,711 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,713 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1499 effective words/s\n",
      "2019-05-12 17:46:13,718 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,720 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,721 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,722 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1287 effective words/s\n",
      "2019-05-12 17:46:13,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,726 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,729 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1514 effective words/s\n",
      "2019-05-12 17:46:13,730 : INFO : training on a 105 raw words (29 effective words) took 0.0s, 646 effective words/s\n",
      "2019-05-12 17:46:13,732 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,733 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,741 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 1692 effective words/s\n",
      "2019-05-12 17:46:13,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,746 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1793 effective words/s\n",
      "2019-05-12 17:46:13,751 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,752 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,753 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 2496 effective words/s\n",
      "2019-05-12 17:46:13,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,757 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:13,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,758 : INFO : EPOCH - 4 : training on 21 raw words (8 effective words) took 0.0s, 3956 effective words/s\n",
      "2019-05-12 17:46:13,761 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,763 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,763 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,766 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1033 effective words/s\n",
      "2019-05-12 17:46:13,768 : INFO : training on a 105 raw words (31 effective words) took 0.0s, 943 effective words/s\n",
      "2019-05-12 17:46:13,768 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,769 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,774 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,777 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1878 effective words/s\n",
      "2019-05-12 17:46:13,779 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,783 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,787 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 1012 effective words/s\n",
      "2019-05-12 17:46:13,791 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,793 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,793 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,794 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2567 effective words/s\n",
      "2019-05-12 17:46:13,796 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,797 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,798 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,801 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1345 effective words/s\n",
      "2019-05-12 17:46:13,804 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,810 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 1492 effective words/s\n",
      "2019-05-12 17:46:13,811 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 853 effective words/s\n",
      "2019-05-12 17:46:13,812 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,813 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,821 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,823 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,824 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,826 : INFO : EPOCH - 1 : training on 21 raw words (9 effective words) took 0.0s, 1709 effective words/s\n",
      "2019-05-12 17:46:13,832 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,834 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:13,835 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,836 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 1281 effective words/s\n",
      "2019-05-12 17:46:13,842 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,843 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,845 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 2897 effective words/s\n",
      "2019-05-12 17:46:13,852 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,853 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,855 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1932 effective words/s\n",
      "2019-05-12 17:46:13,861 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,863 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,864 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,865 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1740 effective words/s\n",
      "2019-05-12 17:46:13,866 : INFO : training on a 105 raw words (36 effective words) took 0.1s, 699 effective words/s\n",
      "2019-05-12 17:46:13,868 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,869 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,872 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,877 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 999 effective words/s\n",
      "2019-05-12 17:46:13,885 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,887 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,889 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1423 effective words/s\n",
      "2019-05-12 17:46:13,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,894 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,896 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1827 effective words/s\n",
      "2019-05-12 17:46:13,902 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,903 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,904 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,906 : INFO : EPOCH - 4 : training on 21 raw words (9 effective words) took 0.0s, 2139 effective words/s\n",
      "2019-05-12 17:46:13,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,913 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2539 effective words/s\n",
      "2019-05-12 17:46:13,921 : INFO : training on a 105 raw words (34 effective words) took 0.1s, 674 effective words/s\n",
      "2019-05-12 17:46:13,922 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,923 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,930 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,932 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,934 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 1731 effective words/s\n",
      "2019-05-12 17:46:13,937 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,938 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,938 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,939 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 3137 effective words/s\n",
      "2019-05-12 17:46:13,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,944 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,945 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2673 effective words/s\n",
      "2019-05-12 17:46:13,948 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,949 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,949 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,951 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2396 effective words/s\n",
      "2019-05-12 17:46:13,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,956 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 2131 effective words/s\n",
      "2019-05-12 17:46:13,960 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 979 effective words/s\n",
      "2019-05-12 17:46:13,961 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:13,962 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:13,968 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,969 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,969 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,970 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 2281 effective words/s\n",
      "2019-05-12 17:46:13,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,976 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 2496 effective words/s\n",
      "2019-05-12 17:46:13,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,982 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:13,985 : INFO : EPOCH - 3 : training on 21 raw words (8 effective words) took 0.0s, 2415 effective words/s\n",
      "2019-05-12 17:46:13,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:13,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:13,998 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,000 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2096 effective words/s\n",
      "2019-05-12 17:46:14,004 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:14,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,007 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,008 : INFO : EPOCH - 5 : training on 21 raw words (9 effective words) took 0.0s, 2577 effective words/s\n",
      "2019-05-12 17:46:14,010 : INFO : training on a 105 raw words (38 effective words) took 0.0s, 810 effective words/s\n",
      "2019-05-12 17:46:14,011 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,012 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,017 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,020 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1743 effective words/s\n",
      "2019-05-12 17:46:14,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,026 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,026 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 3149 effective words/s\n",
      "2019-05-12 17:46:14,030 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,033 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,034 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 1150 effective words/s\n",
      "2019-05-12 17:46:14,038 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,040 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2509 effective words/s\n",
      "2019-05-12 17:46:14,042 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,047 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,048 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 889 effective words/s\n",
      "2019-05-12 17:46:14,049 : INFO : training on a 105 raw words (28 effective words) took 0.0s, 786 effective words/s\n",
      "2019-05-12 17:46:14,049 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,051 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,054 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,057 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,059 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 1885 effective words/s\n",
      "2019-05-12 17:46:14,062 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,067 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 1536 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:14,078 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,079 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,081 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 2310 effective words/s\n",
      "2019-05-12 17:46:14,089 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,091 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,092 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2695 effective words/s\n",
      "2019-05-12 17:46:14,095 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,096 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,097 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,098 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 1735 effective words/s\n",
      "2019-05-12 17:46:14,099 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 732 effective words/s\n",
      "2019-05-12 17:46:14,101 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,102 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,104 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,107 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,109 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,110 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 866 effective words/s\n",
      "2019-05-12 17:46:14,113 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,115 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,119 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,120 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 1068 effective words/s\n",
      "2019-05-12 17:46:14,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,129 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,130 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,132 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1438 effective words/s\n",
      "2019-05-12 17:46:14,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,140 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,141 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,142 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2099 effective words/s\n",
      "2019-05-12 17:46:14,146 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,148 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,151 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1387 effective words/s\n",
      "2019-05-12 17:46:14,152 : INFO : training on a 105 raw words (33 effective words) took 0.0s, 661 effective words/s\n",
      "2019-05-12 17:46:14,155 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,156 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,161 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,161 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,162 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,163 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1892 effective words/s\n",
      "2019-05-12 17:46:14,168 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,169 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,170 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,172 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 1733 effective words/s\n",
      "2019-05-12 17:46:14,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,177 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,178 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2544 effective words/s\n",
      "2019-05-12 17:46:14,181 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,182 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,183 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,186 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1533 effective words/s\n",
      "2019-05-12 17:46:14,189 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,191 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,192 : INFO : EPOCH - 5 : training on 21 raw words (11 effective words) took 0.0s, 3795 effective words/s\n",
      "2019-05-12 17:46:14,193 : INFO : training on a 105 raw words (37 effective words) took 0.0s, 1037 effective words/s\n",
      "2019-05-12 17:46:14,194 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,195 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,200 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,202 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,204 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,205 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1253 effective words/s\n",
      "2019-05-12 17:46:14,208 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,211 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2022 effective words/s\n",
      "2019-05-12 17:46:14,217 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,220 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,221 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 2072 effective words/s\n",
      "2019-05-12 17:46:14,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,228 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2433 effective words/s\n",
      "2019-05-12 17:46:14,231 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,237 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 757 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:14,239 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 788 effective words/s\n",
      "2019-05-12 17:46:14,240 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,241 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,243 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,245 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 3486 effective words/s\n",
      "2019-05-12 17:46:14,247 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,249 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,251 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,252 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 1865 effective words/s\n",
      "2019-05-12 17:46:14,254 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,255 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,256 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,257 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 3236 effective words/s\n",
      "2019-05-12 17:46:14,259 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,262 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,263 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,264 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1450 effective words/s\n",
      "2019-05-12 17:46:14,266 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,267 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,271 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1526 effective words/s\n",
      "2019-05-12 17:46:14,272 : INFO : training on a 105 raw words (39 effective words) took 0.0s, 1286 effective words/s\n",
      "2019-05-12 17:46:14,274 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,275 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,280 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,281 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,283 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2090 effective words/s\n",
      "2019-05-12 17:46:14,287 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,288 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,288 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,289 : INFO : EPOCH - 2 : training on 21 raw words (8 effective words) took 0.0s, 2925 effective words/s\n",
      "2019-05-12 17:46:14,293 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,298 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,299 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,300 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 836 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:14,307 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,309 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,310 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1523 effective words/s\n",
      "2019-05-12 17:46:14,321 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,324 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,325 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,327 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1258 effective words/s\n",
      "2019-05-12 17:46:14,329 : INFO : training on a 105 raw words (33 effective words) took 0.1s, 619 effective words/s\n",
      "2019-05-12 17:46:14,330 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,331 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,335 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,338 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,338 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2287 effective words/s\n",
      "2019-05-12 17:46:14,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,342 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,343 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,344 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 2327 effective words/s\n",
      "2019-05-12 17:46:14,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,347 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,352 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 1217 effective words/s\n",
      "2019-05-12 17:46:14,353 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,359 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,360 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1078 effective words/s\n",
      "2019-05-12 17:46:14,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,371 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1539 effective words/s\n",
      "2019-05-12 17:46:14,372 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 861 effective words/s\n",
      "2019-05-12 17:46:14,373 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,374 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,377 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,378 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,379 : INFO : EPOCH - 1 : training on 21 raw words (7 effective words) took 0.0s, 2332 effective words/s\n",
      "2019-05-12 17:46:14,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,386 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,386 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,387 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 1937 effective words/s\n",
      "2019-05-12 17:46:14,392 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,393 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,394 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,395 : INFO : EPOCH - 3 : training on 21 raw words (9 effective words) took 0.0s, 3151 effective words/s\n",
      "2019-05-12 17:46:14,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,404 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,405 : INFO : EPOCH - 4 : training on 21 raw words (4 effective words) took 0.0s, 664 effective words/s\n",
      "2019-05-12 17:46:14,408 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,410 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,412 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,413 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1193 effective words/s\n",
      "2019-05-12 17:46:14,415 : INFO : training on a 105 raw words (35 effective words) took 0.0s, 851 effective words/s\n",
      "2019-05-12 17:46:14,418 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,419 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,428 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,429 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1892 effective words/s\n",
      "2019-05-12 17:46:14,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,440 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 1707 effective words/s\n",
      "2019-05-12 17:46:14,443 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,445 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,448 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1421 effective words/s\n",
      "2019-05-12 17:46:14,452 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,454 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,454 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,455 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1783 effective words/s\n",
      "2019-05-12 17:46:14,459 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,462 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2694 effective words/s\n",
      "2019-05-12 17:46:14,462 : INFO : training on a 105 raw words (32 effective words) took 0.0s, 783 effective words/s\n",
      "2019-05-12 17:46:14,463 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,465 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:14,471 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,473 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,474 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,476 : INFO : EPOCH - 1 : training on 21 raw words (4 effective words) took 0.0s, 856 effective words/s\n",
      "2019-05-12 17:46:14,479 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,483 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1527 effective words/s\n",
      "2019-05-12 17:46:14,486 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,488 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,490 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1541 effective words/s\n",
      "2019-05-12 17:46:14,493 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,494 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,495 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,496 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2134 effective words/s\n",
      "2019-05-12 17:46:14,499 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,503 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,504 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1146 effective words/s\n",
      "2019-05-12 17:46:14,505 : INFO : training on a 105 raw words (28 effective words) took 0.0s, 706 effective words/s\n",
      "2019-05-12 17:46:14,506 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,509 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,512 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,514 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,516 : INFO : EPOCH - 1 : training on 21 raw words (8 effective words) took 0.0s, 2398 effective words/s\n",
      "2019-05-12 17:46:14,520 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,521 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,522 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,525 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 1416 effective words/s\n",
      "2019-05-12 17:46:14,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,529 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2265 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:14,536 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,538 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,540 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1359 effective words/s\n",
      "2019-05-12 17:46:14,544 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,545 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,545 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,546 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 2253 effective words/s\n",
      "2019-05-12 17:46:14,546 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 941 effective words/s\n",
      "2019-05-12 17:46:14,547 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,548 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,559 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,560 : INFO : EPOCH - 1 : training on 21 raw words (9 effective words) took 0.0s, 2415 effective words/s\n",
      "2019-05-12 17:46:14,562 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,563 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,563 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,566 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 2225 effective words/s\n",
      "2019-05-12 17:46:14,570 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,571 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,573 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,574 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 1499 effective words/s\n",
      "2019-05-12 17:46:14,577 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,579 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,581 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 1601 effective words/s\n",
      "2019-05-12 17:46:14,606 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,609 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,611 : INFO : EPOCH - 5 : training on 21 raw words (6 effective words) took 0.0s, 1052 effective words/s\n",
      "2019-05-12 17:46:14,614 : INFO : training on a 105 raw words (36 effective words) took 0.1s, 557 effective words/s\n",
      "2019-05-12 17:46:14,616 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,618 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,623 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,625 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,626 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1820 effective words/s\n",
      "2019-05-12 17:46:14,629 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,630 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,631 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,632 : INFO : EPOCH - 2 : training on 21 raw words (9 effective words) took 0.0s, 2584 effective words/s\n",
      "2019-05-12 17:46:14,640 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,641 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,642 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,643 : INFO : EPOCH - 3 : training on 21 raw words (5 effective words) took 0.0s, 1717 effective words/s\n",
      "2019-05-12 17:46:14,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,652 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,653 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,654 : INFO : EPOCH - 4 : training on 21 raw words (6 effective words) took 0.0s, 2069 effective words/s\n",
      "2019-05-12 17:46:14,660 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,661 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,663 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 2103 effective words/s\n",
      "2019-05-12 17:46:14,665 : INFO : training on a 105 raw words (34 effective words) took 0.0s, 733 effective words/s\n",
      "2019-05-12 17:46:14,666 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,667 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,674 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,675 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,679 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,682 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 638 effective words/s\n",
      "2019-05-12 17:46:14,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,687 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,689 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,690 : INFO : EPOCH - 2 : training on 21 raw words (7 effective words) took 0.0s, 2264 effective words/s\n",
      "2019-05-12 17:46:14,693 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,693 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,694 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,695 : INFO : EPOCH - 3 : training on 21 raw words (6 effective words) took 0.0s, 2543 effective words/s\n",
      "2019-05-12 17:46:14,701 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,702 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,703 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 2240 effective words/s\n",
      "2019-05-12 17:46:14,705 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,707 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,707 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,708 : INFO : EPOCH - 5 : training on 21 raw words (5 effective words) took 0.0s, 2043 effective words/s\n",
      "2019-05-12 17:46:14,708 : INFO : training on a 105 raw words (30 effective words) took 0.0s, 740 effective words/s\n",
      "2019-05-12 17:46:14,709 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,710 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:14,714 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,715 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,717 : INFO : EPOCH - 1 : training on 21 raw words (5 effective words) took 0.0s, 1792 effective words/s\n",
      "2019-05-12 17:46:14,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,726 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,727 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,727 : INFO : EPOCH - 2 : training on 21 raw words (5 effective words) took 0.0s, 2078 effective words/s\n",
      "2019-05-12 17:46:14,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,734 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,736 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,738 : INFO : EPOCH - 3 : training on 21 raw words (7 effective words) took 0.0s, 987 effective words/s\n",
      "2019-05-12 17:46:14,741 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,742 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,744 : INFO : EPOCH - 4 : training on 21 raw words (5 effective words) took 0.0s, 1447 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-12 17:46:14,749 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,751 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,754 : INFO : EPOCH - 5 : training on 21 raw words (8 effective words) took 0.0s, 1499 effective words/s\n",
      "2019-05-12 17:46:14,756 : INFO : training on a 105 raw words (30 effective words) took 0.0s, 670 effective words/s\n",
      "2019-05-12 17:46:14,756 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,758 : INFO : training model with 3 workers on 16 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-12 17:46:14,761 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,763 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,764 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,765 : INFO : EPOCH - 1 : training on 21 raw words (6 effective words) took 0.0s, 1456 effective words/s\n",
      "2019-05-12 17:46:14,771 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,772 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,773 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,775 : INFO : EPOCH - 2 : training on 21 raw words (6 effective words) took 0.0s, 1375 effective words/s\n",
      "2019-05-12 17:46:14,781 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,782 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,784 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,786 : INFO : EPOCH - 3 : training on 21 raw words (4 effective words) took 0.0s, 764 effective words/s\n",
      "2019-05-12 17:46:14,790 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,792 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,793 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,795 : INFO : EPOCH - 4 : training on 21 raw words (7 effective words) took 0.0s, 1466 effective words/s\n",
      "2019-05-12 17:46:14,801 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-12 17:46:14,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-12 17:46:14,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-12 17:46:14,807 : INFO : EPOCH - 5 : training on 21 raw words (7 effective words) took 0.0s, 1259 effective words/s\n",
      "2019-05-12 17:46:14,808 : INFO : training on a 105 raw words (30 effective words) took 0.0s, 614 effective words/s\n",
      "2019-05-12 17:46:14,809 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-12 17:46:14,814 : INFO : saving Doc2Vec object under d2v.model, separately None\n",
      "2019-05-12 17:46:14,860 : INFO : saved d2v.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "#model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-13 01:00:12,311 : INFO : loading Doc2Vec object from d2v.model\n",
      "2019-05-13 01:00:12,322 : INFO : loading vocabulary recursively from d2v.model.vocabulary.* with mmap=None\n",
      "2019-05-13 01:00:12,324 : INFO : loading trainables recursively from d2v.model.trainables.* with mmap=None\n",
      "2019-05-13 01:00:12,324 : INFO : loading wv recursively from d2v.model.wv.* with mmap=None\n",
      "2019-05-13 01:00:12,325 : INFO : loading docvecs recursively from d2v.model.docvecs.* with mmap=None\n",
      "2019-05-13 01:00:12,326 : INFO : loaded d2v.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [ 0.02048604  0.00358251 -0.00083494  0.01834486 -0.0229164   0.01360157\n",
      " -0.03484753  0.02889411  0.00952353  0.00208281 -0.02063313  0.02297714\n",
      "  0.01071432  0.01861375 -0.00297782  0.01036483  0.01307047 -0.01389063\n",
      "  0.0011141   0.01237258]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'chatbots']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 0.9942704439163208), ('2', 0.9920396208763123), ('3', 0.9884034991264343)]\n"
     ]
    }
   ],
   "source": [
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05729661 -0.02901328  0.2245761  -0.06806073 -0.23302619 -0.51587665\n",
      " -0.50549406  0.4684786   0.3714211   0.20468582  0.20593771  0.07807345\n",
      " -0.22152928  0.0069683  -0.12964012 -0.16135429 -0.00418597  0.09533923\n",
      "  0.06449696  0.4731226 ]\n"
     ]
    }
   ],
   "source": [
    "# to find vector of doc in training data using tags or in other words,\n",
    "# printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "2019-05-13 09:25:16,242 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coding', 0.8716911673545837), ('chat', 0.8567945957183838), ('love', 0.8542259931564331), ('.', 0.8535782098770142), ('i', 0.8273229002952576), ('chatbots', 0.8253207206726074), ('its', 0.8103268146514893), ('python', 0.7978039979934692), ('they', 0.7966268062591553), ('awesome', 0.7929901480674744)]\n"
     ]
    }
   ],
   "source": [
    "ivec = model.infer_vector(doc_words=test_data, steps=20, alpha=0.025)\n",
    "print(model.most_similar(positive=[ivec], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA TOPIC MODEL WITH 10 TOPICS THIS IS ARBITRARY\n",
    "# LdaMulticore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:27:41,742 : INFO : using asymmetric alpha [0.20349778, 0.15460682, 0.124657474, 0.10442834, 0.08984803, 0.07884031, 0.070235424, 0.06332404, 0.057651002, 0.052910853]\n",
      "2019-05-10 17:27:41,744 : INFO : using symmetric eta at 0.1\n",
      "2019-05-10 17:27:41,763 : INFO : using serial LDA version on this node\n",
      "2019-05-10 17:27:41,876 : INFO : running online LDA training, 10 topics, 10 passes over the supplied corpus of 42094 documents, updating every 3000 documents, evaluating every ~0 documents, iterating 100x with a convergence threshold of 0.001000\n",
      "2019-05-10 17:27:41,887 : INFO : training LDA model using 3 processes\n",
      "2019-05-10 17:27:41,949 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:27:42,209 : INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:27:42,214 : INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:27:42,221 : INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:27:42,226 : INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:27:42,231 : INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:27:42,342 : INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:27:42,440 : INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:27:42,677 : INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:44,612 : INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:44,640 : INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:45,092 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:27:45,223 : INFO : topic #9 (0.053): 0.013*\"the\" + 0.008*\"i\" + 0.007*\"to\" + 0.007*\"and\" + 0.006*\"you\" + 0.006*\"a\" + 0.005*\"s\" + 0.004*\"of\" + 0.004*\"in\" + 0.003*\"is\"\n",
      "2019-05-10 17:27:45,234 : INFO : topic #8 (0.058): 0.014*\"the\" + 0.009*\"i\" + 0.008*\"to\" + 0.008*\"you\" + 0.007*\"a\" + 0.006*\"s\" + 0.006*\"it\" + 0.005*\"and\" + 0.004*\"he\" + 0.004*\"that\"\n",
      "2019-05-10 17:27:45,238 : INFO : topic #2 (0.125): 0.023*\"you\" + 0.022*\"i\" + 0.021*\"the\" + 0.013*\"that\" + 0.013*\"to\" + 0.013*\"a\" + 0.010*\"and\" + 0.010*\"s\" + 0.009*\"what\" + 0.009*\"in\"\n",
      "2019-05-10 17:27:45,245 : INFO : topic #1 (0.155): 0.007*\"you\" + 0.006*\"the\" + 0.006*\"a\" + 0.005*\"to\" + 0.003*\"is\" + 0.003*\"in\" + 0.003*\"and\" + 0.003*\"it\" + 0.003*\"s\" + 0.003*\"that\"\n",
      "2019-05-10 17:27:45,248 : INFO : topic #0 (0.203): 0.007*\"the\" + 0.002*\"and\" + 0.002*\"to\" + 0.002*\"s\" + 0.002*\"in\" + 0.002*\"a\" + 0.002*\"of\" + 0.002*\"on\" + 0.001*\"for\" + 0.001*\"out\"\n",
      "2019-05-10 17:27:45,252 : INFO : topic diff=0.685002, rho=0.125000\n",
      "2019-05-10 17:27:45,255 : INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:46,725 : INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:46,901 : INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:47,231 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:27:47,294 : INFO : topic #9 (0.053): 0.021*\"the\" + 0.012*\"to\" + 0.012*\"i\" + 0.011*\"and\" + 0.009*\"you\" + 0.009*\"a\" + 0.007*\"s\" + 0.006*\"of\" + 0.006*\"in\" + 0.006*\"is\"\n",
      "2019-05-10 17:27:47,297 : INFO : topic #8 (0.058): 0.021*\"the\" + 0.013*\"i\" + 0.012*\"to\" + 0.011*\"you\" + 0.010*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.007*\"and\" + 0.006*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:27:47,300 : INFO : topic #2 (0.125): 0.027*\"you\" + 0.027*\"i\" + 0.025*\"the\" + 0.016*\"that\" + 0.016*\"a\" + 0.015*\"to\" + 0.013*\"and\" + 0.012*\"s\" + 0.011*\"in\" + 0.011*\"what\"\n",
      "2019-05-10 17:27:47,304 : INFO : topic #1 (0.155): 0.011*\"you\" + 0.011*\"the\" + 0.010*\"a\" + 0.008*\"to\" + 0.005*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.005*\"it\" + 0.004*\"s\" + 0.004*\"that\"\n",
      "2019-05-10 17:27:47,306 : INFO : topic #0 (0.203): 0.012*\"the\" + 0.005*\"and\" + 0.004*\"to\" + 0.004*\"s\" + 0.004*\"in\" + 0.003*\"a\" + 0.003*\"of\" + 0.003*\"on\" + 0.003*\"out\" + 0.002*\"for\"\n",
      "2019-05-10 17:27:47,310 : INFO : topic diff=0.377893, rho=0.122169\n",
      "2019-05-10 17:27:47,333 : INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:48,678 : INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:48,836 : INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:49,011 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:27:49,107 : INFO : topic #9 (0.053): 0.023*\"the\" + 0.013*\"to\" + 0.013*\"i\" + 0.012*\"and\" + 0.010*\"you\" + 0.010*\"a\" + 0.008*\"s\" + 0.007*\"of\" + 0.007*\"in\" + 0.006*\"is\"\n",
      "2019-05-10 17:27:49,110 : INFO : topic #8 (0.058): 0.023*\"the\" + 0.015*\"i\" + 0.013*\"to\" + 0.012*\"you\" + 0.011*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.008*\"and\" + 0.007*\"he\" + 0.007*\"that\"\n",
      "2019-05-10 17:27:49,113 : INFO : topic #2 (0.125): 0.028*\"the\" + 0.027*\"you\" + 0.027*\"i\" + 0.016*\"a\" + 0.016*\"to\" + 0.016*\"that\" + 0.013*\"and\" + 0.012*\"s\" + 0.012*\"in\" + 0.011*\"it\"\n",
      "2019-05-10 17:27:49,116 : INFO : topic #1 (0.155): 0.012*\"the\" + 0.012*\"you\" + 0.011*\"a\" + 0.008*\"to\" + 0.006*\"in\" + 0.006*\"is\" + 0.005*\"and\" + 0.005*\"it\" + 0.005*\"i\" + 0.005*\"s\"\n",
      "2019-05-10 17:27:49,119 : INFO : topic #0 (0.203): 0.015*\"the\" + 0.005*\"and\" + 0.005*\"to\" + 0.004*\"in\" + 0.004*\"s\" + 0.004*\"a\" + 0.004*\"of\" + 0.004*\"out\" + 0.003*\"on\" + 0.003*\"for\"\n",
      "2019-05-10 17:27:49,123 : INFO : topic diff=0.227179, rho=0.119523\n",
      "2019-05-10 17:27:49,161 : INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:50,340 : INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:50,680 : INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:50,990 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:27:51,084 : INFO : topic #9 (0.053): 0.025*\"the\" + 0.013*\"to\" + 0.013*\"i\" + 0.012*\"and\" + 0.010*\"a\" + 0.010*\"you\" + 0.008*\"s\" + 0.007*\"of\" + 0.007*\"in\" + 0.006*\"is\"\n",
      "2019-05-10 17:27:51,086 : INFO : topic #8 (0.058): 0.024*\"the\" + 0.014*\"i\" + 0.013*\"to\" + 0.012*\"you\" + 0.011*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.008*\"and\" + 0.007*\"he\" + 0.007*\"that\"\n",
      "2019-05-10 17:27:51,090 : INFO : topic #2 (0.125): 0.030*\"the\" + 0.026*\"you\" + 0.026*\"i\" + 0.017*\"a\" + 0.016*\"to\" + 0.015*\"that\" + 0.014*\"and\" + 0.012*\"in\" + 0.012*\"s\" + 0.011*\"of\"\n",
      "2019-05-10 17:27:51,094 : INFO : topic #1 (0.155): 0.012*\"the\" + 0.012*\"you\" + 0.011*\"a\" + 0.008*\"to\" + 0.006*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.005*\"it\" + 0.005*\"i\" + 0.005*\"s\"\n",
      "2019-05-10 17:27:51,098 : INFO : topic #0 (0.203): 0.015*\"the\" + 0.005*\"and\" + 0.005*\"to\" + 0.004*\"in\" + 0.004*\"s\" + 0.004*\"a\" + 0.004*\"of\" + 0.003*\"on\" + 0.003*\"out\" + 0.003*\"for\"\n",
      "2019-05-10 17:27:51,100 : INFO : topic diff=0.166197, rho=0.117041\n",
      "2019-05-10 17:27:51,103 : INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:52,437 : INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:52,618 : INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:52,948 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:27:53,038 : INFO : topic #9 (0.053): 0.025*\"the\" + 0.013*\"to\" + 0.012*\"and\" + 0.012*\"i\" + 0.010*\"a\" + 0.009*\"you\" + 0.008*\"s\" + 0.007*\"of\" + 0.007*\"in\" + 0.006*\"is\"\n",
      "2019-05-10 17:27:53,041 : INFO : topic #8 (0.058): 0.024*\"the\" + 0.014*\"i\" + 0.013*\"to\" + 0.011*\"you\" + 0.011*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.008*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:27:53,043 : INFO : topic #2 (0.125): 0.032*\"the\" + 0.024*\"you\" + 0.024*\"i\" + 0.017*\"a\" + 0.016*\"to\" + 0.015*\"and\" + 0.014*\"that\" + 0.013*\"in\" + 0.012*\"s\" + 0.012*\"of\"\n",
      "2019-05-10 17:27:53,046 : INFO : topic #1 (0.155): 0.012*\"the\" + 0.012*\"you\" + 0.011*\"a\" + 0.008*\"to\" + 0.006*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.005*\"it\" + 0.005*\"i\" + 0.005*\"s\"\n",
      "2019-05-10 17:27:53,048 : INFO : topic #0 (0.203): 0.016*\"the\" + 0.005*\"and\" + 0.005*\"to\" + 0.004*\"in\" + 0.004*\"s\" + 0.004*\"a\" + 0.004*\"ext\" + 0.004*\"of\" + 0.004*\"out\" + 0.003*\"on\"\n",
      "2019-05-10 17:27:53,051 : INFO : topic diff=0.176041, rho=0.114708\n",
      "2019-05-10 17:27:53,054 : INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:54,249 : INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:54,404 : INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:54,724 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:27:54,821 : INFO : topic #9 (0.053): 0.025*\"the\" + 0.014*\"to\" + 0.012*\"and\" + 0.011*\"i\" + 0.010*\"a\" + 0.009*\"you\" + 0.007*\"s\" + 0.007*\"of\" + 0.007*\"in\" + 0.006*\"is\"\n",
      "2019-05-10 17:27:54,823 : INFO : topic #8 (0.058): 0.024*\"the\" + 0.014*\"i\" + 0.013*\"to\" + 0.011*\"you\" + 0.011*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.008*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:27:54,826 : INFO : topic #2 (0.125): 0.034*\"the\" + 0.023*\"you\" + 0.022*\"i\" + 0.017*\"a\" + 0.016*\"to\" + 0.016*\"and\" + 0.014*\"that\" + 0.013*\"in\" + 0.012*\"of\" + 0.012*\"s\"\n",
      "2019-05-10 17:27:54,829 : INFO : topic #1 (0.155): 0.012*\"the\" + 0.011*\"you\" + 0.011*\"a\" + 0.008*\"to\" + 0.006*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.005*\"it\" + 0.005*\"i\" + 0.005*\"s\"\n",
      "2019-05-10 17:27:54,831 : INFO : topic #0 (0.203): 0.016*\"the\" + 0.005*\"and\" + 0.005*\"to\" + 0.005*\"in\" + 0.004*\"s\" + 0.004*\"a\" + 0.004*\"ext\" + 0.004*\"of\" + 0.004*\"on\" + 0.004*\"out\"\n",
      "2019-05-10 17:27:54,836 : INFO : topic diff=0.167006, rho=0.112509\n",
      "2019-05-10 17:27:54,838 : INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:56,122 : INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:56,174 : INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:56,470 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:27:56,565 : INFO : topic #9 (0.053): 0.026*\"the\" + 0.013*\"to\" + 0.013*\"and\" + 0.011*\"i\" + 0.010*\"a\" + 0.009*\"you\" + 0.007*\"s\" + 0.007*\"of\" + 0.007*\"in\" + 0.006*\"is\"\n",
      "2019-05-10 17:27:56,567 : INFO : topic #8 (0.058): 0.024*\"the\" + 0.014*\"i\" + 0.012*\"to\" + 0.011*\"you\" + 0.010*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.008*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:27:56,570 : INFO : topic #2 (0.125): 0.037*\"the\" + 0.021*\"you\" + 0.020*\"i\" + 0.018*\"a\" + 0.017*\"and\" + 0.016*\"to\" + 0.014*\"in\" + 0.014*\"of\" + 0.013*\"that\" + 0.011*\"s\"\n",
      "2019-05-10 17:27:56,573 : INFO : topic #1 (0.155): 0.012*\"the\" + 0.011*\"you\" + 0.011*\"a\" + 0.008*\"to\" + 0.006*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.005*\"it\" + 0.004*\"i\" + 0.004*\"s\"\n",
      "2019-05-10 17:27:56,576 : INFO : topic #0 (0.203): 0.016*\"the\" + 0.005*\"and\" + 0.005*\"to\" + 0.005*\"ext\" + 0.004*\"in\" + 0.004*\"a\" + 0.004*\"s\" + 0.004*\"of\" + 0.004*\"on\" + 0.003*\"out\"\n",
      "2019-05-10 17:27:56,578 : INFO : topic diff=0.164602, rho=0.110432\n",
      "2019-05-10 17:27:56,590 : INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:57,975 : INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:58,306 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:27:58,405 : INFO : topic #9 (0.053): 0.026*\"the\" + 0.013*\"to\" + 0.013*\"and\" + 0.011*\"i\" + 0.010*\"a\" + 0.008*\"you\" + 0.007*\"of\" + 0.007*\"s\" + 0.007*\"in\" + 0.006*\"is\"\n",
      "2019-05-10 17:27:58,408 : INFO : topic #8 (0.058): 0.023*\"the\" + 0.013*\"i\" + 0.012*\"to\" + 0.011*\"you\" + 0.010*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.008*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:27:58,410 : INFO : topic #2 (0.125): 0.039*\"the\" + 0.019*\"you\" + 0.019*\"a\" + 0.019*\"i\" + 0.018*\"and\" + 0.017*\"to\" + 0.014*\"in\" + 0.014*\"of\" + 0.012*\"that\" + 0.011*\"s\"\n",
      "2019-05-10 17:27:58,413 : INFO : topic #1 (0.155): 0.012*\"the\" + 0.011*\"you\" + 0.011*\"a\" + 0.008*\"to\" + 0.006*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.005*\"it\" + 0.004*\"s\" + 0.004*\"i\"\n",
      "2019-05-10 17:27:58,416 : INFO : topic #0 (0.203): 0.016*\"the\" + 0.005*\"and\" + 0.005*\"to\" + 0.005*\"ext\" + 0.004*\"in\" + 0.004*\"a\" + 0.004*\"s\" + 0.004*\"of\" + 0.003*\"out\" + 0.003*\"on\"\n",
      "2019-05-10 17:27:58,421 : INFO : topic diff=0.164667, rho=0.108465\n",
      "2019-05-10 17:27:58,448 : INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 8\n",
      "2019-05-10 17:27:58,464 : INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:59,744 : INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n",
      "2019-05-10 17:27:59,781 : INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:00,082 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:00,194 : INFO : topic #9 (0.053): 0.026*\"the\" + 0.013*\"to\" + 0.013*\"and\" + 0.011*\"i\" + 0.010*\"a\" + 0.008*\"you\" + 0.007*\"of\" + 0.007*\"s\" + 0.007*\"in\" + 0.006*\"is\"\n",
      "2019-05-10 17:28:00,197 : INFO : topic #8 (0.058): 0.023*\"the\" + 0.013*\"i\" + 0.012*\"to\" + 0.011*\"you\" + 0.010*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.008*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:28:00,199 : INFO : topic #2 (0.125): 0.041*\"the\" + 0.020*\"a\" + 0.020*\"and\" + 0.017*\"you\" + 0.017*\"i\" + 0.016*\"to\" + 0.015*\"of\" + 0.015*\"in\" + 0.012*\"that\" + 0.011*\"s\"\n",
      "2019-05-10 17:28:00,202 : INFO : topic #1 (0.155): 0.011*\"the\" + 0.011*\"you\" + 0.011*\"a\" + 0.008*\"to\" + 0.006*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.004*\"it\" + 0.004*\"s\" + 0.004*\"i\"\n",
      "2019-05-10 17:28:00,204 : INFO : topic #0 (0.203): 0.016*\"the\" + 0.005*\"and\" + 0.005*\"to\" + 0.005*\"ext\" + 0.004*\"in\" + 0.004*\"a\" + 0.004*\"s\" + 0.004*\"of\" + 0.003*\"on\" + 0.003*\"out\"\n",
      "2019-05-10 17:28:00,207 : INFO : topic diff=0.175045, rho=0.106600\n",
      "2019-05-10 17:28:00,224 : INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:01,288 : INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:01,667 : INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:01,835 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:01,937 : INFO : topic #9 (0.053): 0.025*\"the\" + 0.013*\"to\" + 0.013*\"and\" + 0.010*\"i\" + 0.010*\"a\" + 0.008*\"you\" + 0.007*\"of\" + 0.007*\"in\" + 0.007*\"s\" + 0.006*\"is\"\n",
      "2019-05-10 17:28:01,940 : INFO : topic #8 (0.058): 0.023*\"the\" + 0.013*\"i\" + 0.012*\"to\" + 0.011*\"you\" + 0.010*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.007*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:28:01,944 : INFO : topic #2 (0.125): 0.044*\"the\" + 0.021*\"a\" + 0.021*\"and\" + 0.017*\"to\" + 0.016*\"you\" + 0.016*\"of\" + 0.016*\"i\" + 0.015*\"in\" + 0.011*\"that\" + 0.010*\"s\"\n",
      "2019-05-10 17:28:01,947 : INFO : topic #1 (0.155): 0.011*\"the\" + 0.011*\"you\" + 0.010*\"a\" + 0.007*\"to\" + 0.006*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.004*\"it\" + 0.004*\"s\" + 0.004*\"i\"\n",
      "2019-05-10 17:28:01,950 : INFO : topic #0 (0.203): 0.015*\"the\" + 0.005*\"and\" + 0.005*\"to\" + 0.004*\"ext\" + 0.004*\"in\" + 0.004*\"a\" + 0.004*\"s\" + 0.004*\"of\" + 0.003*\"out\" + 0.003*\"on\"\n",
      "2019-05-10 17:28:01,953 : INFO : topic diff=0.185859, rho=0.104828\n",
      "2019-05-10 17:28:01,969 : INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:02,990 : INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:03,099 : INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:28:03,424 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:03,490 : INFO : topic #9 (0.053): 0.025*\"the\" + 0.013*\"to\" + 0.012*\"and\" + 0.010*\"i\" + 0.010*\"a\" + 0.008*\"you\" + 0.007*\"of\" + 0.007*\"in\" + 0.007*\"s\" + 0.006*\"is\"\n",
      "2019-05-10 17:28:03,492 : INFO : topic #8 (0.058): 0.023*\"the\" + 0.013*\"i\" + 0.012*\"to\" + 0.011*\"you\" + 0.010*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.007*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:28:03,495 : INFO : topic #2 (0.125): 0.046*\"the\" + 0.022*\"a\" + 0.022*\"and\" + 0.017*\"of\" + 0.017*\"to\" + 0.016*\"in\" + 0.015*\"you\" + 0.015*\"i\" + 0.010*\"that\" + 0.010*\"s\"\n",
      "2019-05-10 17:28:03,498 : INFO : topic #1 (0.155): 0.011*\"the\" + 0.010*\"you\" + 0.010*\"a\" + 0.007*\"to\" + 0.006*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.004*\"it\" + 0.004*\"s\" + 0.004*\"i\"\n",
      "2019-05-10 17:28:03,500 : INFO : topic #0 (0.203): 0.015*\"the\" + 0.005*\"and\" + 0.004*\"to\" + 0.004*\"ext\" + 0.004*\"in\" + 0.004*\"a\" + 0.004*\"s\" + 0.004*\"of\" + 0.003*\"out\" + 0.003*\"on\"\n",
      "2019-05-10 17:28:03,503 : INFO : topic diff=0.182729, rho=0.103142\n",
      "2019-05-10 17:28:03,521 : INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:04,450 : INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:04,965 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:05,033 : INFO : topic #9 (0.053): 0.025*\"the\" + 0.012*\"to\" + 0.012*\"and\" + 0.010*\"i\" + 0.010*\"a\" + 0.008*\"you\" + 0.007*\"of\" + 0.007*\"in\" + 0.007*\"s\" + 0.006*\"is\"\n",
      "2019-05-10 17:28:05,035 : INFO : topic #8 (0.058): 0.022*\"the\" + 0.013*\"i\" + 0.012*\"to\" + 0.010*\"you\" + 0.010*\"a\" + 0.009*\"s\" + 0.008*\"it\" + 0.007*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:28:05,038 : INFO : topic #2 (0.125): 0.049*\"the\" + 0.023*\"and\" + 0.023*\"a\" + 0.018*\"of\" + 0.017*\"to\" + 0.016*\"in\" + 0.013*\"you\" + 0.013*\"i\" + 0.011*\"is\" + 0.010*\"s\"\n",
      "2019-05-10 17:28:05,041 : INFO : topic #1 (0.155): 0.011*\"the\" + 0.010*\"you\" + 0.010*\"a\" + 0.007*\"to\" + 0.005*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.004*\"it\" + 0.004*\"s\" + 0.004*\"i\"\n",
      "2019-05-10 17:28:05,044 : INFO : topic #0 (0.203): 0.015*\"the\" + 0.005*\"and\" + 0.004*\"to\" + 0.004*\"ext\" + 0.004*\"in\" + 0.004*\"a\" + 0.004*\"s\" + 0.004*\"of\" + 0.003*\"out\" + 0.003*\"on\"\n",
      "2019-05-10 17:28:05,047 : INFO : topic diff=0.188284, rho=0.101535\n",
      "2019-05-10 17:28:06,456 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:06,518 : INFO : topic #9 (0.053): 0.024*\"the\" + 0.012*\"to\" + 0.012*\"and\" + 0.010*\"i\" + 0.010*\"a\" + 0.008*\"you\" + 0.007*\"of\" + 0.007*\"in\" + 0.007*\"s\" + 0.006*\"is\"\n",
      "2019-05-10 17:28:06,521 : INFO : topic #8 (0.058): 0.022*\"the\" + 0.012*\"i\" + 0.012*\"to\" + 0.010*\"you\" + 0.010*\"a\" + 0.008*\"s\" + 0.007*\"it\" + 0.007*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:28:06,523 : INFO : topic #2 (0.125): 0.053*\"the\" + 0.025*\"and\" + 0.024*\"a\" + 0.018*\"of\" + 0.018*\"to\" + 0.017*\"in\" + 0.012*\"is\" + 0.012*\"you\" + 0.011*\"i\" + 0.010*\"his\"\n",
      "2019-05-10 17:28:06,526 : INFO : topic #1 (0.155): 0.010*\"the\" + 0.010*\"you\" + 0.010*\"a\" + 0.007*\"to\" + 0.005*\"in\" + 0.005*\"is\" + 0.005*\"and\" + 0.004*\"it\" + 0.004*\"s\" + 0.004*\"i\"\n",
      "2019-05-10 17:28:06,529 : INFO : topic #0 (0.203): 0.014*\"the\" + 0.005*\"and\" + 0.004*\"to\" + 0.004*\"ext\" + 0.004*\"in\" + 0.004*\"a\" + 0.004*\"s\" + 0.003*\"of\" + 0.003*\"out\" + 0.003*\"on\"\n",
      "2019-05-10 17:28:06,532 : INFO : topic diff=0.195294, rho=0.100000\n",
      "2019-05-10 17:28:07,843 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:07,889 : INFO : topic #9 (0.053): 0.024*\"the\" + 0.012*\"to\" + 0.012*\"and\" + 0.010*\"i\" + 0.009*\"a\" + 0.008*\"you\" + 0.007*\"of\" + 0.007*\"in\" + 0.006*\"s\" + 0.005*\"is\"\n",
      "2019-05-10 17:28:07,891 : INFO : topic #8 (0.058): 0.022*\"the\" + 0.012*\"i\" + 0.011*\"to\" + 0.010*\"you\" + 0.009*\"a\" + 0.008*\"s\" + 0.007*\"it\" + 0.007*\"and\" + 0.007*\"he\" + 0.006*\"that\"\n",
      "2019-05-10 17:28:07,893 : INFO : topic #2 (0.125): 0.054*\"the\" + 0.026*\"and\" + 0.024*\"a\" + 0.019*\"of\" + 0.018*\"to\" + 0.017*\"in\" + 0.012*\"is\" + 0.011*\"his\" + 0.011*\"you\" + 0.010*\"i\"\n",
      "2019-05-10 17:28:07,894 : INFO : topic #1 (0.155): 0.010*\"the\" + 0.010*\"you\" + 0.009*\"a\" + 0.007*\"to\" + 0.005*\"in\" + 0.005*\"is\" + 0.004*\"and\" + 0.004*\"it\" + 0.004*\"s\" + 0.004*\"i\"\n",
      "2019-05-10 17:28:07,896 : INFO : topic #0 (0.203): 0.014*\"the\" + 0.005*\"ext\" + 0.005*\"and\" + 0.004*\"to\" + 0.004*\"int\" + 0.004*\"in\" + 0.004*\"a\" + 0.004*\"s\" + 0.003*\"ots\" + 0.003*\"of\"\n",
      "2019-05-10 17:28:07,898 : INFO : topic diff=0.196396, rho=0.098533\n",
      "2019-05-10 17:28:07,901 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:28:07,999 : INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:28:08,000 : INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:28:08,002 : INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:28:08,003 : INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:28:08,005 : INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:28:08,104 : INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:28:08,254 : INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:08,382 : INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:09,484 : INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:09,633 : INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:09,815 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:09,875 : INFO : topic #9 (0.053): 0.024*\"the\" + 0.012*\"and\" + 0.012*\"to\" + 0.009*\"a\" + 0.009*\"i\" + 0.007*\"you\" + 0.007*\"of\" + 0.006*\"in\" + 0.006*\"s\" + 0.005*\"is\"\n",
      "2019-05-10 17:28:09,877 : INFO : topic #8 (0.058): 0.021*\"the\" + 0.012*\"i\" + 0.011*\"to\" + 0.010*\"you\" + 0.009*\"a\" + 0.008*\"s\" + 0.007*\"it\" + 0.007*\"and\" + 0.007*\"he\" + 0.006*\"of\"\n",
      "2019-05-10 17:28:09,879 : INFO : topic #2 (0.125): 0.056*\"the\" + 0.026*\"and\" + 0.024*\"a\" + 0.019*\"of\" + 0.018*\"to\" + 0.017*\"in\" + 0.013*\"is\" + 0.011*\"his\" + 0.010*\"he\" + 0.010*\"you\"\n",
      "2019-05-10 17:28:09,881 : INFO : topic #1 (0.155): 0.010*\"the\" + 0.009*\"you\" + 0.009*\"a\" + 0.007*\"to\" + 0.005*\"in\" + 0.005*\"is\" + 0.004*\"and\" + 0.004*\"s\" + 0.004*\"it\" + 0.004*\"i\"\n",
      "2019-05-10 17:28:09,884 : INFO : topic #0 (0.203): 0.014*\"the\" + 0.005*\"ext\" + 0.005*\"and\" + 0.004*\"int\" + 0.004*\"to\" + 0.004*\"in\" + 0.004*\"s\" + 0.004*\"a\" + 0.003*\"ots\" + 0.003*\"on\"\n",
      "2019-05-10 17:28:09,888 : INFO : topic diff=0.198351, rho=0.096631\n",
      "2019-05-10 17:28:09,891 : INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:10,790 : INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:11,180 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:11,274 : INFO : topic #9 (0.053): 0.023*\"the\" + 0.012*\"and\" + 0.012*\"to\" + 0.009*\"a\" + 0.009*\"i\" + 0.007*\"you\" + 0.006*\"in\" + 0.006*\"of\" + 0.006*\"s\" + 0.005*\"is\"\n",
      "2019-05-10 17:28:11,277 : INFO : topic #8 (0.058): 0.021*\"the\" + 0.012*\"i\" + 0.011*\"to\" + 0.009*\"you\" + 0.009*\"a\" + 0.008*\"s\" + 0.007*\"it\" + 0.007*\"and\" + 0.006*\"he\" + 0.005*\"nantz\"\n",
      "2019-05-10 17:28:11,279 : INFO : topic #2 (0.125): 0.057*\"the\" + 0.027*\"and\" + 0.025*\"a\" + 0.020*\"of\" + 0.018*\"to\" + 0.018*\"in\" + 0.013*\"is\" + 0.012*\"his\" + 0.010*\"he\" + 0.010*\"s\"\n",
      "2019-05-10 17:28:11,281 : INFO : topic #1 (0.155): 0.010*\"the\" + 0.009*\"a\" + 0.009*\"you\" + 0.006*\"to\" + 0.005*\"in\" + 0.005*\"is\" + 0.004*\"and\" + 0.004*\"s\" + 0.004*\"it\" + 0.004*\"i\"\n",
      "2019-05-10 17:28:11,284 : INFO : topic #0 (0.203): 0.013*\"the\" + 0.005*\"ext\" + 0.005*\"and\" + 0.004*\"int\" + 0.004*\"to\" + 0.004*\"in\" + 0.004*\"s\" + 0.003*\"a\" + 0.003*\"ots\" + 0.003*\"on\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:28:11,286 : INFO : topic diff=0.210838, rho=0.096631\n",
      "2019-05-10 17:28:11,289 : INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:11,291 : INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:12,230 : INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:12,416 : INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:12,611 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:12,704 : INFO : topic #9 (0.053): 0.023*\"the\" + 0.012*\"and\" + 0.012*\"to\" + 0.009*\"a\" + 0.009*\"i\" + 0.007*\"you\" + 0.006*\"in\" + 0.006*\"of\" + 0.006*\"s\" + 0.005*\"is\"\n",
      "2019-05-10 17:28:12,712 : INFO : topic #8 (0.058): 0.021*\"the\" + 0.011*\"i\" + 0.011*\"to\" + 0.009*\"you\" + 0.009*\"a\" + 0.008*\"s\" + 0.007*\"it\" + 0.007*\"and\" + 0.006*\"he\" + 0.005*\"nantz\"\n",
      "2019-05-10 17:28:12,716 : INFO : topic #2 (0.125): 0.059*\"the\" + 0.028*\"and\" + 0.027*\"a\" + 0.020*\"of\" + 0.018*\"to\" + 0.018*\"in\" + 0.013*\"is\" + 0.012*\"his\" + 0.010*\"he\" + 0.010*\"her\"\n",
      "2019-05-10 17:28:12,725 : INFO : topic #1 (0.155): 0.009*\"the\" + 0.009*\"a\" + 0.009*\"you\" + 0.006*\"to\" + 0.005*\"in\" + 0.004*\"is\" + 0.004*\"and\" + 0.004*\"s\" + 0.004*\"it\" + 0.003*\"i\"\n",
      "2019-05-10 17:28:12,728 : INFO : topic #0 (0.203): 0.013*\"the\" + 0.009*\"int\" + 0.007*\"ext\" + 0.006*\"day\" + 0.005*\"s\" + 0.004*\"and\" + 0.004*\"to\" + 0.004*\"in\" + 0.003*\"a\" + 0.003*\"ots\"\n",
      "2019-05-10 17:28:12,731 : INFO : topic diff=0.208273, rho=0.096631\n",
      "2019-05-10 17:28:12,734 : INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:13,703 : INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:13,734 : INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:14,009 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:14,100 : INFO : topic #9 (0.053): 0.023*\"the\" + 0.012*\"and\" + 0.011*\"to\" + 0.009*\"a\" + 0.008*\"i\" + 0.007*\"you\" + 0.006*\"in\" + 0.006*\"of\" + 0.006*\"s\" + 0.005*\"is\"\n",
      "2019-05-10 17:28:14,103 : INFO : topic #8 (0.058): 0.020*\"the\" + 0.011*\"i\" + 0.011*\"to\" + 0.009*\"you\" + 0.009*\"a\" + 0.008*\"s\" + 0.007*\"it\" + 0.007*\"and\" + 0.006*\"he\" + 0.005*\"nantz\"\n",
      "2019-05-10 17:28:14,105 : INFO : topic #2 (0.125): 0.060*\"the\" + 0.028*\"and\" + 0.027*\"a\" + 0.020*\"of\" + 0.019*\"to\" + 0.018*\"in\" + 0.013*\"is\" + 0.012*\"his\" + 0.011*\"he\" + 0.010*\"her\"\n",
      "2019-05-10 17:28:14,107 : INFO : topic #1 (0.155): 0.009*\"a\" + 0.009*\"the\" + 0.009*\"you\" + 0.006*\"to\" + 0.005*\"in\" + 0.004*\"revision\" + 0.004*\"is\" + 0.004*\"and\" + 0.003*\"s\" + 0.003*\"it\"\n",
      "2019-05-10 17:28:14,111 : INFO : topic #0 (0.203): 0.012*\"the\" + 0.010*\"int\" + 0.008*\"ext\" + 0.005*\"day\" + 0.005*\"s\" + 0.004*\"and\" + 0.004*\"in\" + 0.004*\"to\" + 0.003*\"a\" + 0.003*\"on\"\n",
      "2019-05-10 17:28:14,113 : INFO : topic diff=0.215324, rho=0.096631\n",
      "2019-05-10 17:28:14,116 : INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:15,063 : INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:15,262 : INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:15,492 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:15,556 : INFO : topic #9 (0.053): 0.022*\"the\" + 0.012*\"and\" + 0.011*\"to\" + 0.009*\"a\" + 0.008*\"i\" + 0.006*\"you\" + 0.006*\"in\" + 0.006*\"of\" + 0.006*\"s\" + 0.005*\"is\"\n",
      "2019-05-10 17:28:15,558 : INFO : topic #8 (0.058): 0.020*\"the\" + 0.011*\"i\" + 0.010*\"to\" + 0.009*\"you\" + 0.009*\"a\" + 0.008*\"s\" + 0.007*\"it\" + 0.006*\"and\" + 0.006*\"he\" + 0.005*\"nantz\"\n",
      "2019-05-10 17:28:15,561 : INFO : topic #2 (0.125): 0.061*\"the\" + 0.028*\"and\" + 0.027*\"a\" + 0.020*\"of\" + 0.019*\"to\" + 0.018*\"in\" + 0.013*\"his\" + 0.013*\"is\" + 0.011*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:15,564 : INFO : topic #1 (0.155): 0.009*\"a\" + 0.009*\"the\" + 0.008*\"you\" + 0.006*\"to\" + 0.005*\"in\" + 0.004*\"revision\" + 0.004*\"is\" + 0.004*\"and\" + 0.003*\"s\" + 0.003*\"it\"\n",
      "2019-05-10 17:28:15,566 : INFO : topic #0 (0.203): 0.014*\"ext\" + 0.013*\"the\" + 0.011*\"int\" + 0.007*\"harry\" + 0.007*\"durrance\" + 0.006*\"day\" + 0.005*\"s\" + 0.004*\"and\" + 0.004*\"in\" + 0.004*\"to\"\n",
      "2019-05-10 17:28:15,569 : INFO : topic diff=0.227372, rho=0.096631\n",
      "2019-05-10 17:28:15,572 : INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:16,389 : INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:16,504 : INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:16,803 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:16,866 : INFO : topic #9 (0.053): 0.020*\"the\" + 0.013*\"to\" + 0.011*\"and\" + 0.008*\"a\" + 0.008*\"b\" + 0.007*\"i\" + 0.007*\"elwood\" + 0.006*\"g\" + 0.006*\"as\" + 0.006*\"in\"\n",
      "2019-05-10 17:28:16,869 : INFO : topic #8 (0.058): 0.019*\"the\" + 0.010*\"i\" + 0.010*\"to\" + 0.009*\"you\" + 0.009*\"a\" + 0.007*\"s\" + 0.006*\"it\" + 0.006*\"and\" + 0.006*\"he\" + 0.005*\"of\"\n",
      "2019-05-10 17:28:16,871 : INFO : topic #2 (0.125): 0.061*\"the\" + 0.028*\"and\" + 0.027*\"a\" + 0.020*\"of\" + 0.019*\"to\" + 0.018*\"in\" + 0.013*\"his\" + 0.012*\"is\" + 0.011*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:16,873 : INFO : topic #1 (0.155): 0.009*\"a\" + 0.008*\"the\" + 0.008*\"you\" + 0.006*\"to\" + 0.005*\"in\" + 0.004*\"revision\" + 0.004*\"is\" + 0.004*\"and\" + 0.003*\"s\" + 0.003*\"it\"\n",
      "2019-05-10 17:28:16,875 : INFO : topic #0 (0.203): 0.014*\"ext\" + 0.012*\"the\" + 0.011*\"int\" + 0.007*\"harry\" + 0.006*\"durrance\" + 0.006*\"day\" + 0.005*\"s\" + 0.004*\"and\" + 0.004*\"in\" + 0.003*\"to\"\n",
      "2019-05-10 17:28:16,877 : INFO : topic diff=0.219294, rho=0.096631\n",
      "2019-05-10 17:28:16,880 : INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:17,650 : INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:17,709 : INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:18,010 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:18,086 : INFO : topic #9 (0.053): 0.020*\"the\" + 0.013*\"to\" + 0.011*\"and\" + 0.008*\"a\" + 0.007*\"b\" + 0.007*\"i\" + 0.007*\"elwood\" + 0.006*\"g\" + 0.006*\"as\" + 0.006*\"in\"\n",
      "2019-05-10 17:28:18,089 : INFO : topic #8 (0.058): 0.019*\"the\" + 0.010*\"i\" + 0.010*\"to\" + 0.008*\"you\" + 0.008*\"a\" + 0.007*\"s\" + 0.006*\"it\" + 0.006*\"and\" + 0.006*\"he\" + 0.005*\"of\"\n",
      "2019-05-10 17:28:18,091 : INFO : topic #2 (0.125): 0.063*\"the\" + 0.028*\"and\" + 0.027*\"a\" + 0.021*\"of\" + 0.019*\"to\" + 0.018*\"in\" + 0.013*\"his\" + 0.013*\"is\" + 0.012*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:18,093 : INFO : topic #1 (0.155): 0.008*\"a\" + 0.008*\"the\" + 0.008*\"you\" + 0.005*\"to\" + 0.004*\"in\" + 0.004*\"revision\" + 0.004*\"is\" + 0.004*\"and\" + 0.003*\"s\" + 0.003*\"it\"\n",
      "2019-05-10 17:28:18,097 : INFO : topic #0 (0.203): 0.015*\"ext\" + 0.012*\"the\" + 0.012*\"int\" + 0.007*\"harry\" + 0.006*\"day\" + 0.006*\"durrance\" + 0.005*\"frodo\" + 0.005*\"s\" + 0.004*\"night\" + 0.004*\"and\"\n",
      "2019-05-10 17:28:18,100 : INFO : topic diff=0.210369, rho=0.096631\n",
      "2019-05-10 17:28:18,103 : INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:18,996 : INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:19,373 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:19,455 : INFO : topic #9 (0.053): 0.019*\"the\" + 0.012*\"to\" + 0.010*\"and\" + 0.008*\"a\" + 0.008*\"b\" + 0.007*\"i\" + 0.007*\"g\" + 0.006*\"elwood\" + 0.006*\"in\" + 0.006*\"as\"\n",
      "2019-05-10 17:28:19,458 : INFO : topic #8 (0.058): 0.018*\"the\" + 0.010*\"i\" + 0.010*\"to\" + 0.008*\"a\" + 0.008*\"you\" + 0.007*\"s\" + 0.006*\"it\" + 0.006*\"and\" + 0.006*\"he\" + 0.005*\"that\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:28:19,463 : INFO : topic #2 (0.125): 0.063*\"the\" + 0.029*\"and\" + 0.027*\"a\" + 0.022*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.013*\"his\" + 0.013*\"is\" + 0.011*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:19,466 : INFO : topic #1 (0.155): 0.008*\"a\" + 0.008*\"the\" + 0.007*\"you\" + 0.006*\"goldenrod\" + 0.005*\"to\" + 0.004*\"in\" + 0.004*\"is\" + 0.004*\"revision\" + 0.004*\"and\" + 0.003*\"s\"\n",
      "2019-05-10 17:28:19,468 : INFO : topic #0 (0.203): 0.014*\"ext\" + 0.011*\"int\" + 0.011*\"the\" + 0.007*\"harry\" + 0.006*\"day\" + 0.006*\"durrance\" + 0.005*\"revised\" + 0.005*\"frodo\" + 0.005*\"draft\" + 0.004*\"s\"\n",
      "2019-05-10 17:28:19,473 : INFO : topic diff=0.210974, rho=0.096631\n",
      "2019-05-10 17:28:19,476 : INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:19,477 : INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:25,373 : INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:25,675 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:25,763 : INFO : topic #9 (0.053): 0.019*\"the\" + 0.012*\"to\" + 0.010*\"and\" + 0.008*\"b\" + 0.008*\"a\" + 0.007*\"i\" + 0.007*\"g\" + 0.006*\"elwood\" + 0.006*\"in\" + 0.006*\"as\"\n",
      "2019-05-10 17:28:25,767 : INFO : topic #8 (0.058): 0.018*\"the\" + 0.010*\"i\" + 0.009*\"to\" + 0.008*\"a\" + 0.008*\"you\" + 0.007*\"s\" + 0.006*\"it\" + 0.006*\"and\" + 0.005*\"he\" + 0.004*\"of\"\n",
      "2019-05-10 17:28:25,770 : INFO : topic #2 (0.125): 0.063*\"the\" + 0.029*\"and\" + 0.028*\"a\" + 0.022*\"of\" + 0.019*\"to\" + 0.018*\"in\" + 0.013*\"is\" + 0.013*\"his\" + 0.012*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:25,772 : INFO : topic #1 (0.155): 0.008*\"a\" + 0.007*\"the\" + 0.007*\"you\" + 0.005*\"goldenrod\" + 0.005*\"to\" + 0.004*\"in\" + 0.004*\"is\" + 0.004*\"revision\" + 0.003*\"and\" + 0.003*\"s\"\n",
      "2019-05-10 17:28:25,775 : INFO : topic #0 (0.203): 0.014*\"ext\" + 0.011*\"int\" + 0.011*\"the\" + 0.006*\"harry\" + 0.006*\"day\" + 0.005*\"durrance\" + 0.005*\"revised\" + 0.005*\"frodo\" + 0.004*\"draft\" + 0.004*\"s\"\n",
      "2019-05-10 17:28:25,778 : INFO : topic diff=0.210133, rho=0.096631\n",
      "2019-05-10 17:28:25,781 : INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:25,782 : INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:26,685 : INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:26,749 : INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:27,031 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:27,126 : INFO : topic #9 (0.053): 0.018*\"the\" + 0.012*\"to\" + 0.010*\"and\" + 0.008*\"b\" + 0.007*\"terrance\" + 0.007*\"a\" + 0.006*\"i\" + 0.006*\"g\" + 0.006*\"wendy\" + 0.006*\"elwood\"\n",
      "2019-05-10 17:28:27,143 : INFO : topic #8 (0.058): 0.017*\"the\" + 0.009*\"i\" + 0.009*\"to\" + 0.008*\"a\" + 0.008*\"you\" + 0.007*\"s\" + 0.006*\"it\" + 0.005*\"and\" + 0.005*\"he\" + 0.004*\"that\"\n",
      "2019-05-10 17:28:27,149 : INFO : topic #2 (0.125): 0.064*\"the\" + 0.029*\"and\" + 0.028*\"a\" + 0.022*\"of\" + 0.019*\"to\" + 0.018*\"in\" + 0.013*\"is\" + 0.013*\"his\" + 0.012*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:27,152 : INFO : topic #1 (0.155): 0.008*\"a\" + 0.007*\"the\" + 0.007*\"you\" + 0.005*\"jamal\" + 0.005*\"goldenrod\" + 0.005*\"to\" + 0.004*\"in\" + 0.004*\"is\" + 0.003*\"revision\" + 0.003*\"and\"\n",
      "2019-05-10 17:28:27,155 : INFO : topic #0 (0.203): 0.013*\"ext\" + 0.011*\"int\" + 0.010*\"the\" + 0.006*\"harry\" + 0.006*\"day\" + 0.005*\"durrance\" + 0.005*\"revised\" + 0.005*\"frodo\" + 0.004*\"draft\" + 0.004*\"s\"\n",
      "2019-05-10 17:28:27,160 : INFO : topic diff=0.224389, rho=0.096631\n",
      "2019-05-10 17:28:27,164 : INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:27,971 : INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:28,076 : INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:28,361 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:28,458 : INFO : topic #9 (0.053): 0.018*\"the\" + 0.011*\"to\" + 0.010*\"and\" + 0.008*\"b\" + 0.007*\"terrance\" + 0.007*\"a\" + 0.006*\"g\" + 0.006*\"i\" + 0.006*\"wendy\" + 0.005*\"elwood\"\n",
      "2019-05-10 17:28:28,463 : INFO : topic #8 (0.058): 0.017*\"the\" + 0.009*\"i\" + 0.009*\"to\" + 0.007*\"a\" + 0.007*\"you\" + 0.006*\"s\" + 0.005*\"it\" + 0.005*\"and\" + 0.005*\"he\" + 0.004*\"that\"\n",
      "2019-05-10 17:28:28,467 : INFO : topic #2 (0.125): 0.064*\"the\" + 0.029*\"and\" + 0.028*\"a\" + 0.022*\"of\" + 0.019*\"to\" + 0.018*\"in\" + 0.014*\"is\" + 0.013*\"his\" + 0.012*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:28,471 : INFO : topic #1 (0.155): 0.008*\"a\" + 0.007*\"the\" + 0.007*\"you\" + 0.005*\"jamal\" + 0.005*\"goldenrod\" + 0.005*\"to\" + 0.004*\"in\" + 0.004*\"is\" + 0.003*\"and\" + 0.003*\"revision\"\n",
      "2019-05-10 17:28:28,483 : INFO : topic #0 (0.203): 0.012*\"ext\" + 0.010*\"the\" + 0.010*\"int\" + 0.006*\"harry\" + 0.005*\"day\" + 0.005*\"durrance\" + 0.005*\"revised\" + 0.004*\"frodo\" + 0.004*\"draft\" + 0.004*\"s\"\n",
      "2019-05-10 17:28:28,492 : INFO : topic diff=0.213919, rho=0.096631\n",
      "2019-05-10 17:28:28,496 : INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:28,987 : INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:29,688 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:29,751 : INFO : topic #9 (0.053): 0.017*\"the\" + 0.011*\"to\" + 0.010*\"and\" + 0.009*\"b\" + 0.007*\"a\" + 0.007*\"terrance\" + 0.006*\"g\" + 0.006*\"i\" + 0.006*\"wendy\" + 0.005*\"elwood\"\n",
      "2019-05-10 17:28:29,754 : INFO : topic #8 (0.058): 0.016*\"the\" + 0.009*\"i\" + 0.008*\"to\" + 0.007*\"a\" + 0.007*\"you\" + 0.006*\"s\" + 0.005*\"it\" + 0.005*\"and\" + 0.005*\"he\" + 0.004*\"of\"\n",
      "2019-05-10 17:28:29,758 : INFO : topic #2 (0.125): 0.066*\"the\" + 0.029*\"and\" + 0.028*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.014*\"is\" + 0.013*\"his\" + 0.012*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:29,759 : INFO : topic #1 (0.155): 0.007*\"a\" + 0.007*\"the\" + 0.006*\"you\" + 0.004*\"jamal\" + 0.004*\"to\" + 0.004*\"goldenrod\" + 0.004*\"in\" + 0.003*\"is\" + 0.003*\"and\" + 0.003*\"revision\"\n",
      "2019-05-10 17:28:29,763 : INFO : topic #0 (0.203): 0.012*\"ext\" + 0.010*\"int\" + 0.010*\"the\" + 0.008*\"frodo\" + 0.005*\"harry\" + 0.005*\"day\" + 0.005*\"durrance\" + 0.004*\"revised\" + 0.004*\"gandalf\" + 0.004*\"draft\"\n",
      "2019-05-10 17:28:29,766 : INFO : topic diff=0.206602, rho=0.096631\n",
      "2019-05-10 17:28:30,904 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:30,969 : INFO : topic #9 (0.053): 0.017*\"the\" + 0.010*\"to\" + 0.009*\"and\" + 0.008*\"b\" + 0.007*\"a\" + 0.007*\"terrance\" + 0.006*\"g\" + 0.006*\"wendy\" + 0.006*\"i\" + 0.005*\"elwood\"\n",
      "2019-05-10 17:28:30,971 : INFO : topic #8 (0.058): 0.015*\"the\" + 0.008*\"i\" + 0.008*\"to\" + 0.007*\"a\" + 0.007*\"you\" + 0.006*\"s\" + 0.005*\"it\" + 0.005*\"and\" + 0.005*\"he\" + 0.004*\"of\"\n",
      "2019-05-10 17:28:30,974 : INFO : topic #2 (0.125): 0.067*\"the\" + 0.030*\"and\" + 0.028*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.014*\"is\" + 0.013*\"his\" + 0.012*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:30,976 : INFO : topic #1 (0.155): 0.007*\"a\" + 0.006*\"the\" + 0.006*\"you\" + 0.005*\"scully\" + 0.004*\"sie\" + 0.004*\"in\" + 0.004*\"jamal\" + 0.004*\"to\" + 0.004*\"goldenrod\" + 0.003*\"rowan\"\n",
      "2019-05-10 17:28:30,979 : INFO : topic #0 (0.203): 0.011*\"ext\" + 0.009*\"int\" + 0.009*\"the\" + 0.008*\"frodo\" + 0.005*\"harry\" + 0.005*\"day\" + 0.004*\"durrance\" + 0.004*\"revised\" + 0.004*\"gandalf\" + 0.004*\"draft\"\n",
      "2019-05-10 17:28:30,983 : INFO : topic diff=0.207351, rho=0.096631\n",
      "2019-05-10 17:28:32,142 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:32,187 : INFO : topic #9 (0.053): 0.016*\"the\" + 0.010*\"to\" + 0.009*\"and\" + 0.008*\"b\" + 0.006*\"a\" + 0.006*\"g\" + 0.006*\"terrance\" + 0.006*\"wendy\" + 0.005*\"i\" + 0.005*\"in\"\n",
      "2019-05-10 17:28:32,189 : INFO : topic #8 (0.058): 0.015*\"the\" + 0.008*\"i\" + 0.008*\"to\" + 0.007*\"a\" + 0.007*\"you\" + 0.006*\"s\" + 0.005*\"it\" + 0.005*\"mcu\" + 0.005*\"and\" + 0.005*\"he\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:28:32,191 : INFO : topic #2 (0.125): 0.067*\"the\" + 0.030*\"and\" + 0.028*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.015*\"is\" + 0.014*\"his\" + 0.013*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:32,193 : INFO : topic #1 (0.155): 0.006*\"a\" + 0.006*\"the\" + 0.006*\"you\" + 0.005*\"scully\" + 0.004*\"sie\" + 0.004*\"jamal\" + 0.004*\"to\" + 0.004*\"in\" + 0.004*\"goldenrod\" + 0.003*\"rowan\"\n",
      "2019-05-10 17:28:32,194 : INFO : topic #0 (0.203): 0.013*\"ext\" + 0.012*\"ots\" + 0.011*\"int\" + 0.009*\"the\" + 0.007*\"frodo\" + 0.005*\"day\" + 0.005*\"harry\" + 0.004*\"durrance\" + 0.004*\"revised\" + 0.004*\"night\"\n",
      "2019-05-10 17:28:32,196 : INFO : topic diff=0.203009, rho=0.096631\n",
      "2019-05-10 17:28:32,199 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:28:32,284 : INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:28:32,285 : INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:28:32,286 : INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:28:32,288 : INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:28:32,289 : INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:28:32,373 : INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:28:32,423 : INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:32,567 : INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:33,580 : INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:33,885 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:33,973 : INFO : topic #9 (0.053): 0.015*\"the\" + 0.011*\"phillip\" + 0.011*\"b\" + 0.010*\"to\" + 0.009*\"and\" + 0.006*\"g\" + 0.006*\"a\" + 0.006*\"terrance\" + 0.006*\"wendy\" + 0.005*\"butch\"\n",
      "2019-05-10 17:28:33,975 : INFO : topic #8 (0.058): 0.017*\"ace\" + 0.014*\"the\" + 0.007*\"i\" + 0.007*\"to\" + 0.006*\"a\" + 0.006*\"you\" + 0.006*\"s\" + 0.005*\"it\" + 0.005*\"and\" + 0.004*\"he\"\n",
      "2019-05-10 17:28:33,978 : INFO : topic #2 (0.125): 0.067*\"the\" + 0.030*\"and\" + 0.028*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.015*\"is\" + 0.014*\"his\" + 0.013*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:33,980 : INFO : topic #1 (0.155): 0.006*\"a\" + 0.006*\"the\" + 0.005*\"you\" + 0.004*\"scully\" + 0.004*\"sie\" + 0.004*\"to\" + 0.004*\"in\" + 0.004*\"jamal\" + 0.003*\"goldenrod\" + 0.003*\"is\"\n",
      "2019-05-10 17:28:33,984 : INFO : topic #0 (0.203): 0.012*\"ext\" + 0.011*\"ots\" + 0.011*\"int\" + 0.008*\"the\" + 0.007*\"frodo\" + 0.005*\"day\" + 0.005*\"harry\" + 0.004*\"night\" + 0.004*\"durrance\" + 0.004*\"revised\"\n",
      "2019-05-10 17:28:33,987 : INFO : topic diff=0.201846, rho=0.096183\n",
      "2019-05-10 17:28:33,991 : INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:33,993 : INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:34,607 : INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:35,144 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:35,210 : INFO : topic #9 (0.053): 0.015*\"the\" + 0.011*\"b\" + 0.011*\"phillip\" + 0.009*\"to\" + 0.009*\"and\" + 0.006*\"g\" + 0.006*\"a\" + 0.005*\"wendy\" + 0.005*\"terrance\" + 0.005*\"butch\"\n",
      "2019-05-10 17:28:35,213 : INFO : topic #8 (0.058): 0.015*\"ace\" + 0.015*\"nantz\" + 0.014*\"the\" + 0.007*\"to\" + 0.007*\"i\" + 0.006*\"a\" + 0.006*\"you\" + 0.005*\"s\" + 0.004*\"it\" + 0.004*\"he\"\n",
      "2019-05-10 17:28:35,215 : INFO : topic #2 (0.125): 0.068*\"the\" + 0.031*\"and\" + 0.029*\"a\" + 0.024*\"of\" + 0.020*\"to\" + 0.019*\"in\" + 0.015*\"is\" + 0.014*\"his\" + 0.013*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:35,217 : INFO : topic #1 (0.155): 0.006*\"a\" + 0.005*\"the\" + 0.005*\"you\" + 0.004*\"scully\" + 0.004*\"sie\" + 0.004*\"in\" + 0.004*\"to\" + 0.003*\"jamal\" + 0.003*\"goldenrod\" + 0.003*\"is\"\n",
      "2019-05-10 17:28:35,220 : INFO : topic #0 (0.203): 0.012*\"ext\" + 0.011*\"int\" + 0.011*\"ots\" + 0.008*\"the\" + 0.007*\"frodo\" + 0.005*\"day\" + 0.005*\"harry\" + 0.004*\"night\" + 0.003*\"durrance\" + 0.003*\"s\"\n",
      "2019-05-10 17:28:35,223 : INFO : topic diff=0.212282, rho=0.096183\n",
      "2019-05-10 17:28:35,225 : INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:35,227 : INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:36,011 : INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:36,306 : INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:36,474 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:36,568 : INFO : topic #9 (0.053): 0.014*\"the\" + 0.011*\"b\" + 0.010*\"phillip\" + 0.009*\"to\" + 0.008*\"and\" + 0.006*\"g\" + 0.006*\"a\" + 0.005*\"wendy\" + 0.005*\"terrance\" + 0.005*\"butch\"\n",
      "2019-05-10 17:28:36,571 : INFO : topic #8 (0.058): 0.015*\"ace\" + 0.015*\"nantz\" + 0.013*\"the\" + 0.007*\"to\" + 0.007*\"i\" + 0.006*\"a\" + 0.006*\"you\" + 0.005*\"s\" + 0.004*\"it\" + 0.004*\"he\"\n",
      "2019-05-10 17:28:36,573 : INFO : topic #2 (0.125): 0.068*\"the\" + 0.031*\"and\" + 0.029*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.019*\"in\" + 0.014*\"is\" + 0.014*\"his\" + 0.013*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:36,576 : INFO : topic #1 (0.155): 0.006*\"a\" + 0.005*\"the\" + 0.005*\"you\" + 0.004*\"scully\" + 0.004*\"sie\" + 0.004*\"in\" + 0.003*\"to\" + 0.003*\"jamal\" + 0.003*\"goldenrod\" + 0.003*\"is\"\n",
      "2019-05-10 17:28:36,579 : INFO : topic #0 (0.203): 0.018*\"int\" + 0.015*\"ext\" + 0.013*\"day\" + 0.009*\"ots\" + 0.007*\"the\" + 0.007*\"s\" + 0.006*\"frodo\" + 0.006*\"night\" + 0.004*\"harry\" + 0.003*\"van\"\n",
      "2019-05-10 17:28:36,582 : INFO : topic diff=0.187039, rho=0.096183\n",
      "2019-05-10 17:28:36,585 : INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:37,218 : INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:37,469 : INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:37,787 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:37,886 : INFO : topic #9 (0.053): 0.014*\"b\" + 0.014*\"the\" + 0.010*\"phillip\" + 0.009*\"to\" + 0.008*\"and\" + 0.006*\"g\" + 0.006*\"a\" + 0.005*\"wendy\" + 0.005*\"terrance\" + 0.005*\"butch\"\n",
      "2019-05-10 17:28:37,889 : INFO : topic #8 (0.058): 0.014*\"ace\" + 0.014*\"nantz\" + 0.013*\"the\" + 0.007*\"to\" + 0.006*\"i\" + 0.006*\"a\" + 0.005*\"you\" + 0.005*\"s\" + 0.004*\"it\" + 0.004*\"and\"\n",
      "2019-05-10 17:28:37,892 : INFO : topic #2 (0.125): 0.069*\"the\" + 0.030*\"and\" + 0.030*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.014*\"is\" + 0.014*\"his\" + 0.013*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:37,893 : INFO : topic #1 (0.155): 0.008*\"revision\" + 0.006*\"a\" + 0.005*\"the\" + 0.005*\"you\" + 0.004*\"pink\" + 0.004*\"scully\" + 0.003*\"sie\" + 0.003*\"in\" + 0.003*\"to\" + 0.003*\"goldenrod\"\n",
      "2019-05-10 17:28:37,896 : INFO : topic #0 (0.203): 0.020*\"int\" + 0.016*\"ext\" + 0.013*\"day\" + 0.009*\"ots\" + 0.007*\"the\" + 0.007*\"s\" + 0.005*\"night\" + 0.005*\"frodo\" + 0.004*\"harry\" + 0.004*\"erin\"\n",
      "2019-05-10 17:28:37,898 : INFO : topic diff=0.181615, rho=0.096183\n",
      "2019-05-10 17:28:37,901 : INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:38,711 : INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:38,744 : INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:39,120 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:39,203 : INFO : topic #9 (0.053): 0.014*\"b\" + 0.013*\"the\" + 0.009*\"phillip\" + 0.008*\"to\" + 0.008*\"and\" + 0.006*\"g\" + 0.005*\"a\" + 0.005*\"wendy\" + 0.005*\"terrance\" + 0.005*\"butch\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:28:39,206 : INFO : topic #8 (0.058): 0.014*\"ace\" + 0.013*\"nantz\" + 0.012*\"the\" + 0.006*\"to\" + 0.006*\"i\" + 0.005*\"a\" + 0.005*\"you\" + 0.005*\"s\" + 0.004*\"it\" + 0.004*\"and\"\n",
      "2019-05-10 17:28:39,208 : INFO : topic #2 (0.125): 0.069*\"the\" + 0.030*\"and\" + 0.029*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.015*\"his\" + 0.014*\"is\" + 0.014*\"he\" + 0.012*\"her\"\n",
      "2019-05-10 17:28:39,210 : INFO : topic #1 (0.155): 0.008*\"revision\" + 0.006*\"a\" + 0.005*\"the\" + 0.004*\"you\" + 0.004*\"pink\" + 0.003*\"scully\" + 0.003*\"in\" + 0.003*\"sie\" + 0.003*\"to\" + 0.003*\"goldenrod\"\n",
      "2019-05-10 17:28:39,213 : INFO : topic #0 (0.203): 0.023*\"ext\" + 0.021*\"harry\" + 0.019*\"int\" + 0.015*\"day\" + 0.013*\"durrance\" + 0.008*\"trench\" + 0.008*\"ethne\" + 0.007*\"the\" + 0.007*\"ots\" + 0.006*\"s\"\n",
      "2019-05-10 17:28:39,216 : INFO : topic diff=0.194850, rho=0.096183\n",
      "2019-05-10 17:28:39,220 : INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:40,160 : INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:40,338 : INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:40,498 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:40,591 : INFO : topic #9 (0.053): 0.019*\"b\" + 0.016*\"g\" + 0.013*\"wendy\" + 0.012*\"elwood\" + 0.012*\"to\" + 0.011*\"the\" + 0.011*\"camera\" + 0.009*\"phillip\" + 0.008*\"veta\" + 0.007*\"as\"\n",
      "2019-05-10 17:28:40,593 : INFO : topic #8 (0.058): 0.013*\"ace\" + 0.012*\"nantz\" + 0.012*\"the\" + 0.006*\"to\" + 0.006*\"i\" + 0.005*\"a\" + 0.005*\"you\" + 0.004*\"s\" + 0.004*\"it\" + 0.004*\"and\"\n",
      "2019-05-10 17:28:40,597 : INFO : topic #2 (0.125): 0.069*\"the\" + 0.030*\"and\" + 0.029*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.015*\"his\" + 0.014*\"is\" + 0.014*\"he\" + 0.012*\"her\"\n",
      "2019-05-10 17:28:40,601 : INFO : topic #1 (0.155): 0.008*\"revision\" + 0.006*\"a\" + 0.004*\"pink\" + 0.004*\"the\" + 0.004*\"you\" + 0.003*\"scully\" + 0.003*\"sie\" + 0.003*\"in\" + 0.003*\"to\" + 0.003*\"goldenrod\"\n",
      "2019-05-10 17:28:40,604 : INFO : topic #0 (0.203): 0.022*\"ext\" + 0.020*\"harry\" + 0.018*\"int\" + 0.015*\"day\" + 0.012*\"durrance\" + 0.008*\"trench\" + 0.007*\"ethne\" + 0.007*\"the\" + 0.006*\"ots\" + 0.006*\"night\"\n",
      "2019-05-10 17:28:40,607 : INFO : topic diff=0.179401, rho=0.096183\n",
      "2019-05-10 17:28:40,610 : INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:41,462 : INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:41,477 : INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:41,785 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:41,852 : INFO : topic #9 (0.053): 0.019*\"b\" + 0.016*\"g\" + 0.013*\"wendy\" + 0.012*\"elwood\" + 0.011*\"to\" + 0.011*\"the\" + 0.010*\"camera\" + 0.008*\"phillip\" + 0.008*\"veta\" + 0.007*\"as\"\n",
      "2019-05-10 17:28:41,856 : INFO : topic #8 (0.058): 0.013*\"ace\" + 0.012*\"nantz\" + 0.011*\"the\" + 0.006*\"to\" + 0.005*\"i\" + 0.005*\"a\" + 0.005*\"you\" + 0.004*\"s\" + 0.003*\"it\" + 0.003*\"and\"\n",
      "2019-05-10 17:28:41,861 : INFO : topic #2 (0.125): 0.070*\"the\" + 0.030*\"and\" + 0.029*\"a\" + 0.024*\"of\" + 0.020*\"to\" + 0.019*\"in\" + 0.015*\"his\" + 0.014*\"is\" + 0.014*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:41,863 : INFO : topic #1 (0.155): 0.007*\"revision\" + 0.006*\"a\" + 0.005*\"pink\" + 0.004*\"the\" + 0.004*\"you\" + 0.003*\"de\" + 0.003*\"in\" + 0.003*\"scully\" + 0.003*\"sie\" + 0.003*\"blue\"\n",
      "2019-05-10 17:28:41,868 : INFO : topic #0 (0.203): 0.022*\"ext\" + 0.019*\"harry\" + 0.018*\"int\" + 0.014*\"day\" + 0.012*\"frodo\" + 0.011*\"durrance\" + 0.008*\"night\" + 0.007*\"trench\" + 0.006*\"the\" + 0.006*\"ethne\"\n",
      "2019-05-10 17:28:41,870 : INFO : topic diff=0.157104, rho=0.096183\n",
      "2019-05-10 17:28:41,872 : INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:42,704 : INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:42,737 : INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:43,044 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:43,159 : INFO : topic #9 (0.053): 0.019*\"b\" + 0.016*\"g\" + 0.012*\"wendy\" + 0.011*\"elwood\" + 0.011*\"to\" + 0.010*\"the\" + 0.010*\"camera\" + 0.008*\"phillip\" + 0.007*\"veta\" + 0.007*\"as\"\n",
      "2019-05-10 17:28:43,162 : INFO : topic #8 (0.058): 0.012*\"ace\" + 0.011*\"nantz\" + 0.010*\"the\" + 0.005*\"to\" + 0.005*\"i\" + 0.005*\"a\" + 0.004*\"you\" + 0.004*\"s\" + 0.003*\"it\" + 0.003*\"he\"\n",
      "2019-05-10 17:28:43,166 : INFO : topic #2 (0.125): 0.070*\"the\" + 0.031*\"and\" + 0.029*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.014*\"his\" + 0.014*\"is\" + 0.013*\"he\" + 0.012*\"her\"\n",
      "2019-05-10 17:28:43,168 : INFO : topic #1 (0.155): 0.008*\"goldenrod\" + 0.006*\"revision\" + 0.006*\"a\" + 0.005*\"pink\" + 0.004*\"the\" + 0.004*\"you\" + 0.003*\"de\" + 0.003*\"scully\" + 0.003*\"in\" + 0.003*\"sie\"\n",
      "2019-05-10 17:28:43,171 : INFO : topic #0 (0.203): 0.021*\"ext\" + 0.019*\"harry\" + 0.017*\"int\" + 0.014*\"day\" + 0.011*\"frodo\" + 0.010*\"durrance\" + 0.007*\"nd\" + 0.007*\"night\" + 0.007*\"revised\" + 0.007*\"draft\"\n",
      "2019-05-10 17:28:43,173 : INFO : topic diff=0.148156, rho=0.096183\n",
      "2019-05-10 17:28:43,176 : INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:43,844 : INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:43,995 : INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:44,329 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:44,394 : INFO : topic #9 (0.053): 0.019*\"b\" + 0.016*\"g\" + 0.012*\"wendy\" + 0.010*\"elwood\" + 0.010*\"to\" + 0.010*\"the\" + 0.010*\"camera\" + 0.008*\"phillip\" + 0.007*\"veta\" + 0.006*\"as\"\n",
      "2019-05-10 17:28:44,396 : INFO : topic #8 (0.058): 0.011*\"ace\" + 0.010*\"nantz\" + 0.010*\"the\" + 0.005*\"to\" + 0.005*\"i\" + 0.005*\"a\" + 0.004*\"you\" + 0.004*\"s\" + 0.003*\"it\" + 0.003*\"he\"\n",
      "2019-05-10 17:28:44,398 : INFO : topic #2 (0.125): 0.070*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.024*\"of\" + 0.020*\"to\" + 0.019*\"in\" + 0.015*\"his\" + 0.014*\"is\" + 0.013*\"he\" + 0.012*\"her\"\n",
      "2019-05-10 17:28:44,409 : INFO : topic #1 (0.155): 0.007*\"goldenrod\" + 0.006*\"revision\" + 0.006*\"a\" + 0.005*\"pink\" + 0.004*\"de\" + 0.004*\"the\" + 0.004*\"you\" + 0.003*\"scully\" + 0.003*\"in\" + 0.003*\"blue\"\n",
      "2019-05-10 17:28:44,411 : INFO : topic #0 (0.203): 0.020*\"ext\" + 0.018*\"harry\" + 0.017*\"int\" + 0.013*\"day\" + 0.010*\"frodo\" + 0.010*\"durrance\" + 0.007*\"nd\" + 0.007*\"night\" + 0.007*\"revised\" + 0.007*\"draft\"\n",
      "2019-05-10 17:28:44,415 : INFO : topic diff=0.138151, rho=0.096183\n",
      "2019-05-10 17:28:44,421 : INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:45,095 : INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:45,380 : INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:45,625 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:45,691 : INFO : topic #9 (0.053): 0.019*\"b\" + 0.015*\"phillip\" + 0.015*\"g\" + 0.013*\"wendy\" + 0.012*\"terrance\" + 0.010*\"to\" + 0.010*\"the\" + 0.009*\"elwood\" + 0.009*\"camera\" + 0.006*\"veta\"\n",
      "2019-05-10 17:28:45,694 : INFO : topic #8 (0.058): 0.011*\"ace\" + 0.010*\"nantz\" + 0.009*\"the\" + 0.005*\"to\" + 0.005*\"i\" + 0.004*\"a\" + 0.004*\"you\" + 0.004*\"s\" + 0.003*\"it\" + 0.003*\"he\"\n",
      "2019-05-10 17:28:45,697 : INFO : topic #2 (0.125): 0.070*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"is\" + 0.015*\"his\" + 0.014*\"he\" + 0.012*\"her\"\n",
      "2019-05-10 17:28:45,699 : INFO : topic #1 (0.155): 0.008*\"jamal\" + 0.007*\"goldenrod\" + 0.005*\"revision\" + 0.005*\"a\" + 0.004*\"pink\" + 0.004*\"the\" + 0.003*\"you\" + 0.003*\"de\" + 0.003*\"sie\" + 0.003*\"ze\"\n",
      "2019-05-10 17:28:45,701 : INFO : topic #0 (0.203): 0.019*\"ext\" + 0.019*\"harry\" + 0.016*\"int\" + 0.013*\"day\" + 0.010*\"frodo\" + 0.009*\"durrance\" + 0.007*\"nd\" + 0.007*\"night\" + 0.007*\"revised\" + 0.006*\"draft\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:28:45,704 : INFO : topic diff=0.152475, rho=0.096183\n",
      "2019-05-10 17:28:45,708 : INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:46,383 : INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:46,621 : INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:46,922 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:46,986 : INFO : topic #9 (0.053): 0.019*\"b\" + 0.015*\"phillip\" + 0.014*\"g\" + 0.012*\"wendy\" + 0.011*\"terrance\" + 0.009*\"to\" + 0.009*\"the\" + 0.009*\"elwood\" + 0.008*\"camera\" + 0.006*\"veta\"\n",
      "2019-05-10 17:28:46,989 : INFO : topic #8 (0.058): 0.011*\"ace\" + 0.009*\"nantz\" + 0.009*\"the\" + 0.005*\"to\" + 0.004*\"i\" + 0.004*\"a\" + 0.004*\"you\" + 0.003*\"s\" + 0.003*\"it\" + 0.003*\"he\"\n",
      "2019-05-10 17:28:46,991 : INFO : topic #2 (0.125): 0.070*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.024*\"of\" + 0.020*\"to\" + 0.019*\"in\" + 0.015*\"is\" + 0.014*\"his\" + 0.014*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:46,995 : INFO : topic #1 (0.155): 0.007*\"jamal\" + 0.006*\"goldenrod\" + 0.005*\"a\" + 0.005*\"revision\" + 0.005*\"de\" + 0.004*\"pink\" + 0.003*\"the\" + 0.003*\"ze\" + 0.003*\"you\" + 0.003*\"der\"\n",
      "2019-05-10 17:28:46,997 : INFO : topic #0 (0.203): 0.018*\"harry\" + 0.017*\"ext\" + 0.015*\"int\" + 0.012*\"day\" + 0.009*\"frodo\" + 0.008*\"durrance\" + 0.007*\"night\" + 0.006*\"nd\" + 0.006*\"revised\" + 0.006*\"draft\"\n",
      "2019-05-10 17:28:47,000 : INFO : topic diff=0.136784, rho=0.096183\n",
      "2019-05-10 17:28:47,003 : INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:47,478 : INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:48,115 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:48,235 : INFO : topic #9 (0.053): 0.020*\"b\" + 0.016*\"g\" + 0.014*\"phillip\" + 0.012*\"wendy\" + 0.011*\"terrance\" + 0.009*\"to\" + 0.009*\"the\" + 0.008*\"elwood\" + 0.008*\"camera\" + 0.006*\"and\"\n",
      "2019-05-10 17:28:48,243 : INFO : topic #8 (0.058): 0.010*\"ace\" + 0.009*\"nantz\" + 0.008*\"the\" + 0.004*\"to\" + 0.004*\"i\" + 0.004*\"a\" + 0.003*\"you\" + 0.003*\"s\" + 0.003*\"melissa\" + 0.003*\"it\"\n",
      "2019-05-10 17:28:48,248 : INFO : topic #2 (0.125): 0.071*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"is\" + 0.015*\"his\" + 0.014*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:48,260 : INFO : topic #1 (0.155): 0.007*\"jamal\" + 0.006*\"de\" + 0.005*\"goldenrod\" + 0.005*\"a\" + 0.004*\"ze\" + 0.004*\"revision\" + 0.004*\"pink\" + 0.004*\"ja\" + 0.004*\"la\" + 0.003*\"est\"\n",
      "2019-05-10 17:28:48,265 : INFO : topic #0 (0.203): 0.017*\"harry\" + 0.017*\"frodo\" + 0.016*\"ext\" + 0.014*\"int\" + 0.011*\"day\" + 0.008*\"gandalf\" + 0.008*\"durrance\" + 0.006*\"night\" + 0.006*\"nd\" + 0.006*\"draft\"\n",
      "2019-05-10 17:28:48,276 : INFO : topic diff=0.124329, rho=0.096183\n",
      "2019-05-10 17:28:49,325 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:49,416 : INFO : topic #9 (0.053): 0.020*\"b\" + 0.015*\"g\" + 0.013*\"phillip\" + 0.012*\"wendy\" + 0.010*\"terrance\" + 0.009*\"to\" + 0.008*\"the\" + 0.008*\"elwood\" + 0.007*\"camera\" + 0.005*\"and\"\n",
      "2019-05-10 17:28:49,419 : INFO : topic #8 (0.058): 0.010*\"ace\" + 0.008*\"nantz\" + 0.008*\"the\" + 0.004*\"to\" + 0.004*\"i\" + 0.004*\"a\" + 0.003*\"you\" + 0.003*\"mcu\" + 0.003*\"s\" + 0.003*\"melissa\"\n",
      "2019-05-10 17:28:49,421 : INFO : topic #2 (0.125): 0.072*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"is\" + 0.015*\"his\" + 0.014*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:49,422 : INFO : topic #1 (0.155): 0.008*\"scully\" + 0.007*\"rowan\" + 0.006*\"sie\" + 0.006*\"jamal\" + 0.005*\"la\" + 0.005*\"de\" + 0.005*\"papa\" + 0.005*\"ja\" + 0.005*\"goldenrod\" + 0.004*\"a\"\n",
      "2019-05-10 17:28:49,425 : INFO : topic #0 (0.203): 0.019*\"harry\" + 0.016*\"frodo\" + 0.015*\"ext\" + 0.013*\"int\" + 0.011*\"day\" + 0.007*\"gandalf\" + 0.007*\"durrance\" + 0.006*\"night\" + 0.006*\"nd\" + 0.006*\"draft\"\n",
      "2019-05-10 17:28:49,429 : INFO : topic diff=0.120435, rho=0.096183\n",
      "2019-05-10 17:28:50,500 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:50,546 : INFO : topic #9 (0.053): 0.020*\"b\" + 0.015*\"g\" + 0.012*\"phillip\" + 0.012*\"wendy\" + 0.009*\"terrance\" + 0.008*\"to\" + 0.008*\"the\" + 0.007*\"elwood\" + 0.007*\"camera\" + 0.007*\"sieu\"\n",
      "2019-05-10 17:28:50,548 : INFO : topic #8 (0.058): 0.009*\"ace\" + 0.008*\"mcu\" + 0.007*\"nantz\" + 0.007*\"the\" + 0.004*\"to\" + 0.004*\"i\" + 0.003*\"a\" + 0.003*\"you\" + 0.003*\"s\" + 0.002*\"melissa\"\n",
      "2019-05-10 17:28:50,550 : INFO : topic #2 (0.125): 0.071*\"the\" + 0.032*\"and\" + 0.029*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"is\" + 0.015*\"his\" + 0.014*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:50,552 : INFO : topic #1 (0.155): 0.008*\"scully\" + 0.006*\"rowan\" + 0.006*\"sie\" + 0.006*\"jamal\" + 0.005*\"de\" + 0.005*\"la\" + 0.004*\"ja\" + 0.004*\"papa\" + 0.004*\"goldenrod\" + 0.004*\"a\"\n",
      "2019-05-10 17:28:50,554 : INFO : topic #0 (0.203): 0.020*\"harry\" + 0.016*\"ext\" + 0.015*\"ots\" + 0.015*\"int\" + 0.014*\"frodo\" + 0.011*\"day\" + 0.007*\"mcu\" + 0.006*\"gandalf\" + 0.006*\"durrance\" + 0.006*\"night\"\n",
      "2019-05-10 17:28:50,556 : INFO : topic diff=0.115788, rho=0.096183\n",
      "2019-05-10 17:28:50,559 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:28:50,644 : INFO : PROGRESS: pass 3, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:28:50,646 : INFO : PROGRESS: pass 3, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:28:50,647 : INFO : PROGRESS: pass 3, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:28:50,648 : INFO : PROGRESS: pass 3, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:28:50,650 : INFO : PROGRESS: pass 3, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:28:50,754 : INFO : PROGRESS: pass 3, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:28:50,785 : INFO : PROGRESS: pass 3, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:50,911 : INFO : PROGRESS: pass 3, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:51,897 : INFO : PROGRESS: pass 3, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:52,047 : INFO : PROGRESS: pass 3, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:52,249 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:52,312 : INFO : topic #9 (0.053): 0.022*\"b\" + 0.021*\"phillip\" + 0.014*\"g\" + 0.011*\"wendy\" + 0.010*\"butch\" + 0.009*\"dickson\" + 0.009*\"terrance\" + 0.008*\"to\" + 0.008*\"the\" + 0.006*\"elwood\"\n",
      "2019-05-10 17:28:52,315 : INFO : topic #8 (0.058): 0.036*\"ace\" + 0.009*\"finkle\" + 0.008*\"einhorn\" + 0.007*\"mcu\" + 0.007*\"melissa\" + 0.007*\"the\" + 0.006*\"snowflake\" + 0.006*\"nantz\" + 0.005*\"marino\" + 0.004*\"to\"\n",
      "2019-05-10 17:28:52,317 : INFO : topic #2 (0.125): 0.072*\"the\" + 0.032*\"and\" + 0.029*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"is\" + 0.015*\"his\" + 0.014*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:52,319 : INFO : topic #1 (0.155): 0.007*\"scully\" + 0.007*\"papa\" + 0.006*\"rowan\" + 0.005*\"sie\" + 0.005*\"jamal\" + 0.005*\"de\" + 0.004*\"la\" + 0.004*\"herr\" + 0.004*\"ja\" + 0.004*\"der\"\n",
      "2019-05-10 17:28:52,321 : INFO : topic #0 (0.203): 0.019*\"harry\" + 0.016*\"ext\" + 0.014*\"int\" + 0.014*\"ots\" + 0.013*\"frodo\" + 0.011*\"day\" + 0.007*\"mcu\" + 0.007*\"night\" + 0.006*\"gandalf\" + 0.006*\"durrance\"\n",
      "2019-05-10 17:28:52,324 : INFO : topic diff=0.115279, rho=0.095741\n",
      "2019-05-10 17:28:52,327 : INFO : PROGRESS: pass 3, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:52,983 : INFO : PROGRESS: pass 3, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:53,158 : INFO : PROGRESS: pass 3, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:53,444 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:53,508 : INFO : topic #9 (0.053): 0.021*\"b\" + 0.019*\"phillip\" + 0.015*\"g\" + 0.010*\"wendy\" + 0.009*\"butch\" + 0.008*\"dickson\" + 0.008*\"terrance\" + 0.007*\"to\" + 0.007*\"the\" + 0.006*\"camera\"\n",
      "2019-05-10 17:28:53,510 : INFO : topic #8 (0.058): 0.031*\"ace\" + 0.024*\"nantz\" + 0.008*\"finkle\" + 0.007*\"imlay\" + 0.007*\"einhorn\" + 0.006*\"the\" + 0.006*\"mcu\" + 0.006*\"melissa\" + 0.006*\"martinez\" + 0.005*\"snowflake\"\n",
      "2019-05-10 17:28:53,512 : INFO : topic #2 (0.125): 0.072*\"the\" + 0.032*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"is\" + 0.015*\"his\" + 0.014*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:53,515 : INFO : topic #1 (0.155): 0.007*\"papa\" + 0.007*\"scully\" + 0.005*\"rowan\" + 0.005*\"sie\" + 0.005*\"jamal\" + 0.005*\"de\" + 0.004*\"herr\" + 0.004*\"la\" + 0.004*\"a\" + 0.004*\"ja\"\n",
      "2019-05-10 17:28:53,518 : INFO : topic #0 (0.203): 0.022*\"harry\" + 0.015*\"ext\" + 0.014*\"int\" + 0.013*\"ots\" + 0.012*\"frodo\" + 0.010*\"day\" + 0.006*\"night\" + 0.006*\"mcu\" + 0.006*\"gandalf\" + 0.005*\"durrance\"\n",
      "2019-05-10 17:28:53,521 : INFO : topic diff=0.127888, rho=0.095741\n",
      "2019-05-10 17:28:53,524 : INFO : PROGRESS: pass 3, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:54,050 : INFO : PROGRESS: pass 3, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:54,543 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:54,719 : INFO : topic #9 (0.053): 0.021*\"b\" + 0.019*\"phillip\" + 0.015*\"g\" + 0.010*\"wendy\" + 0.009*\"butch\" + 0.008*\"dickson\" + 0.007*\"terrance\" + 0.007*\"to\" + 0.007*\"the\" + 0.006*\"camera\"\n",
      "2019-05-10 17:28:54,722 : INFO : topic #8 (0.058): 0.031*\"ace\" + 0.022*\"nantz\" + 0.007*\"finkle\" + 0.006*\"imlay\" + 0.006*\"einhorn\" + 0.006*\"the\" + 0.005*\"mcu\" + 0.005*\"melissa\" + 0.005*\"martinez\" + 0.005*\"snowflake\"\n",
      "2019-05-10 17:28:54,724 : INFO : topic #2 (0.125): 0.072*\"the\" + 0.032*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"his\" + 0.015*\"is\" + 0.014*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:54,726 : INFO : topic #1 (0.155): 0.007*\"papa\" + 0.006*\"scully\" + 0.005*\"rowan\" + 0.005*\"la\" + 0.005*\"de\" + 0.005*\"sie\" + 0.004*\"jamal\" + 0.004*\"herr\" + 0.004*\"a\" + 0.004*\"ja\"\n",
      "2019-05-10 17:28:54,729 : INFO : topic #0 (0.203): 0.021*\"int\" + 0.020*\"harry\" + 0.019*\"day\" + 0.018*\"ext\" + 0.011*\"ots\" + 0.011*\"frodo\" + 0.009*\"s\" + 0.008*\"night\" + 0.006*\"mcu\" + 0.005*\"gandalf\"\n",
      "2019-05-10 17:28:54,732 : INFO : topic diff=0.094677, rho=0.095741\n",
      "2019-05-10 17:28:54,735 : INFO : PROGRESS: pass 3, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:54,736 : INFO : PROGRESS: pass 3, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:55,238 : INFO : PROGRESS: pass 3, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:55,648 : INFO : PROGRESS: pass 3, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:55,933 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:55,999 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.018*\"phillip\" + 0.015*\"g\" + 0.009*\"wendy\" + 0.008*\"butch\" + 0.007*\"dickson\" + 0.007*\"terrance\" + 0.007*\"the\" + 0.007*\"to\" + 0.006*\"camera\"\n",
      "2019-05-10 17:28:56,002 : INFO : topic #8 (0.058): 0.029*\"ace\" + 0.021*\"nantz\" + 0.007*\"finkle\" + 0.006*\"imlay\" + 0.006*\"einhorn\" + 0.006*\"the\" + 0.006*\"melissa\" + 0.005*\"mcu\" + 0.005*\"martinez\" + 0.005*\"snowflake\"\n",
      "2019-05-10 17:28:56,004 : INFO : topic #2 (0.125): 0.073*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:56,007 : INFO : topic #1 (0.155): 0.010*\"revision\" + 0.007*\"papa\" + 0.006*\"pink\" + 0.006*\"scully\" + 0.005*\"la\" + 0.005*\"de\" + 0.004*\"rowan\" + 0.004*\"sie\" + 0.004*\"a\" + 0.004*\"jamal\"\n",
      "2019-05-10 17:28:56,009 : INFO : topic #0 (0.203): 0.023*\"harry\" + 0.023*\"int\" + 0.019*\"ext\" + 0.017*\"day\" + 0.011*\"ots\" + 0.010*\"frodo\" + 0.008*\"s\" + 0.008*\"night\" + 0.007*\"erin\" + 0.005*\"mcu\"\n",
      "2019-05-10 17:28:56,012 : INFO : topic diff=0.091108, rho=0.095741\n",
      "2019-05-10 17:28:56,014 : INFO : PROGRESS: pass 3, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:56,624 : INFO : PROGRESS: pass 3, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:56,823 : INFO : PROGRESS: pass 3, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:57,231 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:57,296 : INFO : topic #9 (0.053): 0.027*\"b\" + 0.016*\"phillip\" + 0.014*\"g\" + 0.010*\"tao\" + 0.008*\"wendy\" + 0.008*\"butch\" + 0.007*\"dickson\" + 0.006*\"the\" + 0.006*\"to\" + 0.006*\"terrance\"\n",
      "2019-05-10 17:28:57,298 : INFO : topic #8 (0.058): 0.027*\"ace\" + 0.019*\"nantz\" + 0.006*\"finkle\" + 0.006*\"imlay\" + 0.005*\"the\" + 0.005*\"einhorn\" + 0.005*\"melissa\" + 0.005*\"mcu\" + 0.005*\"martinez\" + 0.004*\"snowflake\"\n",
      "2019-05-10 17:28:57,300 : INFO : topic #2 (0.125): 0.073*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.015*\"is\" + 0.012*\"her\"\n",
      "2019-05-10 17:28:57,303 : INFO : topic #1 (0.155): 0.009*\"revision\" + 0.007*\"papa\" + 0.006*\"pink\" + 0.005*\"scully\" + 0.005*\"de\" + 0.005*\"la\" + 0.004*\"sie\" + 0.004*\"a\" + 0.004*\"rowan\" + 0.004*\"jamal\"\n",
      "2019-05-10 17:28:57,305 : INFO : topic #0 (0.203): 0.056*\"harry\" + 0.024*\"ext\" + 0.021*\"day\" + 0.020*\"int\" + 0.015*\"trench\" + 0.014*\"durrance\" + 0.010*\"blaze\" + 0.009*\"ethne\" + 0.009*\"willoughby\" + 0.008*\"night\"\n",
      "2019-05-10 17:28:57,308 : INFO : topic diff=0.111662, rho=0.095741\n",
      "2019-05-10 17:28:57,311 : INFO : PROGRESS: pass 3, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:58,400 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:58,577 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.024*\"g\" + 0.017*\"wendy\" + 0.016*\"camera\" + 0.014*\"phillip\" + 0.014*\"elwood\" + 0.013*\"wichita\" + 0.010*\"to\" + 0.009*\"veta\" + 0.009*\"wilson\"\n",
      "2019-05-10 17:28:58,579 : INFO : topic #8 (0.058): 0.025*\"ace\" + 0.018*\"nantz\" + 0.006*\"finkle\" + 0.005*\"melissa\" + 0.005*\"imlay\" + 0.005*\"the\" + 0.005*\"einhorn\" + 0.004*\"mcu\" + 0.004*\"martinez\" + 0.004*\"sergeant\"\n",
      "2019-05-10 17:28:58,582 : INFO : topic #2 (0.125): 0.073*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.014*\"is\" + 0.012*\"her\"\n",
      "2019-05-10 17:28:58,584 : INFO : topic #1 (0.155): 0.009*\"revision\" + 0.006*\"papa\" + 0.006*\"pink\" + 0.005*\"scully\" + 0.005*\"de\" + 0.004*\"la\" + 0.004*\"sie\" + 0.004*\"a\" + 0.004*\"rowan\" + 0.004*\"der\"\n",
      "2019-05-10 17:28:58,586 : INFO : topic #0 (0.203): 0.054*\"harry\" + 0.023*\"ext\" + 0.020*\"day\" + 0.019*\"int\" + 0.014*\"trench\" + 0.013*\"durrance\" + 0.009*\"blaze\" + 0.009*\"ethne\" + 0.008*\"willoughby\" + 0.008*\"night\"\n",
      "2019-05-10 17:28:58,590 : INFO : topic diff=0.095589, rho=0.095741\n",
      "2019-05-10 17:28:58,594 : INFO : PROGRESS: pass 3, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 7\n",
      "2019-05-10 17:28:58,596 : INFO : PROGRESS: pass 3, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 8\n",
      "2019-05-10 17:28:58,597 : INFO : PROGRESS: pass 3, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:59,408 : INFO : PROGRESS: pass 3, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:59,442 : INFO : PROGRESS: pass 3, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n",
      "2019-05-10 17:28:59,747 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:28:59,811 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.024*\"g\" + 0.016*\"wendy\" + 0.015*\"camera\" + 0.013*\"phillip\" + 0.013*\"elwood\" + 0.012*\"wichita\" + 0.009*\"to\" + 0.009*\"veta\" + 0.008*\"wilson\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:28:59,813 : INFO : topic #8 (0.058): 0.024*\"ace\" + 0.017*\"nantz\" + 0.005*\"finkle\" + 0.005*\"sergeant\" + 0.005*\"melissa\" + 0.005*\"imlay\" + 0.005*\"the\" + 0.005*\"einhorn\" + 0.004*\"mcu\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:28:59,815 : INFO : topic #2 (0.125): 0.073*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:28:59,817 : INFO : topic #1 (0.155): 0.008*\"revision\" + 0.006*\"pink\" + 0.006*\"papa\" + 0.005*\"de\" + 0.005*\"la\" + 0.004*\"scully\" + 0.004*\"a\" + 0.004*\"sie\" + 0.004*\"est\" + 0.004*\"rowan\"\n",
      "2019-05-10 17:28:59,820 : INFO : topic #0 (0.203): 0.052*\"harry\" + 0.023*\"ext\" + 0.019*\"day\" + 0.019*\"int\" + 0.015*\"frodo\" + 0.012*\"trench\" + 0.012*\"durrance\" + 0.010*\"night\" + 0.008*\"blaze\" + 0.008*\"ethne\"\n",
      "2019-05-10 17:28:59,825 : INFO : topic diff=0.074763, rho=0.095741\n",
      "2019-05-10 17:28:59,827 : INFO : PROGRESS: pass 3, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:00,578 : INFO : PROGRESS: pass 3, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:00,744 : INFO : PROGRESS: pass 3, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:00,905 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:00,998 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.024*\"g\" + 0.015*\"wendy\" + 0.015*\"camera\" + 0.013*\"phillip\" + 0.012*\"elwood\" + 0.011*\"wichita\" + 0.009*\"to\" + 0.008*\"veta\" + 0.008*\"wilson\"\n",
      "2019-05-10 17:29:01,002 : INFO : topic #8 (0.058): 0.023*\"ace\" + 0.015*\"nantz\" + 0.005*\"finkle\" + 0.005*\"sergeant\" + 0.005*\"melissa\" + 0.004*\"the\" + 0.004*\"imlay\" + 0.004*\"einhorn\" + 0.004*\"martinez\" + 0.004*\"mcu\"\n",
      "2019-05-10 17:29:01,012 : INFO : topic #2 (0.125): 0.073*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"his\" + 0.014*\"he\" + 0.014*\"is\" + 0.012*\"her\"\n",
      "2019-05-10 17:29:01,017 : INFO : topic #1 (0.155): 0.007*\"revision\" + 0.007*\"goldenrod\" + 0.006*\"pink\" + 0.006*\"papa\" + 0.005*\"de\" + 0.005*\"la\" + 0.004*\"scully\" + 0.004*\"a\" + 0.003*\"est\" + 0.003*\"sie\"\n",
      "2019-05-10 17:29:01,022 : INFO : topic #0 (0.203): 0.050*\"harry\" + 0.021*\"ext\" + 0.018*\"day\" + 0.018*\"int\" + 0.014*\"frodo\" + 0.011*\"trench\" + 0.011*\"durrance\" + 0.009*\"night\" + 0.009*\"nd\" + 0.008*\"blaze\"\n",
      "2019-05-10 17:29:01,025 : INFO : topic diff=0.067816, rho=0.095741\n",
      "2019-05-10 17:29:01,027 : INFO : PROGRESS: pass 3, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:01,583 : INFO : PROGRESS: pass 3, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:01,787 : INFO : PROGRESS: pass 3, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:02,109 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:02,193 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.024*\"g\" + 0.015*\"wendy\" + 0.014*\"camera\" + 0.012*\"phillip\" + 0.012*\"elwood\" + 0.011*\"wichita\" + 0.008*\"to\" + 0.008*\"wilson\" + 0.008*\"veta\"\n",
      "2019-05-10 17:29:02,195 : INFO : topic #8 (0.058): 0.022*\"ace\" + 0.014*\"nantz\" + 0.005*\"finkle\" + 0.005*\"sergeant\" + 0.004*\"melissa\" + 0.004*\"the\" + 0.004*\"imlay\" + 0.004*\"einhorn\" + 0.004*\"martinez\" + 0.004*\"mcu\"\n",
      "2019-05-10 17:29:02,198 : INFO : topic #2 (0.125): 0.073*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.012*\"her\"\n",
      "2019-05-10 17:29:02,200 : INFO : topic #1 (0.155): 0.007*\"revision\" + 0.007*\"goldenrod\" + 0.006*\"de\" + 0.006*\"pink\" + 0.006*\"papa\" + 0.005*\"la\" + 0.004*\"scully\" + 0.004*\"a\" + 0.003*\"est\" + 0.003*\"blue\"\n",
      "2019-05-10 17:29:02,202 : INFO : topic #0 (0.203): 0.048*\"harry\" + 0.020*\"ext\" + 0.018*\"day\" + 0.017*\"int\" + 0.013*\"frodo\" + 0.011*\"trench\" + 0.010*\"durrance\" + 0.009*\"willoughby\" + 0.009*\"night\" + 0.008*\"nd\"\n",
      "2019-05-10 17:29:02,204 : INFO : topic diff=0.059563, rho=0.095741\n",
      "2019-05-10 17:29:02,207 : INFO : PROGRESS: pass 3, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:02,781 : INFO : PROGRESS: pass 3, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:02,932 : INFO : PROGRESS: pass 3, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:03,273 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:03,364 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.022*\"g\" + 0.020*\"phillip\" + 0.016*\"wendy\" + 0.013*\"terrance\" + 0.013*\"camera\" + 0.010*\"elwood\" + 0.010*\"wichita\" + 0.008*\"to\" + 0.007*\"wilson\"\n",
      "2019-05-10 17:29:03,368 : INFO : topic #8 (0.058): 0.021*\"ace\" + 0.013*\"nantz\" + 0.005*\"sergeant\" + 0.004*\"finkle\" + 0.004*\"melissa\" + 0.004*\"the\" + 0.004*\"imlay\" + 0.004*\"einhorn\" + 0.004*\"martinez\" + 0.003*\"mcu\"\n",
      "2019-05-10 17:29:03,377 : INFO : topic #2 (0.125): 0.073*\"the\" + 0.031*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.012*\"her\"\n",
      "2019-05-10 17:29:03,391 : INFO : topic #1 (0.155): 0.009*\"jamal\" + 0.006*\"revision\" + 0.006*\"goldenrod\" + 0.006*\"papa\" + 0.006*\"de\" + 0.005*\"pink\" + 0.004*\"rupees\" + 0.004*\"la\" + 0.004*\"ze\" + 0.004*\"sie\"\n",
      "2019-05-10 17:29:03,395 : INFO : topic #0 (0.203): 0.056*\"harry\" + 0.019*\"ext\" + 0.017*\"day\" + 0.016*\"int\" + 0.013*\"frodo\" + 0.010*\"trench\" + 0.010*\"durrance\" + 0.009*\"willoughby\" + 0.008*\"night\" + 0.008*\"nd\"\n",
      "2019-05-10 17:29:03,400 : INFO : topic diff=0.078665, rho=0.095741\n",
      "2019-05-10 17:29:03,406 : INFO : PROGRESS: pass 3, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:04,036 : INFO : PROGRESS: pass 3, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:04,195 : INFO : PROGRESS: pass 3, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:04,464 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:04,569 : INFO : topic #9 (0.053): 0.028*\"b\" + 0.021*\"g\" + 0.019*\"phillip\" + 0.015*\"wendy\" + 0.013*\"terrance\" + 0.012*\"camera\" + 0.010*\"elwood\" + 0.009*\"wichita\" + 0.008*\"to\" + 0.007*\"kelly\"\n",
      "2019-05-10 17:29:04,572 : INFO : topic #8 (0.058): 0.021*\"ace\" + 0.012*\"nantz\" + 0.004*\"sergeant\" + 0.004*\"finkle\" + 0.004*\"melissa\" + 0.004*\"the\" + 0.004*\"imlay\" + 0.003*\"einhorn\" + 0.003*\"martinez\" + 0.003*\"mcu\"\n",
      "2019-05-10 17:29:04,574 : INFO : topic #2 (0.125): 0.073*\"the\" + 0.031*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.015*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:04,577 : INFO : topic #1 (0.155): 0.008*\"jamal\" + 0.007*\"de\" + 0.006*\"la\" + 0.005*\"revision\" + 0.005*\"goldenrod\" + 0.005*\"papa\" + 0.005*\"pink\" + 0.004*\"ze\" + 0.004*\"der\" + 0.004*\"est\"\n",
      "2019-05-10 17:29:04,579 : INFO : topic #0 (0.203): 0.052*\"harry\" + 0.018*\"ext\" + 0.016*\"day\" + 0.015*\"int\" + 0.011*\"frodo\" + 0.009*\"trench\" + 0.009*\"durrance\" + 0.008*\"night\" + 0.008*\"willoughby\" + 0.007*\"nd\"\n",
      "2019-05-10 17:29:04,581 : INFO : topic diff=0.066670, rho=0.095741\n",
      "2019-05-10 17:29:04,584 : INFO : PROGRESS: pass 3, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:05,017 : INFO : PROGRESS: pass 3, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:05,699 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:05,794 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.022*\"g\" + 0.018*\"phillip\" + 0.015*\"wendy\" + 0.012*\"terrance\" + 0.012*\"camera\" + 0.009*\"elwood\" + 0.008*\"wichita\" + 0.007*\"to\" + 0.007*\"kelly\"\n",
      "2019-05-10 17:29:05,796 : INFO : topic #8 (0.058): 0.019*\"ace\" + 0.011*\"nantz\" + 0.005*\"melissa\" + 0.004*\"sergeant\" + 0.004*\"finkle\" + 0.003*\"einhorn\" + 0.003*\"the\" + 0.003*\"imlay\" + 0.003*\"martinez\" + 0.003*\"wigand\"\n",
      "2019-05-10 17:29:05,800 : INFO : topic #2 (0.125): 0.074*\"the\" + 0.032*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.016*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:05,802 : INFO : topic #1 (0.155): 0.008*\"de\" + 0.007*\"jamal\" + 0.007*\"la\" + 0.006*\"papa\" + 0.005*\"ze\" + 0.005*\"pink\" + 0.005*\"revision\" + 0.005*\"ja\" + 0.005*\"goldenrod\" + 0.004*\"est\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:05,805 : INFO : topic #0 (0.203): 0.051*\"harry\" + 0.020*\"frodo\" + 0.016*\"ext\" + 0.015*\"day\" + 0.014*\"int\" + 0.009*\"gandalf\" + 0.009*\"trench\" + 0.008*\"durrance\" + 0.007*\"night\" + 0.007*\"willoughby\"\n",
      "2019-05-10 17:29:05,808 : INFO : topic diff=0.059597, rho=0.095741\n",
      "2019-05-10 17:29:06,831 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:06,893 : INFO : topic #9 (0.053): 0.030*\"b\" + 0.022*\"g\" + 0.017*\"phillip\" + 0.015*\"wendy\" + 0.011*\"terrance\" + 0.011*\"camera\" + 0.009*\"elwood\" + 0.008*\"wichita\" + 0.007*\"to\" + 0.006*\"kelly\"\n",
      "2019-05-10 17:29:06,895 : INFO : topic #8 (0.058): 0.018*\"ace\" + 0.010*\"nantz\" + 0.005*\"melissa\" + 0.004*\"sergeant\" + 0.003*\"finkle\" + 0.003*\"einhorn\" + 0.003*\"the\" + 0.003*\"imlay\" + 0.003*\"martinez\" + 0.003*\"mcu\"\n",
      "2019-05-10 17:29:06,898 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.016*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:06,900 : INFO : topic #1 (0.155): 0.010*\"papa\" + 0.010*\"scully\" + 0.008*\"rowan\" + 0.008*\"la\" + 0.007*\"de\" + 0.007*\"sie\" + 0.006*\"jamal\" + 0.005*\"ja\" + 0.005*\"der\" + 0.005*\"ze\"\n",
      "2019-05-10 17:29:06,902 : INFO : topic #0 (0.203): 0.054*\"harry\" + 0.018*\"frodo\" + 0.015*\"ext\" + 0.014*\"day\" + 0.013*\"int\" + 0.009*\"gandalf\" + 0.008*\"trench\" + 0.008*\"durrance\" + 0.007*\"night\" + 0.007*\"willoughby\"\n",
      "2019-05-10 17:29:06,907 : INFO : topic diff=0.056649, rho=0.095741\n",
      "2019-05-10 17:29:07,911 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:07,953 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.022*\"g\" + 0.016*\"phillip\" + 0.014*\"wendy\" + 0.010*\"camera\" + 0.010*\"terrance\" + 0.009*\"kelly\" + 0.008*\"elwood\" + 0.008*\"sieu\" + 0.007*\"wichita\"\n",
      "2019-05-10 17:29:07,955 : INFO : topic #8 (0.058): 0.017*\"ace\" + 0.009*\"nantz\" + 0.008*\"alex\" + 0.006*\"mcu\" + 0.005*\"melissa\" + 0.005*\"sergeant\" + 0.003*\"finkle\" + 0.003*\"martinez\" + 0.003*\"the\" + 0.003*\"einhorn\"\n",
      "2019-05-10 17:29:07,957 : INFO : topic #2 (0.125): 0.074*\"the\" + 0.032*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.016*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:07,958 : INFO : topic #1 (0.155): 0.010*\"papa\" + 0.009*\"scully\" + 0.007*\"rowan\" + 0.007*\"la\" + 0.007*\"de\" + 0.006*\"sie\" + 0.006*\"jamal\" + 0.005*\"ja\" + 0.005*\"ze\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:07,960 : INFO : topic #0 (0.203): 0.050*\"harry\" + 0.019*\"darcy\" + 0.016*\"ext\" + 0.015*\"frodo\" + 0.014*\"day\" + 0.014*\"ots\" + 0.014*\"int\" + 0.014*\"scene\" + 0.012*\"mcu\" + 0.007*\"gandalf\"\n",
      "2019-05-10 17:29:07,962 : INFO : topic diff=0.057754, rho=0.095741\n",
      "2019-05-10 17:29:07,965 : INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:29:08,045 : INFO : PROGRESS: pass 4, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:29:08,047 : INFO : PROGRESS: pass 4, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:29:08,048 : INFO : PROGRESS: pass 4, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:29:08,049 : INFO : PROGRESS: pass 4, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:29:08,051 : INFO : PROGRESS: pass 4, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:29:08,165 : INFO : PROGRESS: pass 4, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:29:08,282 : INFO : PROGRESS: pass 4, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:08,388 : INFO : PROGRESS: pass 4, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:09,202 : INFO : PROGRESS: pass 4, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:09,335 : INFO : PROGRESS: pass 4, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:09,517 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:09,607 : INFO : topic #9 (0.053): 0.031*\"b\" + 0.025*\"phillip\" + 0.021*\"g\" + 0.013*\"wendy\" + 0.013*\"butch\" + 0.011*\"dickson\" + 0.009*\"camera\" + 0.009*\"terrance\" + 0.009*\"kelly\" + 0.007*\"elwood\"\n",
      "2019-05-10 17:29:09,609 : INFO : topic #8 (0.058): 0.047*\"ace\" + 0.013*\"alex\" + 0.012*\"finkle\" + 0.009*\"einhorn\" + 0.009*\"melissa\" + 0.008*\"snowflake\" + 0.008*\"nantz\" + 0.006*\"marino\" + 0.005*\"mcu\" + 0.004*\"sergeant\"\n",
      "2019-05-10 17:29:09,612 : INFO : topic #2 (0.125): 0.074*\"the\" + 0.032*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.016*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:09,615 : INFO : topic #1 (0.155): 0.012*\"papa\" + 0.008*\"scully\" + 0.007*\"de\" + 0.007*\"la\" + 0.007*\"rowan\" + 0.006*\"herr\" + 0.006*\"sie\" + 0.005*\"jamal\" + 0.005*\"der\" + 0.005*\"ja\"\n",
      "2019-05-10 17:29:09,618 : INFO : topic #0 (0.203): 0.048*\"harry\" + 0.018*\"darcy\" + 0.015*\"ext\" + 0.015*\"frodo\" + 0.014*\"int\" + 0.014*\"day\" + 0.013*\"ots\" + 0.013*\"scene\" + 0.011*\"mcu\" + 0.008*\"night\"\n",
      "2019-05-10 17:29:09,620 : INFO : topic diff=0.060189, rho=0.095306\n",
      "2019-05-10 17:29:09,624 : INFO : PROGRESS: pass 4, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:10,103 : INFO : PROGRESS: pass 4, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:10,664 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:10,724 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.023*\"phillip\" + 0.021*\"g\" + 0.012*\"wendy\" + 0.011*\"butch\" + 0.010*\"dickson\" + 0.010*\"camera\" + 0.008*\"terrance\" + 0.008*\"kelly\" + 0.007*\"lowrey\"\n",
      "2019-05-10 17:29:10,727 : INFO : topic #8 (0.058): 0.040*\"ace\" + 0.027*\"nantz\" + 0.011*\"alex\" + 0.010*\"finkle\" + 0.009*\"sergeant\" + 0.008*\"imlay\" + 0.008*\"einhorn\" + 0.008*\"melissa\" + 0.007*\"marines\" + 0.007*\"martinez\"\n",
      "2019-05-10 17:29:10,729 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.016*\"his\" + 0.015*\"is\" + 0.015*\"he\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:10,731 : INFO : topic #1 (0.155): 0.013*\"papa\" + 0.007*\"scully\" + 0.006*\"de\" + 0.006*\"la\" + 0.006*\"rowan\" + 0.006*\"herr\" + 0.005*\"sie\" + 0.005*\"jamal\" + 0.005*\"ja\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:10,733 : INFO : topic #0 (0.203): 0.051*\"harry\" + 0.017*\"darcy\" + 0.014*\"ext\" + 0.014*\"frodo\" + 0.013*\"int\" + 0.013*\"day\" + 0.012*\"ots\" + 0.012*\"scene\" + 0.010*\"mcu\" + 0.008*\"night\"\n",
      "2019-05-10 17:29:10,736 : INFO : topic diff=0.075201, rho=0.095306\n",
      "2019-05-10 17:29:10,738 : INFO : PROGRESS: pass 4, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:10,739 : INFO : PROGRESS: pass 4, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:11,226 : INFO : PROGRESS: pass 4, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:11,604 : INFO : PROGRESS: pass 4, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:11,795 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:11,889 : INFO : topic #9 (0.053): 0.029*\"b\" + 0.022*\"phillip\" + 0.021*\"g\" + 0.011*\"butch\" + 0.011*\"wendy\" + 0.009*\"dickson\" + 0.009*\"camera\" + 0.008*\"kelly\" + 0.008*\"terrance\" + 0.007*\"lowrey\"\n",
      "2019-05-10 17:29:11,891 : INFO : topic #8 (0.058): 0.040*\"ace\" + 0.025*\"nantz\" + 0.011*\"alex\" + 0.010*\"finkle\" + 0.008*\"sergeant\" + 0.007*\"imlay\" + 0.007*\"einhorn\" + 0.007*\"melissa\" + 0.006*\"martinez\" + 0.006*\"marines\"\n",
      "2019-05-10 17:29:11,894 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:11,896 : INFO : topic #1 (0.155): 0.012*\"papa\" + 0.007*\"la\" + 0.007*\"scully\" + 0.007*\"de\" + 0.006*\"monsieur\" + 0.006*\"herr\" + 0.006*\"rowan\" + 0.005*\"sie\" + 0.005*\"jamal\" + 0.004*\"der\"\n",
      "2019-05-10 17:29:11,898 : INFO : topic #0 (0.203): 0.047*\"harry\" + 0.020*\"day\" + 0.020*\"int\" + 0.017*\"ext\" + 0.015*\"darcy\" + 0.012*\"frodo\" + 0.011*\"ots\" + 0.011*\"scene\" + 0.010*\"s\" + 0.009*\"mcu\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:11,901 : INFO : topic diff=0.044715, rho=0.095306\n",
      "2019-05-10 17:29:11,903 : INFO : PROGRESS: pass 4, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:12,349 : INFO : PROGRESS: pass 4, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:12,738 : INFO : PROGRESS: pass 4, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:13,085 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:13,167 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.021*\"g\" + 0.020*\"phillip\" + 0.010*\"wendy\" + 0.010*\"butch\" + 0.009*\"camera\" + 0.009*\"dickson\" + 0.008*\"kelly\" + 0.007*\"terrance\" + 0.006*\"lowrey\"\n",
      "2019-05-10 17:29:13,170 : INFO : topic #8 (0.058): 0.038*\"ace\" + 0.023*\"nantz\" + 0.010*\"alex\" + 0.009*\"finkle\" + 0.007*\"melissa\" + 0.007*\"sergeant\" + 0.007*\"imlay\" + 0.007*\"einhorn\" + 0.006*\"martinez\" + 0.006*\"marines\"\n",
      "2019-05-10 17:29:13,173 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:13,174 : INFO : topic #1 (0.155): 0.012*\"papa\" + 0.010*\"revision\" + 0.007*\"la\" + 0.007*\"de\" + 0.006*\"scully\" + 0.006*\"pink\" + 0.006*\"monsieur\" + 0.006*\"herr\" + 0.005*\"rowan\" + 0.005*\"sie\"\n",
      "2019-05-10 17:29:13,177 : INFO : topic #0 (0.203): 0.050*\"harry\" + 0.022*\"int\" + 0.019*\"day\" + 0.018*\"ext\" + 0.014*\"darcy\" + 0.011*\"frodo\" + 0.010*\"ots\" + 0.010*\"scene\" + 0.009*\"s\" + 0.009*\"night\"\n",
      "2019-05-10 17:29:13,179 : INFO : topic diff=0.044019, rho=0.095306\n",
      "2019-05-10 17:29:13,182 : INFO : PROGRESS: pass 4, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:13,648 : INFO : PROGRESS: pass 4, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:13,986 : INFO : PROGRESS: pass 4, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:14,287 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:14,396 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.020*\"g\" + 0.019*\"phillip\" + 0.017*\"tao\" + 0.010*\"butch\" + 0.010*\"wendy\" + 0.008*\"camera\" + 0.008*\"dickson\" + 0.007*\"kelly\" + 0.007*\"terrance\"\n",
      "2019-05-10 17:29:14,400 : INFO : topic #8 (0.058): 0.035*\"ace\" + 0.022*\"nantz\" + 0.010*\"alex\" + 0.008*\"finkle\" + 0.007*\"melissa\" + 0.007*\"sergeant\" + 0.006*\"imlay\" + 0.006*\"einhorn\" + 0.006*\"martinez\" + 0.005*\"snowflake\"\n",
      "2019-05-10 17:29:14,406 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.012*\"her\"\n",
      "2019-05-10 17:29:14,408 : INFO : topic #1 (0.155): 0.012*\"papa\" + 0.010*\"revision\" + 0.007*\"la\" + 0.006*\"de\" + 0.006*\"pink\" + 0.006*\"scully\" + 0.006*\"monsieur\" + 0.005*\"herr\" + 0.005*\"rowan\" + 0.004*\"sie\"\n",
      "2019-05-10 17:29:14,415 : INFO : topic #0 (0.203): 0.079*\"harry\" + 0.023*\"ext\" + 0.022*\"day\" + 0.019*\"int\" + 0.017*\"trench\" + 0.013*\"durrance\" + 0.012*\"blaze\" + 0.010*\"darcy\" + 0.009*\"willoughby\" + 0.009*\"ethne\"\n",
      "2019-05-10 17:29:14,426 : INFO : topic diff=0.067798, rho=0.095306\n",
      "2019-05-10 17:29:14,430 : INFO : PROGRESS: pass 4, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:15,143 : INFO : PROGRESS: pass 4, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:15,458 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:15,658 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.028*\"g\" + 0.019*\"camera\" + 0.018*\"wendy\" + 0.016*\"phillip\" + 0.014*\"elwood\" + 0.014*\"wichita\" + 0.012*\"tao\" + 0.010*\"kelly\" + 0.010*\"wilson\"\n",
      "2019-05-10 17:29:15,660 : INFO : topic #8 (0.058): 0.032*\"ace\" + 0.020*\"nantz\" + 0.011*\"alex\" + 0.008*\"finkle\" + 0.007*\"melissa\" + 0.007*\"sergeant\" + 0.006*\"imlay\" + 0.006*\"einhorn\" + 0.005*\"martinez\" + 0.005*\"snowflake\"\n",
      "2019-05-10 17:29:15,663 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:15,665 : INFO : topic #1 (0.155): 0.011*\"papa\" + 0.009*\"revision\" + 0.006*\"monsieur\" + 0.006*\"la\" + 0.006*\"de\" + 0.006*\"pink\" + 0.005*\"scully\" + 0.005*\"herr\" + 0.005*\"rowan\" + 0.004*\"der\"\n",
      "2019-05-10 17:29:15,667 : INFO : topic #0 (0.203): 0.077*\"harry\" + 0.022*\"ext\" + 0.022*\"day\" + 0.018*\"int\" + 0.016*\"trench\" + 0.013*\"durrance\" + 0.011*\"blaze\" + 0.009*\"darcy\" + 0.009*\"night\" + 0.009*\"willoughby\"\n",
      "2019-05-10 17:29:15,670 : INFO : topic diff=0.063339, rho=0.095306\n",
      "2019-05-10 17:29:15,673 : INFO : PROGRESS: pass 4, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:15,675 : INFO : PROGRESS: pass 4, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:16,537 : INFO : PROGRESS: pass 4, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:16,717 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:16,882 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.029*\"g\" + 0.018*\"camera\" + 0.017*\"wendy\" + 0.015*\"phillip\" + 0.013*\"elwood\" + 0.013*\"wichita\" + 0.011*\"tao\" + 0.010*\"kelly\" + 0.009*\"wilson\"\n",
      "2019-05-10 17:29:16,885 : INFO : topic #8 (0.058): 0.030*\"ace\" + 0.018*\"nantz\" + 0.010*\"alex\" + 0.008*\"sergeant\" + 0.007*\"finkle\" + 0.007*\"melissa\" + 0.005*\"imlay\" + 0.005*\"einhorn\" + 0.005*\"hector\" + 0.005*\"snowflake\"\n",
      "2019-05-10 17:29:16,887 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:16,889 : INFO : topic #1 (0.155): 0.010*\"papa\" + 0.008*\"revision\" + 0.007*\"la\" + 0.007*\"pink\" + 0.007*\"de\" + 0.006*\"monsieur\" + 0.005*\"scully\" + 0.005*\"herr\" + 0.004*\"rowan\" + 0.004*\"der\"\n",
      "2019-05-10 17:29:16,891 : INFO : topic #0 (0.203): 0.073*\"harry\" + 0.021*\"ext\" + 0.020*\"day\" + 0.018*\"int\" + 0.016*\"frodo\" + 0.014*\"trench\" + 0.011*\"durrance\" + 0.010*\"blaze\" + 0.010*\"night\" + 0.009*\"darcy\"\n",
      "2019-05-10 17:29:16,894 : INFO : topic diff=0.040999, rho=0.095306\n",
      "2019-05-10 17:29:16,897 : INFO : PROGRESS: pass 4, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:16,898 : INFO : PROGRESS: pass 4, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:17,725 : INFO : PROGRESS: pass 4, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:17,809 : INFO : PROGRESS: pass 4, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:18,131 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:18,206 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.029*\"g\" + 0.018*\"camera\" + 0.016*\"wendy\" + 0.014*\"phillip\" + 0.013*\"wichita\" + 0.013*\"elwood\" + 0.010*\"tao\" + 0.010*\"kelly\" + 0.009*\"wilson\"\n",
      "2019-05-10 17:29:18,210 : INFO : topic #8 (0.058): 0.029*\"ace\" + 0.017*\"nantz\" + 0.009*\"alex\" + 0.008*\"sergeant\" + 0.006*\"finkle\" + 0.006*\"melissa\" + 0.005*\"imlay\" + 0.005*\"einhorn\" + 0.005*\"hector\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:29:18,212 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:18,214 : INFO : topic #1 (0.155): 0.010*\"papa\" + 0.007*\"monsieur\" + 0.007*\"revision\" + 0.007*\"la\" + 0.007*\"de\" + 0.007*\"pink\" + 0.005*\"scully\" + 0.004*\"herr\" + 0.004*\"der\" + 0.004*\"rowan\"\n",
      "2019-05-10 17:29:18,216 : INFO : topic #0 (0.203): 0.069*\"harry\" + 0.020*\"ext\" + 0.019*\"day\" + 0.017*\"int\" + 0.014*\"frodo\" + 0.013*\"trench\" + 0.010*\"durrance\" + 0.010*\"blaze\" + 0.009*\"night\" + 0.009*\"nd\"\n",
      "2019-05-10 17:29:18,219 : INFO : topic diff=0.037451, rho=0.095306\n",
      "2019-05-10 17:29:18,224 : INFO : PROGRESS: pass 4, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:18,738 : INFO : PROGRESS: pass 4, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:18,996 : INFO : PROGRESS: pass 4, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:19,313 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:19,377 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.028*\"g\" + 0.017*\"camera\" + 0.016*\"wendy\" + 0.014*\"phillip\" + 0.012*\"wichita\" + 0.012*\"elwood\" + 0.010*\"tao\" + 0.009*\"kelly\" + 0.009*\"wilson\"\n",
      "2019-05-10 17:29:19,381 : INFO : topic #8 (0.058): 0.027*\"ace\" + 0.016*\"nantz\" + 0.010*\"alex\" + 0.007*\"sergeant\" + 0.006*\"finkle\" + 0.006*\"melissa\" + 0.005*\"imlay\" + 0.005*\"einhorn\" + 0.004*\"harris\" + 0.004*\"hector\"\n",
      "2019-05-10 17:29:19,383 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:19,385 : INFO : topic #1 (0.155): 0.010*\"papa\" + 0.008*\"de\" + 0.007*\"monsieur\" + 0.007*\"revision\" + 0.007*\"la\" + 0.006*\"pink\" + 0.004*\"scully\" + 0.004*\"herr\" + 0.004*\"der\" + 0.004*\"est\"\n",
      "2019-05-10 17:29:19,387 : INFO : topic #0 (0.203): 0.067*\"harry\" + 0.019*\"ext\" + 0.019*\"day\" + 0.016*\"int\" + 0.014*\"frodo\" + 0.012*\"trench\" + 0.010*\"durrance\" + 0.010*\"willoughby\" + 0.009*\"blaze\" + 0.009*\"night\"\n",
      "2019-05-10 17:29:19,393 : INFO : topic diff=0.039132, rho=0.095306\n",
      "2019-05-10 17:29:19,395 : INFO : PROGRESS: pass 4, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:19,998 : INFO : PROGRESS: pass 4, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:20,058 : INFO : PROGRESS: pass 4, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:20,460 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:20,525 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.026*\"g\" + 0.022*\"phillip\" + 0.016*\"wendy\" + 0.015*\"camera\" + 0.014*\"terrance\" + 0.011*\"wichita\" + 0.010*\"elwood\" + 0.009*\"tao\" + 0.009*\"kelly\"\n",
      "2019-05-10 17:29:20,528 : INFO : topic #8 (0.058): 0.026*\"ace\" + 0.014*\"nantz\" + 0.009*\"alex\" + 0.007*\"sergeant\" + 0.005*\"finkle\" + 0.005*\"melissa\" + 0.004*\"imlay\" + 0.004*\"harris\" + 0.004*\"einhorn\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:29:20,532 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.015*\"is\" + 0.012*\"her\"\n",
      "2019-05-10 17:29:20,542 : INFO : topic #1 (0.155): 0.009*\"jamal\" + 0.009*\"papa\" + 0.007*\"de\" + 0.006*\"monsieur\" + 0.006*\"la\" + 0.006*\"revision\" + 0.006*\"pink\" + 0.005*\"rupees\" + 0.005*\"ze\" + 0.004*\"herr\"\n",
      "2019-05-10 17:29:20,546 : INFO : topic #0 (0.203): 0.076*\"harry\" + 0.018*\"ext\" + 0.018*\"day\" + 0.016*\"int\" + 0.013*\"frodo\" + 0.012*\"trench\" + 0.009*\"durrance\" + 0.009*\"willoughby\" + 0.009*\"night\" + 0.009*\"blaze\"\n",
      "2019-05-10 17:29:20,560 : INFO : topic diff=0.053334, rho=0.095306\n",
      "2019-05-10 17:29:20,563 : INFO : PROGRESS: pass 4, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:21,022 : INFO : PROGRESS: pass 4, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:21,274 : INFO : PROGRESS: pass 4, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:21,559 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:21,650 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.025*\"g\" + 0.020*\"phillip\" + 0.015*\"wendy\" + 0.015*\"camera\" + 0.013*\"terrance\" + 0.010*\"wichita\" + 0.010*\"elwood\" + 0.009*\"kelly\" + 0.008*\"tao\"\n",
      "2019-05-10 17:29:21,653 : INFO : topic #8 (0.058): 0.026*\"ace\" + 0.013*\"nantz\" + 0.009*\"alex\" + 0.007*\"sergeant\" + 0.005*\"finkle\" + 0.005*\"melissa\" + 0.004*\"martinez\" + 0.004*\"imlay\" + 0.004*\"harris\" + 0.004*\"einhorn\"\n",
      "2019-05-10 17:29:21,655 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.015*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:21,657 : INFO : topic #1 (0.155): 0.008*\"de\" + 0.008*\"papa\" + 0.008*\"jamal\" + 0.008*\"la\" + 0.006*\"monsieur\" + 0.005*\"revision\" + 0.005*\"pink\" + 0.005*\"der\" + 0.005*\"ze\" + 0.005*\"rupees\"\n",
      "2019-05-10 17:29:21,659 : INFO : topic #0 (0.203): 0.071*\"harry\" + 0.017*\"day\" + 0.017*\"ext\" + 0.014*\"int\" + 0.012*\"frodo\" + 0.011*\"trench\" + 0.009*\"night\" + 0.008*\"durrance\" + 0.008*\"willoughby\" + 0.008*\"blaze\"\n",
      "2019-05-10 17:29:21,662 : INFO : topic diff=0.044617, rho=0.095306\n",
      "2019-05-10 17:29:21,665 : INFO : PROGRESS: pass 4, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:22,166 : INFO : PROGRESS: pass 4, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:22,746 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:22,810 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.026*\"g\" + 0.019*\"phillip\" + 0.016*\"wendy\" + 0.014*\"camera\" + 0.012*\"terrance\" + 0.010*\"wichita\" + 0.009*\"kelly\" + 0.009*\"elwood\" + 0.008*\"tao\"\n",
      "2019-05-10 17:29:22,813 : INFO : topic #8 (0.058): 0.024*\"ace\" + 0.012*\"nantz\" + 0.008*\"alex\" + 0.007*\"sergeant\" + 0.006*\"melissa\" + 0.005*\"finkle\" + 0.004*\"einhorn\" + 0.004*\"wigand\" + 0.004*\"martinez\" + 0.004*\"harris\"\n",
      "2019-05-10 17:29:22,815 : INFO : topic #2 (0.125): 0.075*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.026*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.016*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:22,817 : INFO : topic #1 (0.155): 0.010*\"de\" + 0.010*\"papa\" + 0.009*\"la\" + 0.007*\"jamal\" + 0.007*\"monsieur\" + 0.006*\"ze\" + 0.005*\"pink\" + 0.005*\"ja\" + 0.005*\"revision\" + 0.005*\"est\"\n",
      "2019-05-10 17:29:22,819 : INFO : topic #0 (0.203): 0.070*\"harry\" + 0.019*\"frodo\" + 0.016*\"day\" + 0.015*\"ext\" + 0.014*\"int\" + 0.010*\"trench\" + 0.010*\"gandalf\" + 0.008*\"night\" + 0.008*\"durrance\" + 0.008*\"willoughby\"\n",
      "2019-05-10 17:29:22,822 : INFO : topic diff=0.038909, rho=0.095306\n",
      "2019-05-10 17:29:23,770 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:23,831 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.026*\"g\" + 0.018*\"phillip\" + 0.016*\"wendy\" + 0.013*\"camera\" + 0.011*\"terrance\" + 0.009*\"wichita\" + 0.009*\"kelly\" + 0.009*\"elwood\" + 0.007*\"f\"\n",
      "2019-05-10 17:29:23,834 : INFO : topic #8 (0.058): 0.023*\"ace\" + 0.011*\"nantz\" + 0.009*\"alex\" + 0.007*\"sergeant\" + 0.006*\"melissa\" + 0.004*\"finkle\" + 0.003*\"einhorn\" + 0.003*\"wigand\" + 0.003*\"martinez\" + 0.003*\"harris\"\n",
      "2019-05-10 17:29:23,836 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:23,838 : INFO : topic #1 (0.155): 0.013*\"papa\" + 0.010*\"scully\" + 0.010*\"la\" + 0.009*\"de\" + 0.008*\"rowan\" + 0.007*\"sie\" + 0.006*\"jamal\" + 0.006*\"monsieur\" + 0.006*\"ja\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:23,840 : INFO : topic #0 (0.203): 0.073*\"harry\" + 0.018*\"frodo\" + 0.015*\"day\" + 0.015*\"ext\" + 0.013*\"int\" + 0.010*\"trench\" + 0.009*\"gandalf\" + 0.008*\"night\" + 0.007*\"durrance\" + 0.007*\"willoughby\"\n",
      "2019-05-10 17:29:23,843 : INFO : topic diff=0.041693, rho=0.095306\n",
      "2019-05-10 17:29:24,786 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:24,833 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.025*\"g\" + 0.017*\"phillip\" + 0.014*\"wendy\" + 0.013*\"camera\" + 0.012*\"kelly\" + 0.010*\"terrance\" + 0.008*\"wichita\" + 0.008*\"elwood\" + 0.008*\"sieu\"\n",
      "2019-05-10 17:29:24,835 : INFO : topic #8 (0.058): 0.023*\"alex\" + 0.021*\"ace\" + 0.010*\"nantz\" + 0.007*\"sergeant\" + 0.005*\"melissa\" + 0.004*\"finkle\" + 0.003*\"harris\" + 0.003*\"martinez\" + 0.003*\"mcu\" + 0.003*\"einhorn\"\n",
      "2019-05-10 17:29:24,837 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.030*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.016*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:24,839 : INFO : topic #1 (0.155): 0.013*\"papa\" + 0.009*\"scully\" + 0.009*\"la\" + 0.008*\"de\" + 0.008*\"rowan\" + 0.006*\"sie\" + 0.006*\"jamal\" + 0.006*\"monsieur\" + 0.005*\"ja\" + 0.005*\"ze\"\n",
      "2019-05-10 17:29:24,840 : INFO : topic #0 (0.203): 0.063*\"harry\" + 0.042*\"scene\" + 0.026*\"darcy\" + 0.015*\"day\" + 0.015*\"frodo\" + 0.014*\"ext\" + 0.014*\"mcu\" + 0.013*\"int\" + 0.013*\"ots\" + 0.008*\"night\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:24,842 : INFO : topic diff=0.043773, rho=0.095306\n",
      "2019-05-10 17:29:24,846 : INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:29:24,928 : INFO : PROGRESS: pass 5, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:29:24,929 : INFO : PROGRESS: pass 5, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:29:24,931 : INFO : PROGRESS: pass 5, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:29:24,933 : INFO : PROGRESS: pass 5, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:29:24,934 : INFO : PROGRESS: pass 5, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:29:25,035 : INFO : PROGRESS: pass 5, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:29:25,190 : INFO : PROGRESS: pass 5, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:25,321 : INFO : PROGRESS: pass 5, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:26,079 : INFO : PROGRESS: pass 5, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:26,188 : INFO : PROGRESS: pass 5, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:26,496 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:26,561 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.026*\"phillip\" + 0.023*\"g\" + 0.014*\"butch\" + 0.013*\"wendy\" + 0.012*\"kelly\" + 0.011*\"camera\" + 0.011*\"dickson\" + 0.009*\"terrance\" + 0.008*\"wichita\"\n",
      "2019-05-10 17:29:26,563 : INFO : topic #8 (0.058): 0.051*\"ace\" + 0.034*\"alex\" + 0.013*\"finkle\" + 0.010*\"melissa\" + 0.010*\"einhorn\" + 0.008*\"snowflake\" + 0.008*\"nantz\" + 0.007*\"marino\" + 0.006*\"sergeant\" + 0.004*\"dolphin\"\n",
      "2019-05-10 17:29:26,566 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:26,570 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.008*\"de\" + 0.008*\"scully\" + 0.008*\"la\" + 0.007*\"rowan\" + 0.007*\"herr\" + 0.006*\"mozart\" + 0.006*\"sie\" + 0.006*\"monsieur\" + 0.005*\"jamal\"\n",
      "2019-05-10 17:29:26,575 : INFO : topic #0 (0.203): 0.060*\"harry\" + 0.040*\"scene\" + 0.025*\"darcy\" + 0.014*\"day\" + 0.014*\"frodo\" + 0.014*\"ext\" + 0.013*\"mcu\" + 0.013*\"int\" + 0.012*\"ots\" + 0.008*\"night\"\n",
      "2019-05-10 17:29:26,578 : INFO : topic diff=0.046627, rho=0.094876\n",
      "2019-05-10 17:29:26,585 : INFO : PROGRESS: pass 5, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:27,035 : INFO : PROGRESS: pass 5, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:27,319 : INFO : PROGRESS: pass 5, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:27,620 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:27,687 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.023*\"g\" + 0.023*\"phillip\" + 0.013*\"wendy\" + 0.012*\"butch\" + 0.011*\"camera\" + 0.011*\"kelly\" + 0.010*\"dickson\" + 0.009*\"lowrey\" + 0.008*\"terrance\"\n",
      "2019-05-10 17:29:27,690 : INFO : topic #8 (0.058): 0.043*\"ace\" + 0.028*\"alex\" + 0.028*\"nantz\" + 0.011*\"finkle\" + 0.011*\"sergeant\" + 0.008*\"melissa\" + 0.008*\"imlay\" + 0.008*\"einhorn\" + 0.007*\"marines\" + 0.007*\"hector\"\n",
      "2019-05-10 17:29:27,692 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:27,694 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.008*\"de\" + 0.008*\"la\" + 0.008*\"scully\" + 0.007*\"herr\" + 0.007*\"rowan\" + 0.006*\"monsieur\" + 0.006*\"mozart\" + 0.005*\"sie\" + 0.005*\"jamal\"\n",
      "2019-05-10 17:29:27,696 : INFO : topic #0 (0.203): 0.063*\"harry\" + 0.038*\"scene\" + 0.023*\"darcy\" + 0.014*\"day\" + 0.013*\"frodo\" + 0.013*\"ext\" + 0.012*\"int\" + 0.012*\"mcu\" + 0.011*\"ots\" + 0.008*\"night\"\n",
      "2019-05-10 17:29:27,698 : INFO : topic diff=0.061714, rho=0.094876\n",
      "2019-05-10 17:29:27,701 : INFO : PROGRESS: pass 5, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:28,577 : INFO : PROGRESS: pass 5, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:29,021 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:29,116 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.023*\"g\" + 0.023*\"phillip\" + 0.012*\"butch\" + 0.012*\"wendy\" + 0.011*\"kelly\" + 0.011*\"camera\" + 0.009*\"dickson\" + 0.008*\"lowrey\" + 0.008*\"terrance\"\n",
      "2019-05-10 17:29:29,119 : INFO : topic #8 (0.058): 0.043*\"ace\" + 0.027*\"alex\" + 0.025*\"nantz\" + 0.010*\"finkle\" + 0.010*\"sergeant\" + 0.008*\"melissa\" + 0.008*\"imlay\" + 0.007*\"einhorn\" + 0.007*\"marines\" + 0.007*\"martinez\"\n",
      "2019-05-10 17:29:29,138 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.019*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:29,140 : INFO : topic #1 (0.155): 0.014*\"papa\" + 0.009*\"monsieur\" + 0.009*\"la\" + 0.008*\"de\" + 0.007*\"scully\" + 0.007*\"herr\" + 0.006*\"rowan\" + 0.005*\"mozart\" + 0.005*\"sie\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:29,142 : INFO : topic #0 (0.203): 0.059*\"harry\" + 0.034*\"scene\" + 0.021*\"darcy\" + 0.020*\"day\" + 0.018*\"int\" + 0.015*\"ext\" + 0.012*\"frodo\" + 0.011*\"mcu\" + 0.010*\"s\" + 0.010*\"ots\"\n",
      "2019-05-10 17:29:29,145 : INFO : topic diff=0.033590, rho=0.094876\n",
      "2019-05-10 17:29:29,148 : INFO : PROGRESS: pass 5, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:29,149 : INFO : PROGRESS: pass 5, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:29,587 : INFO : PROGRESS: pass 5, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:30,104 : INFO : PROGRESS: pass 5, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:30,339 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:30,406 : INFO : topic #9 (0.053): 0.041*\"b\" + 0.023*\"g\" + 0.021*\"phillip\" + 0.011*\"butch\" + 0.011*\"wendy\" + 0.011*\"camera\" + 0.010*\"kelly\" + 0.009*\"dickson\" + 0.008*\"lowrey\" + 0.007*\"terrance\"\n",
      "2019-05-10 17:29:30,408 : INFO : topic #8 (0.058): 0.041*\"ace\" + 0.026*\"alex\" + 0.024*\"nantz\" + 0.010*\"finkle\" + 0.009*\"sergeant\" + 0.008*\"melissa\" + 0.007*\"imlay\" + 0.007*\"einhorn\" + 0.006*\"marines\" + 0.006*\"martinez\"\n",
      "2019-05-10 17:29:30,411 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:30,413 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.010*\"revision\" + 0.009*\"monsieur\" + 0.008*\"la\" + 0.008*\"de\" + 0.007*\"scully\" + 0.006*\"herr\" + 0.006*\"pink\" + 0.006*\"rowan\" + 0.005*\"mozart\"\n",
      "2019-05-10 17:29:30,416 : INFO : topic #0 (0.203): 0.061*\"harry\" + 0.032*\"scene\" + 0.020*\"int\" + 0.019*\"darcy\" + 0.019*\"day\" + 0.016*\"ext\" + 0.011*\"frodo\" + 0.010*\"mcu\" + 0.010*\"s\" + 0.009*\"ots\"\n",
      "2019-05-10 17:29:30,418 : INFO : topic diff=0.035609, rho=0.094876\n",
      "2019-05-10 17:29:30,421 : INFO : PROGRESS: pass 5, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:31,081 : INFO : PROGRESS: pass 5, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:31,190 : INFO : PROGRESS: pass 5, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:31,503 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:31,566 : INFO : topic #9 (0.053): 0.038*\"b\" + 0.022*\"g\" + 0.019*\"phillip\" + 0.019*\"tao\" + 0.011*\"butch\" + 0.010*\"camera\" + 0.010*\"wendy\" + 0.010*\"kelly\" + 0.008*\"dickson\" + 0.007*\"lowrey\"\n",
      "2019-05-10 17:29:31,568 : INFO : topic #8 (0.058): 0.038*\"ace\" + 0.026*\"alex\" + 0.022*\"nantz\" + 0.009*\"sergeant\" + 0.009*\"finkle\" + 0.008*\"melissa\" + 0.006*\"imlay\" + 0.006*\"einhorn\" + 0.006*\"hector\" + 0.006*\"marines\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:31,570 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.012*\"her\"\n",
      "2019-05-10 17:29:31,574 : INFO : topic #1 (0.155): 0.014*\"papa\" + 0.009*\"revision\" + 0.008*\"monsieur\" + 0.008*\"la\" + 0.008*\"de\" + 0.006*\"scully\" + 0.006*\"pink\" + 0.006*\"herr\" + 0.005*\"rowan\" + 0.004*\"mozart\"\n",
      "2019-05-10 17:29:31,577 : INFO : topic #0 (0.203): 0.085*\"harry\" + 0.024*\"scene\" + 0.022*\"day\" + 0.021*\"ext\" + 0.018*\"int\" + 0.017*\"trench\" + 0.014*\"darcy\" + 0.013*\"durrance\" + 0.012*\"blaze\" + 0.010*\"s\"\n",
      "2019-05-10 17:29:31,580 : INFO : topic diff=0.055283, rho=0.094876\n",
      "2019-05-10 17:29:31,583 : INFO : PROGRESS: pass 5, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:32,645 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:32,814 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.029*\"g\" + 0.020*\"camera\" + 0.018*\"wendy\" + 0.016*\"phillip\" + 0.015*\"wichita\" + 0.014*\"elwood\" + 0.013*\"tao\" + 0.012*\"kelly\" + 0.010*\"wilson\"\n",
      "2019-05-10 17:29:32,817 : INFO : topic #8 (0.058): 0.034*\"ace\" + 0.028*\"alex\" + 0.020*\"nantz\" + 0.009*\"sergeant\" + 0.008*\"finkle\" + 0.008*\"melissa\" + 0.007*\"kase\" + 0.006*\"imlay\" + 0.006*\"einhorn\" + 0.005*\"hector\"\n",
      "2019-05-10 17:29:32,819 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:32,822 : INFO : topic #1 (0.155): 0.014*\"papa\" + 0.009*\"monsieur\" + 0.009*\"revision\" + 0.007*\"la\" + 0.007*\"de\" + 0.006*\"pink\" + 0.006*\"herr\" + 0.006*\"scully\" + 0.005*\"der\" + 0.005*\"rowan\"\n",
      "2019-05-10 17:29:32,824 : INFO : topic #0 (0.203): 0.084*\"harry\" + 0.023*\"scene\" + 0.022*\"day\" + 0.020*\"ext\" + 0.017*\"int\" + 0.016*\"trench\" + 0.013*\"darcy\" + 0.012*\"durrance\" + 0.012*\"blaze\" + 0.009*\"s\"\n",
      "2019-05-10 17:29:32,827 : INFO : topic diff=0.056949, rho=0.094876\n",
      "2019-05-10 17:29:32,830 : INFO : PROGRESS: pass 5, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 7\n",
      "2019-05-10 17:29:32,832 : INFO : PROGRESS: pass 5, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:32,834 : INFO : PROGRESS: pass 5, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:33,452 : INFO : PROGRESS: pass 5, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:33,595 : INFO : PROGRESS: pass 5, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:33,867 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:33,958 : INFO : topic #9 (0.053): 0.038*\"b\" + 0.030*\"g\" + 0.020*\"camera\" + 0.018*\"wendy\" + 0.016*\"phillip\" + 0.014*\"wichita\" + 0.013*\"elwood\" + 0.013*\"tao\" + 0.012*\"kelly\" + 0.009*\"wilson\"\n",
      "2019-05-10 17:29:33,960 : INFO : topic #8 (0.058): 0.033*\"ace\" + 0.027*\"alex\" + 0.018*\"nantz\" + 0.010*\"sergeant\" + 0.007*\"finkle\" + 0.007*\"melissa\" + 0.007*\"kase\" + 0.006*\"hector\" + 0.005*\"imlay\" + 0.005*\"einhorn\"\n",
      "2019-05-10 17:29:33,962 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:33,964 : INFO : topic #1 (0.155): 0.013*\"papa\" + 0.009*\"la\" + 0.009*\"monsieur\" + 0.008*\"de\" + 0.008*\"revision\" + 0.007*\"pink\" + 0.005*\"herr\" + 0.005*\"scully\" + 0.004*\"der\" + 0.004*\"rowan\"\n",
      "2019-05-10 17:29:33,967 : INFO : topic #0 (0.203): 0.079*\"harry\" + 0.022*\"scene\" + 0.020*\"day\" + 0.020*\"ext\" + 0.017*\"int\" + 0.015*\"frodo\" + 0.014*\"trench\" + 0.012*\"darcy\" + 0.011*\"blaze\" + 0.011*\"durrance\"\n",
      "2019-05-10 17:29:33,970 : INFO : topic diff=0.033554, rho=0.094876\n",
      "2019-05-10 17:29:33,972 : INFO : PROGRESS: pass 5, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:34,546 : INFO : PROGRESS: pass 5, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:34,692 : INFO : PROGRESS: pass 5, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:34,968 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:35,079 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.029*\"g\" + 0.019*\"camera\" + 0.016*\"wendy\" + 0.015*\"phillip\" + 0.014*\"harvey\" + 0.013*\"wichita\" + 0.012*\"elwood\" + 0.012*\"tao\" + 0.012*\"kelly\"\n",
      "2019-05-10 17:29:35,082 : INFO : topic #8 (0.058): 0.031*\"ace\" + 0.025*\"alex\" + 0.017*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"finkle\" + 0.007*\"melissa\" + 0.006*\"kase\" + 0.006*\"hector\" + 0.005*\"imlay\" + 0.005*\"einhorn\"\n",
      "2019-05-10 17:29:35,084 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:35,087 : INFO : topic #1 (0.155): 0.012*\"papa\" + 0.011*\"monsieur\" + 0.009*\"la\" + 0.008*\"de\" + 0.007*\"revision\" + 0.007*\"pink\" + 0.005*\"herr\" + 0.005*\"scully\" + 0.004*\"der\" + 0.004*\"rowan\"\n",
      "2019-05-10 17:29:35,088 : INFO : topic #0 (0.203): 0.075*\"harry\" + 0.021*\"scene\" + 0.019*\"day\" + 0.019*\"ext\" + 0.016*\"int\" + 0.014*\"frodo\" + 0.013*\"trench\" + 0.011*\"darcy\" + 0.010*\"blaze\" + 0.010*\"durrance\"\n",
      "2019-05-10 17:29:35,092 : INFO : topic diff=0.032537, rho=0.094876\n",
      "2019-05-10 17:29:35,095 : INFO : PROGRESS: pass 5, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:35,776 : INFO : PROGRESS: pass 5, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:35,985 : INFO : PROGRESS: pass 5, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:36,170 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:36,262 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.028*\"g\" + 0.018*\"camera\" + 0.016*\"wendy\" + 0.014*\"phillip\" + 0.013*\"harvey\" + 0.012*\"wichita\" + 0.012*\"elwood\" + 0.011*\"kelly\" + 0.011*\"tao\"\n",
      "2019-05-10 17:29:36,264 : INFO : topic #8 (0.058): 0.029*\"ace\" + 0.025*\"alex\" + 0.016*\"nantz\" + 0.009*\"sergeant\" + 0.006*\"finkle\" + 0.006*\"melissa\" + 0.006*\"kase\" + 0.005*\"harris\" + 0.005*\"hector\" + 0.005*\"imlay\"\n",
      "2019-05-10 17:29:36,266 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:36,268 : INFO : topic #1 (0.155): 0.012*\"papa\" + 0.011*\"monsieur\" + 0.010*\"de\" + 0.008*\"la\" + 0.007*\"revision\" + 0.006*\"pink\" + 0.005*\"herr\" + 0.005*\"scully\" + 0.004*\"der\" + 0.004*\"est\"\n",
      "2019-05-10 17:29:36,270 : INFO : topic #0 (0.203): 0.073*\"harry\" + 0.020*\"scene\" + 0.019*\"day\" + 0.018*\"ext\" + 0.016*\"int\" + 0.013*\"frodo\" + 0.012*\"trench\" + 0.011*\"darcy\" + 0.010*\"blaze\" + 0.010*\"willoughby\"\n",
      "2019-05-10 17:29:36,272 : INFO : topic diff=0.034334, rho=0.094876\n",
      "2019-05-10 17:29:36,275 : INFO : PROGRESS: pass 5, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:36,841 : INFO : PROGRESS: pass 5, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:36,998 : INFO : PROGRESS: pass 5, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:37,311 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:37,407 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.026*\"g\" + 0.022*\"phillip\" + 0.017*\"wendy\" + 0.016*\"camera\" + 0.013*\"terrance\" + 0.012*\"harvey\" + 0.011*\"wichita\" + 0.010*\"elwood\" + 0.010*\"kelly\"\n",
      "2019-05-10 17:29:37,410 : INFO : topic #8 (0.058): 0.028*\"ace\" + 0.023*\"alex\" + 0.014*\"nantz\" + 0.009*\"sergeant\" + 0.006*\"finkle\" + 0.006*\"melissa\" + 0.005*\"kase\" + 0.005*\"harris\" + 0.005*\"hector\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:29:37,412 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:37,414 : INFO : topic #1 (0.155): 0.011*\"papa\" + 0.009*\"monsieur\" + 0.009*\"jamal\" + 0.009*\"de\" + 0.008*\"la\" + 0.006*\"revision\" + 0.006*\"pink\" + 0.006*\"rupees\" + 0.005*\"herr\" + 0.005*\"ze\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:37,416 : INFO : topic #0 (0.203): 0.082*\"harry\" + 0.020*\"scene\" + 0.018*\"day\" + 0.017*\"ext\" + 0.015*\"int\" + 0.013*\"frodo\" + 0.012*\"trench\" + 0.010*\"darcy\" + 0.009*\"blaze\" + 0.009*\"night\"\n",
      "2019-05-10 17:29:37,418 : INFO : topic diff=0.045633, rho=0.094876\n",
      "2019-05-10 17:29:37,421 : INFO : PROGRESS: pass 5, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:37,925 : INFO : PROGRESS: pass 5, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:38,191 : INFO : PROGRESS: pass 5, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:38,458 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:38,548 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.025*\"g\" + 0.021*\"phillip\" + 0.016*\"camera\" + 0.016*\"wendy\" + 0.013*\"terrance\" + 0.011*\"harvey\" + 0.011*\"kelly\" + 0.010*\"wichita\" + 0.010*\"elwood\"\n",
      "2019-05-10 17:29:38,550 : INFO : topic #8 (0.058): 0.028*\"ace\" + 0.023*\"alex\" + 0.013*\"nantz\" + 0.008*\"sergeant\" + 0.005*\"melissa\" + 0.005*\"finkle\" + 0.005*\"kase\" + 0.005*\"harris\" + 0.004*\"hector\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:29:38,552 : INFO : topic #2 (0.125): 0.076*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:38,554 : INFO : topic #1 (0.155): 0.010*\"papa\" + 0.010*\"de\" + 0.009*\"la\" + 0.009*\"monsieur\" + 0.008*\"jamal\" + 0.005*\"dickie\" + 0.005*\"pink\" + 0.005*\"revision\" + 0.005*\"der\" + 0.005*\"rupees\"\n",
      "2019-05-10 17:29:38,557 : INFO : topic #0 (0.203): 0.077*\"harry\" + 0.018*\"scene\" + 0.017*\"day\" + 0.016*\"ext\" + 0.014*\"int\" + 0.011*\"frodo\" + 0.011*\"trench\" + 0.009*\"darcy\" + 0.009*\"night\" + 0.009*\"blaze\"\n",
      "2019-05-10 17:29:38,560 : INFO : topic diff=0.038814, rho=0.094876\n",
      "2019-05-10 17:29:38,562 : INFO : PROGRESS: pass 5, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:38,898 : INFO : PROGRESS: pass 5, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:39,514 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:39,634 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.026*\"g\" + 0.019*\"phillip\" + 0.016*\"wendy\" + 0.015*\"camera\" + 0.012*\"terrance\" + 0.011*\"kelly\" + 0.010*\"harvey\" + 0.010*\"wichita\" + 0.009*\"elwood\"\n",
      "2019-05-10 17:29:39,639 : INFO : topic #8 (0.058): 0.026*\"ace\" + 0.021*\"alex\" + 0.012*\"nantz\" + 0.008*\"sergeant\" + 0.007*\"melissa\" + 0.005*\"finkle\" + 0.005*\"wigand\" + 0.005*\"kase\" + 0.005*\"harris\" + 0.004*\"hector\"\n",
      "2019-05-10 17:29:39,644 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:39,646 : INFO : topic #1 (0.155): 0.011*\"papa\" + 0.011*\"de\" + 0.010*\"la\" + 0.010*\"monsieur\" + 0.007*\"jamal\" + 0.006*\"ze\" + 0.005*\"pink\" + 0.005*\"ja\" + 0.005*\"dickie\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:39,648 : INFO : topic #0 (0.203): 0.076*\"harry\" + 0.019*\"frodo\" + 0.017*\"scene\" + 0.016*\"day\" + 0.015*\"ext\" + 0.013*\"int\" + 0.010*\"trench\" + 0.009*\"gandalf\" + 0.009*\"darcy\" + 0.008*\"night\"\n",
      "2019-05-10 17:29:39,653 : INFO : topic diff=0.033176, rho=0.094876\n",
      "2019-05-10 17:29:40,684 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:40,747 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.026*\"g\" + 0.018*\"phillip\" + 0.016*\"wendy\" + 0.014*\"camera\" + 0.011*\"terrance\" + 0.010*\"kelly\" + 0.010*\"harvey\" + 0.009*\"wichita\" + 0.008*\"elwood\"\n",
      "2019-05-10 17:29:40,750 : INFO : topic #8 (0.058): 0.024*\"ace\" + 0.022*\"alex\" + 0.011*\"nantz\" + 0.008*\"sergeant\" + 0.007*\"melissa\" + 0.004*\"finkle\" + 0.004*\"wigand\" + 0.004*\"kase\" + 0.004*\"harris\" + 0.004*\"hector\"\n",
      "2019-05-10 17:29:40,753 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:40,754 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.011*\"la\" + 0.010*\"de\" + 0.010*\"scully\" + 0.008*\"monsieur\" + 0.008*\"rowan\" + 0.006*\"sie\" + 0.006*\"jamal\" + 0.006*\"ja\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:40,759 : INFO : topic #0 (0.203): 0.079*\"harry\" + 0.018*\"frodo\" + 0.016*\"scene\" + 0.015*\"day\" + 0.014*\"ext\" + 0.012*\"int\" + 0.010*\"trench\" + 0.009*\"gandalf\" + 0.008*\"darcy\" + 0.008*\"night\"\n",
      "2019-05-10 17:29:40,762 : INFO : topic diff=0.038556, rho=0.094876\n",
      "2019-05-10 17:29:41,700 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:41,744 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.024*\"g\" + 0.017*\"phillip\" + 0.014*\"wendy\" + 0.014*\"kelly\" + 0.014*\"camera\" + 0.010*\"terrance\" + 0.009*\"harvey\" + 0.009*\"wichita\" + 0.008*\"elwood\"\n",
      "2019-05-10 17:29:41,747 : INFO : topic #8 (0.058): 0.039*\"alex\" + 0.022*\"ace\" + 0.010*\"nantz\" + 0.008*\"sergeant\" + 0.006*\"melissa\" + 0.004*\"harris\" + 0.004*\"finkle\" + 0.004*\"wigand\" + 0.004*\"kase\" + 0.004*\"hector\"\n",
      "2019-05-10 17:29:41,748 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:41,750 : INFO : topic #1 (0.155): 0.014*\"papa\" + 0.010*\"la\" + 0.010*\"de\" + 0.009*\"scully\" + 0.008*\"monsieur\" + 0.008*\"rowan\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.005*\"ja\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:41,752 : INFO : topic #0 (0.203): 0.066*\"harry\" + 0.060*\"scene\" + 0.026*\"darcy\" + 0.015*\"ms\" + 0.015*\"day\" + 0.014*\"frodo\" + 0.014*\"mcu\" + 0.013*\"ext\" + 0.012*\"int\" + 0.012*\"ots\"\n",
      "2019-05-10 17:29:41,753 : INFO : topic diff=0.039163, rho=0.094876\n",
      "2019-05-10 17:29:41,756 : INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:29:41,831 : INFO : PROGRESS: pass 6, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:29:41,832 : INFO : PROGRESS: pass 6, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:29:41,833 : INFO : PROGRESS: pass 6, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:29:41,834 : INFO : PROGRESS: pass 6, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:29:41,836 : INFO : PROGRESS: pass 6, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:29:41,936 : INFO : PROGRESS: pass 6, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:29:42,064 : INFO : PROGRESS: pass 6, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:42,175 : INFO : PROGRESS: pass 6, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:42,890 : INFO : PROGRESS: pass 6, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:42,953 : INFO : PROGRESS: pass 6, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:43,273 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:43,335 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.025*\"phillip\" + 0.023*\"g\" + 0.014*\"butch\" + 0.013*\"kelly\" + 0.013*\"wendy\" + 0.012*\"camera\" + 0.012*\"harvey\" + 0.011*\"dickson\" + 0.009*\"terrance\"\n",
      "2019-05-10 17:29:43,345 : INFO : topic #8 (0.058): 0.052*\"ace\" + 0.050*\"alex\" + 0.013*\"finkle\" + 0.010*\"melissa\" + 0.010*\"einhorn\" + 0.008*\"snowflake\" + 0.008*\"nantz\" + 0.008*\"sergeant\" + 0.007*\"marino\" + 0.004*\"dolphin\"\n",
      "2019-05-10 17:29:43,350 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:43,357 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.010*\"de\" + 0.009*\"la\" + 0.008*\"scully\" + 0.008*\"mozart\" + 0.007*\"monsieur\" + 0.007*\"herr\" + 0.007*\"rowan\" + 0.005*\"sie\" + 0.005*\"jamal\"\n",
      "2019-05-10 17:29:43,363 : INFO : topic #0 (0.203): 0.064*\"harry\" + 0.058*\"scene\" + 0.025*\"darcy\" + 0.014*\"ms\" + 0.014*\"day\" + 0.013*\"frodo\" + 0.013*\"mcu\" + 0.013*\"ext\" + 0.012*\"int\" + 0.011*\"ots\"\n",
      "2019-05-10 17:29:43,366 : INFO : topic diff=0.041749, rho=0.094451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:43,369 : INFO : PROGRESS: pass 6, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:43,766 : INFO : PROGRESS: pass 6, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:44,348 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:44,446 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.023*\"g\" + 0.023*\"phillip\" + 0.013*\"butch\" + 0.013*\"wendy\" + 0.012*\"camera\" + 0.012*\"kelly\" + 0.012*\"harvey\" + 0.010*\"dickson\" + 0.009*\"lowrey\"\n",
      "2019-05-10 17:29:44,449 : INFO : topic #8 (0.058): 0.044*\"ace\" + 0.042*\"alex\" + 0.027*\"nantz\" + 0.012*\"sergeant\" + 0.011*\"finkle\" + 0.009*\"hector\" + 0.009*\"melissa\" + 0.008*\"einhorn\" + 0.008*\"imlay\" + 0.008*\"marines\"\n",
      "2019-05-10 17:29:44,452 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:44,454 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.009*\"de\" + 0.009*\"la\" + 0.008*\"monsieur\" + 0.008*\"scully\" + 0.007*\"mozart\" + 0.007*\"herr\" + 0.007*\"rowan\" + 0.005*\"sie\" + 0.005*\"jamal\"\n",
      "2019-05-10 17:29:44,456 : INFO : topic #0 (0.203): 0.066*\"harry\" + 0.056*\"scene\" + 0.024*\"darcy\" + 0.014*\"ms\" + 0.014*\"day\" + 0.012*\"frodo\" + 0.012*\"mcu\" + 0.012*\"ext\" + 0.012*\"int\" + 0.010*\"ots\"\n",
      "2019-05-10 17:29:44,459 : INFO : topic diff=0.056415, rho=0.094451\n",
      "2019-05-10 17:29:44,463 : INFO : PROGRESS: pass 6, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:44,466 : INFO : PROGRESS: pass 6, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:44,800 : INFO : PROGRESS: pass 6, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:45,285 : INFO : PROGRESS: pass 6, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:45,475 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:45,538 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.023*\"g\" + 0.023*\"phillip\" + 0.013*\"butch\" + 0.013*\"kelly\" + 0.012*\"camera\" + 0.012*\"wendy\" + 0.011*\"harvey\" + 0.009*\"dickson\" + 0.009*\"lowrey\"\n",
      "2019-05-10 17:29:45,540 : INFO : topic #8 (0.058): 0.044*\"ace\" + 0.040*\"alex\" + 0.025*\"nantz\" + 0.011*\"sergeant\" + 0.010*\"finkle\" + 0.009*\"hector\" + 0.008*\"melissa\" + 0.007*\"einhorn\" + 0.007*\"imlay\" + 0.007*\"marines\"\n",
      "2019-05-10 17:29:45,542 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:45,544 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.011*\"monsieur\" + 0.009*\"de\" + 0.009*\"la\" + 0.007*\"herr\" + 0.007*\"scully\" + 0.007*\"mozart\" + 0.006*\"rowan\" + 0.005*\"sie\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:45,546 : INFO : topic #0 (0.203): 0.062*\"harry\" + 0.051*\"scene\" + 0.021*\"darcy\" + 0.020*\"day\" + 0.017*\"int\" + 0.014*\"ext\" + 0.013*\"ms\" + 0.011*\"frodo\" + 0.011*\"mcu\" + 0.010*\"s\"\n",
      "2019-05-10 17:29:45,552 : INFO : topic diff=0.031115, rho=0.094451\n",
      "2019-05-10 17:29:45,555 : INFO : PROGRESS: pass 6, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:45,745 : INFO : PROGRESS: pass 6, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:46,204 : INFO : PROGRESS: pass 6, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:46,586 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:46,648 : INFO : topic #9 (0.053): 0.042*\"b\" + 0.022*\"g\" + 0.021*\"phillip\" + 0.012*\"butch\" + 0.012*\"kelly\" + 0.012*\"camera\" + 0.011*\"wendy\" + 0.011*\"harvey\" + 0.009*\"donnie\" + 0.009*\"dickson\"\n",
      "2019-05-10 17:29:46,651 : INFO : topic #8 (0.058): 0.041*\"ace\" + 0.038*\"alex\" + 0.023*\"nantz\" + 0.011*\"sergeant\" + 0.010*\"finkle\" + 0.009*\"melissa\" + 0.008*\"hector\" + 0.007*\"einhorn\" + 0.007*\"imlay\" + 0.007*\"marines\"\n",
      "2019-05-10 17:29:46,653 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:46,654 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.011*\"monsieur\" + 0.010*\"revision\" + 0.009*\"de\" + 0.009*\"la\" + 0.007*\"herr\" + 0.007*\"scully\" + 0.006*\"mozart\" + 0.006*\"pink\" + 0.005*\"rowan\"\n",
      "2019-05-10 17:29:46,657 : INFO : topic #0 (0.203): 0.064*\"harry\" + 0.048*\"scene\" + 0.020*\"darcy\" + 0.019*\"int\" + 0.019*\"day\" + 0.015*\"ext\" + 0.013*\"ms\" + 0.010*\"frodo\" + 0.010*\"mcu\" + 0.010*\"s\"\n",
      "2019-05-10 17:29:46,659 : INFO : topic diff=0.032696, rho=0.094451\n",
      "2019-05-10 17:29:46,661 : INFO : PROGRESS: pass 6, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:46,895 : INFO : PROGRESS: pass 6, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:47,427 : INFO : PROGRESS: pass 6, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:47,689 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:47,755 : INFO : topic #9 (0.053): 0.039*\"b\" + 0.021*\"g\" + 0.019*\"phillip\" + 0.019*\"tao\" + 0.011*\"butch\" + 0.011*\"kelly\" + 0.011*\"camera\" + 0.010*\"wendy\" + 0.010*\"harvey\" + 0.008*\"donnie\"\n",
      "2019-05-10 17:29:47,757 : INFO : topic #8 (0.058): 0.039*\"alex\" + 0.038*\"ace\" + 0.022*\"nantz\" + 0.010*\"sergeant\" + 0.009*\"finkle\" + 0.008*\"hector\" + 0.008*\"melissa\" + 0.006*\"einhorn\" + 0.006*\"imlay\" + 0.006*\"marines\"\n",
      "2019-05-10 17:29:47,761 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:47,763 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.010*\"monsieur\" + 0.009*\"revision\" + 0.009*\"de\" + 0.009*\"la\" + 0.006*\"herr\" + 0.006*\"scully\" + 0.006*\"mozart\" + 0.006*\"pink\" + 0.005*\"rowan\"\n",
      "2019-05-10 17:29:47,765 : INFO : topic #0 (0.203): 0.086*\"harry\" + 0.036*\"scene\" + 0.021*\"day\" + 0.020*\"ext\" + 0.017*\"int\" + 0.016*\"trench\" + 0.015*\"darcy\" + 0.012*\"blaze\" + 0.012*\"durrance\" + 0.010*\"ms\"\n",
      "2019-05-10 17:29:47,767 : INFO : topic diff=0.050197, rho=0.094451\n",
      "2019-05-10 17:29:47,770 : INFO : PROGRESS: pass 6, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:48,357 : INFO : PROGRESS: pass 6, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:48,526 : INFO : PROGRESS: pass 6, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:48,833 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:48,897 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.029*\"g\" + 0.021*\"camera\" + 0.018*\"wendy\" + 0.016*\"phillip\" + 0.015*\"wichita\" + 0.014*\"elwood\" + 0.014*\"tao\" + 0.013*\"kelly\" + 0.010*\"harvey\"\n",
      "2019-05-10 17:29:48,900 : INFO : topic #8 (0.058): 0.041*\"alex\" + 0.035*\"ace\" + 0.020*\"nantz\" + 0.010*\"kase\" + 0.010*\"sergeant\" + 0.008*\"finkle\" + 0.008*\"melissa\" + 0.007*\"hector\" + 0.006*\"einhorn\" + 0.006*\"imlay\"\n",
      "2019-05-10 17:29:48,904 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:48,905 : INFO : topic #1 (0.155): 0.014*\"papa\" + 0.011*\"monsieur\" + 0.008*\"de\" + 0.008*\"revision\" + 0.008*\"la\" + 0.006*\"herr\" + 0.006*\"pink\" + 0.006*\"marylin\" + 0.006*\"scully\" + 0.006*\"mozart\"\n",
      "2019-05-10 17:29:48,907 : INFO : topic #0 (0.203): 0.084*\"harry\" + 0.036*\"scene\" + 0.021*\"day\" + 0.019*\"ext\" + 0.016*\"int\" + 0.015*\"trench\" + 0.014*\"darcy\" + 0.012*\"blaze\" + 0.011*\"durrance\" + 0.010*\"ms\"\n",
      "2019-05-10 17:29:48,910 : INFO : topic diff=0.053932, rho=0.094451\n",
      "2019-05-10 17:29:48,914 : INFO : PROGRESS: pass 6, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:49,464 : INFO : PROGRESS: pass 6, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:49,738 : INFO : PROGRESS: pass 6, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:49,927 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:50,023 : INFO : topic #9 (0.053): 0.039*\"b\" + 0.029*\"g\" + 0.020*\"camera\" + 0.018*\"wendy\" + 0.015*\"phillip\" + 0.014*\"wichita\" + 0.013*\"elwood\" + 0.013*\"tao\" + 0.013*\"kelly\" + 0.013*\"harvey\"\n",
      "2019-05-10 17:29:50,026 : INFO : topic #8 (0.058): 0.040*\"alex\" + 0.033*\"ace\" + 0.018*\"nantz\" + 0.011*\"sergeant\" + 0.009*\"kase\" + 0.009*\"hector\" + 0.008*\"melissa\" + 0.007*\"finkle\" + 0.005*\"einhorn\" + 0.005*\"imlay\"\n",
      "2019-05-10 17:29:50,028 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:50,030 : INFO : topic #1 (0.155): 0.013*\"papa\" + 0.010*\"monsieur\" + 0.009*\"la\" + 0.009*\"de\" + 0.008*\"revision\" + 0.007*\"pink\" + 0.006*\"herr\" + 0.005*\"marylin\" + 0.005*\"scully\" + 0.005*\"mozart\"\n",
      "2019-05-10 17:29:50,031 : INFO : topic #0 (0.203): 0.080*\"harry\" + 0.034*\"scene\" + 0.020*\"day\" + 0.019*\"ext\" + 0.016*\"int\" + 0.014*\"frodo\" + 0.014*\"trench\" + 0.013*\"darcy\" + 0.010*\"blaze\" + 0.010*\"night\"\n",
      "2019-05-10 17:29:50,034 : INFO : topic diff=0.030372, rho=0.094451\n",
      "2019-05-10 17:29:50,037 : INFO : PROGRESS: pass 6, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:50,715 : INFO : PROGRESS: pass 6, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:50,904 : INFO : PROGRESS: pass 6, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:51,115 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:51,195 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.028*\"g\" + 0.023*\"harvey\" + 0.019*\"camera\" + 0.016*\"wendy\" + 0.015*\"phillip\" + 0.013*\"wichita\" + 0.013*\"kelly\" + 0.012*\"elwood\" + 0.012*\"tao\"\n",
      "2019-05-10 17:29:51,198 : INFO : topic #8 (0.058): 0.037*\"alex\" + 0.031*\"ace\" + 0.017*\"nantz\" + 0.010*\"sergeant\" + 0.009*\"kase\" + 0.008*\"hector\" + 0.007*\"melissa\" + 0.007*\"finkle\" + 0.005*\"einhorn\" + 0.005*\"imlay\"\n",
      "2019-05-10 17:29:51,202 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:51,205 : INFO : topic #1 (0.155): 0.013*\"papa\" + 0.013*\"monsieur\" + 0.010*\"de\" + 0.009*\"la\" + 0.007*\"revision\" + 0.007*\"pink\" + 0.005*\"herr\" + 0.005*\"scully\" + 0.005*\"marylin\" + 0.005*\"mozart\"\n",
      "2019-05-10 17:29:51,207 : INFO : topic #0 (0.203): 0.076*\"harry\" + 0.033*\"scene\" + 0.019*\"day\" + 0.018*\"ext\" + 0.015*\"int\" + 0.013*\"frodo\" + 0.013*\"trench\" + 0.012*\"darcy\" + 0.010*\"blaze\" + 0.009*\"night\"\n",
      "2019-05-10 17:29:51,212 : INFO : topic diff=0.029580, rho=0.094451\n",
      "2019-05-10 17:29:51,216 : INFO : PROGRESS: pass 6, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:51,866 : INFO : PROGRESS: pass 6, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:51,878 : INFO : PROGRESS: pass 6, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:52,286 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:52,350 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.027*\"g\" + 0.022*\"harvey\" + 0.019*\"camera\" + 0.016*\"wendy\" + 0.014*\"phillip\" + 0.012*\"kelly\" + 0.012*\"wichita\" + 0.011*\"tao\" + 0.011*\"elwood\"\n",
      "2019-05-10 17:29:52,352 : INFO : topic #8 (0.058): 0.037*\"alex\" + 0.030*\"ace\" + 0.015*\"nantz\" + 0.009*\"sergeant\" + 0.008*\"kase\" + 0.008*\"hector\" + 0.006*\"melissa\" + 0.006*\"finkle\" + 0.006*\"harris\" + 0.005*\"einhorn\"\n",
      "2019-05-10 17:29:52,354 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.016*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:52,356 : INFO : topic #1 (0.155): 0.013*\"papa\" + 0.013*\"monsieur\" + 0.011*\"de\" + 0.009*\"la\" + 0.006*\"revision\" + 0.006*\"pink\" + 0.005*\"herr\" + 0.005*\"scully\" + 0.005*\"marylin\" + 0.004*\"mozart\"\n",
      "2019-05-10 17:29:52,358 : INFO : topic #0 (0.203): 0.074*\"harry\" + 0.032*\"scene\" + 0.019*\"day\" + 0.017*\"ext\" + 0.015*\"int\" + 0.013*\"frodo\" + 0.012*\"trench\" + 0.011*\"darcy\" + 0.010*\"blaze\" + 0.009*\"night\"\n",
      "2019-05-10 17:29:52,361 : INFO : topic diff=0.031805, rho=0.094451\n",
      "2019-05-10 17:29:52,364 : INFO : PROGRESS: pass 6, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:52,918 : INFO : PROGRESS: pass 6, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:53,025 : INFO : PROGRESS: pass 6, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:53,353 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:53,418 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.025*\"g\" + 0.021*\"phillip\" + 0.020*\"harvey\" + 0.017*\"camera\" + 0.016*\"wendy\" + 0.013*\"terrance\" + 0.011*\"kelly\" + 0.011*\"wichita\" + 0.010*\"tao\"\n",
      "2019-05-10 17:29:53,421 : INFO : topic #8 (0.058): 0.034*\"alex\" + 0.029*\"ace\" + 0.014*\"nantz\" + 0.010*\"sergeant\" + 0.007*\"kase\" + 0.007*\"hector\" + 0.006*\"harris\" + 0.006*\"melissa\" + 0.006*\"finkle\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:29:53,423 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:53,425 : INFO : topic #1 (0.155): 0.011*\"papa\" + 0.011*\"monsieur\" + 0.010*\"de\" + 0.009*\"jamal\" + 0.009*\"la\" + 0.006*\"pink\" + 0.006*\"revision\" + 0.006*\"rupees\" + 0.005*\"herr\" + 0.004*\"ze\"\n",
      "2019-05-10 17:29:53,427 : INFO : topic #0 (0.203): 0.083*\"harry\" + 0.032*\"scene\" + 0.018*\"day\" + 0.016*\"ext\" + 0.014*\"int\" + 0.012*\"frodo\" + 0.012*\"trench\" + 0.011*\"darcy\" + 0.009*\"blaze\" + 0.009*\"night\"\n",
      "2019-05-10 17:29:53,429 : INFO : topic diff=0.042449, rho=0.094451\n",
      "2019-05-10 17:29:53,432 : INFO : PROGRESS: pass 6, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:53,845 : INFO : PROGRESS: pass 6, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:54,083 : INFO : PROGRESS: pass 6, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:54,376 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:54,476 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.024*\"g\" + 0.020*\"phillip\" + 0.019*\"harvey\" + 0.016*\"camera\" + 0.015*\"wendy\" + 0.012*\"terrance\" + 0.012*\"kelly\" + 0.010*\"wichita\" + 0.009*\"tao\"\n",
      "2019-05-10 17:29:54,478 : INFO : topic #8 (0.058): 0.034*\"alex\" + 0.029*\"ace\" + 0.013*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"kase\" + 0.006*\"hector\" + 0.006*\"melissa\" + 0.006*\"harris\" + 0.005*\"finkle\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:29:54,481 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:54,483 : INFO : topic #1 (0.155): 0.011*\"de\" + 0.011*\"monsieur\" + 0.010*\"papa\" + 0.010*\"la\" + 0.008*\"jamal\" + 0.008*\"dickie\" + 0.005*\"pink\" + 0.005*\"der\" + 0.005*\"revision\" + 0.005*\"herr\"\n",
      "2019-05-10 17:29:54,486 : INFO : topic #0 (0.203): 0.078*\"harry\" + 0.030*\"scene\" + 0.017*\"day\" + 0.015*\"ext\" + 0.013*\"int\" + 0.011*\"frodo\" + 0.011*\"trench\" + 0.010*\"darcy\" + 0.009*\"night\" + 0.009*\"ms\"\n",
      "2019-05-10 17:29:54,490 : INFO : topic diff=0.036721, rho=0.094451\n",
      "2019-05-10 17:29:54,492 : INFO : PROGRESS: pass 6, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:54,945 : INFO : PROGRESS: pass 6, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:55,495 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:55,591 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.025*\"g\" + 0.019*\"phillip\" + 0.018*\"harvey\" + 0.015*\"wendy\" + 0.015*\"camera\" + 0.012*\"kelly\" + 0.012*\"terrance\" + 0.010*\"wichita\" + 0.009*\"tao\"\n",
      "2019-05-10 17:29:55,593 : INFO : topic #8 (0.058): 0.032*\"alex\" + 0.027*\"ace\" + 0.012*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"melissa\" + 0.006*\"kase\" + 0.006*\"hector\" + 0.005*\"wigand\" + 0.005*\"harris\" + 0.005*\"finkle\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:29:55,595 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.025*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:55,597 : INFO : topic #1 (0.155): 0.012*\"de\" + 0.012*\"papa\" + 0.011*\"monsieur\" + 0.010*\"la\" + 0.007*\"jamal\" + 0.007*\"dickie\" + 0.006*\"ze\" + 0.005*\"pink\" + 0.005*\"ja\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:55,600 : INFO : topic #0 (0.203): 0.077*\"harry\" + 0.028*\"scene\" + 0.018*\"frodo\" + 0.016*\"day\" + 0.014*\"ext\" + 0.012*\"int\" + 0.010*\"trench\" + 0.009*\"gandalf\" + 0.009*\"darcy\" + 0.008*\"night\"\n",
      "2019-05-10 17:29:55,602 : INFO : topic diff=0.029992, rho=0.094451\n",
      "2019-05-10 17:29:56,521 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:56,583 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.024*\"g\" + 0.018*\"phillip\" + 0.016*\"harvey\" + 0.016*\"wendy\" + 0.015*\"camera\" + 0.012*\"kelly\" + 0.011*\"terrance\" + 0.009*\"wichita\" + 0.008*\"tao\"\n",
      "2019-05-10 17:29:56,585 : INFO : topic #8 (0.058): 0.033*\"alex\" + 0.025*\"ace\" + 0.011*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"melissa\" + 0.006*\"kase\" + 0.005*\"hector\" + 0.005*\"wigand\" + 0.005*\"harris\" + 0.005*\"finkle\"\n",
      "2019-05-10 17:29:56,587 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:56,589 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.011*\"la\" + 0.011*\"de\" + 0.010*\"monsieur\" + 0.010*\"scully\" + 0.008*\"rowan\" + 0.006*\"sie\" + 0.006*\"jamal\" + 0.006*\"dickie\" + 0.006*\"ja\"\n",
      "2019-05-10 17:29:56,592 : INFO : topic #0 (0.203): 0.080*\"harry\" + 0.027*\"scene\" + 0.017*\"frodo\" + 0.015*\"day\" + 0.013*\"ext\" + 0.012*\"int\" + 0.010*\"trench\" + 0.009*\"gandalf\" + 0.009*\"darcy\" + 0.008*\"night\"\n",
      "2019-05-10 17:29:56,594 : INFO : topic diff=0.036945, rho=0.094451\n",
      "2019-05-10 17:29:57,433 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:57,476 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.023*\"g\" + 0.017*\"phillip\" + 0.015*\"kelly\" + 0.015*\"harvey\" + 0.014*\"wendy\" + 0.014*\"camera\" + 0.010*\"terrance\" + 0.009*\"madame\" + 0.009*\"wichita\"\n",
      "2019-05-10 17:29:57,478 : INFO : topic #8 (0.058): 0.049*\"alex\" + 0.023*\"ace\" + 0.010*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"hector\" + 0.006*\"melissa\" + 0.005*\"kase\" + 0.005*\"harris\" + 0.005*\"wigand\" + 0.004*\"finkle\"\n",
      "2019-05-10 17:29:57,479 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:57,480 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.010*\"la\" + 0.010*\"de\" + 0.009*\"monsieur\" + 0.009*\"scully\" + 0.008*\"rowan\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.005*\"ja\" + 0.005*\"dickie\"\n",
      "2019-05-10 17:29:57,482 : INFO : topic #0 (0.203): 0.067*\"scene\" + 0.065*\"harry\" + 0.031*\"nick\" + 0.025*\"darcy\" + 0.019*\"ms\" + 0.014*\"day\" + 0.013*\"mcu\" + 0.013*\"frodo\" + 0.012*\"ext\" + 0.012*\"dan\"\n",
      "2019-05-10 17:29:57,484 : INFO : topic diff=0.038817, rho=0.094451\n",
      "2019-05-10 17:29:57,487 : INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:29:57,565 : INFO : PROGRESS: pass 7, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:29:57,566 : INFO : PROGRESS: pass 7, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:29:57,568 : INFO : PROGRESS: pass 7, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:29:57,570 : INFO : PROGRESS: pass 7, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:29:57,571 : INFO : PROGRESS: pass 7, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:29:57,594 : INFO : PROGRESS: pass 7, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:29:57,779 : INFO : PROGRESS: pass 7, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:29:57,839 : INFO : PROGRESS: pass 7, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:58,506 : INFO : PROGRESS: pass 7, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:58,773 : INFO : PROGRESS: pass 7, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:58,976 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:29:59,037 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.025*\"phillip\" + 0.022*\"g\" + 0.018*\"harvey\" + 0.015*\"kelly\" + 0.014*\"butch\" + 0.013*\"wendy\" + 0.013*\"camera\" + 0.011*\"dickson\" + 0.009*\"terrance\"\n",
      "2019-05-10 17:29:59,039 : INFO : topic #8 (0.058): 0.059*\"alex\" + 0.051*\"ace\" + 0.013*\"finkle\" + 0.010*\"melissa\" + 0.009*\"einhorn\" + 0.008*\"snowflake\" + 0.008*\"sergeant\" + 0.008*\"nantz\" + 0.007*\"marino\" + 0.005*\"hector\"\n",
      "2019-05-10 17:29:59,041 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:29:59,043 : INFO : topic #1 (0.155): 0.017*\"papa\" + 0.010*\"de\" + 0.010*\"la\" + 0.009*\"mozart\" + 0.008*\"monsieur\" + 0.008*\"scully\" + 0.007*\"herr\" + 0.007*\"rowan\" + 0.006*\"ripley\" + 0.005*\"der\"\n",
      "2019-05-10 17:29:59,045 : INFO : topic #0 (0.203): 0.065*\"scene\" + 0.063*\"harry\" + 0.030*\"nick\" + 0.024*\"darcy\" + 0.018*\"ms\" + 0.014*\"day\" + 0.013*\"dan\" + 0.012*\"mcu\" + 0.012*\"frodo\" + 0.012*\"ext\"\n",
      "2019-05-10 17:29:59,048 : INFO : topic diff=0.038950, rho=0.094033\n",
      "2019-05-10 17:29:59,051 : INFO : PROGRESS: pass 7, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:59,431 : INFO : PROGRESS: pass 7, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:29:59,749 : INFO : PROGRESS: pass 7, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:00,007 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:00,100 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.023*\"phillip\" + 0.022*\"g\" + 0.017*\"harvey\" + 0.013*\"kelly\" + 0.013*\"butch\" + 0.013*\"camera\" + 0.013*\"wendy\" + 0.010*\"dickson\" + 0.009*\"lowrey\"\n",
      "2019-05-10 17:30:00,103 : INFO : topic #8 (0.058): 0.049*\"alex\" + 0.043*\"ace\" + 0.027*\"nantz\" + 0.017*\"hector\" + 0.012*\"sergeant\" + 0.011*\"finkle\" + 0.009*\"melissa\" + 0.008*\"marines\" + 0.008*\"einhorn\" + 0.008*\"imlay\"\n",
      "2019-05-10 17:30:00,106 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.015*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:30:00,108 : INFO : topic #1 (0.155): 0.017*\"papa\" + 0.010*\"de\" + 0.009*\"la\" + 0.009*\"monsieur\" + 0.008*\"mozart\" + 0.008*\"scully\" + 0.007*\"herr\" + 0.006*\"rowan\" + 0.006*\"ripley\" + 0.005*\"sie\"\n",
      "2019-05-10 17:30:00,111 : INFO : topic #0 (0.203): 0.065*\"harry\" + 0.063*\"scene\" + 0.031*\"nick\" + 0.022*\"darcy\" + 0.018*\"ms\" + 0.013*\"day\" + 0.013*\"dan\" + 0.012*\"mcu\" + 0.012*\"frodo\" + 0.011*\"ext\"\n",
      "2019-05-10 17:30:00,113 : INFO : topic diff=0.052961, rho=0.094033\n",
      "2019-05-10 17:30:00,116 : INFO : PROGRESS: pass 7, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:00,546 : INFO : PROGRESS: pass 7, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:00,918 : INFO : PROGRESS: pass 7, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:01,078 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:01,186 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.022*\"phillip\" + 0.022*\"g\" + 0.017*\"harvey\" + 0.014*\"kelly\" + 0.013*\"butch\" + 0.012*\"camera\" + 0.012*\"wendy\" + 0.009*\"dickson\" + 0.009*\"lowrey\"\n",
      "2019-05-10 17:30:01,188 : INFO : topic #8 (0.058): 0.047*\"alex\" + 0.044*\"ace\" + 0.025*\"nantz\" + 0.016*\"hector\" + 0.012*\"sergeant\" + 0.010*\"finkle\" + 0.008*\"melissa\" + 0.008*\"marines\" + 0.007*\"einhorn\" + 0.007*\"imlay\"\n",
      "2019-05-10 17:30:01,191 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:01,192 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.012*\"monsieur\" + 0.010*\"de\" + 0.010*\"la\" + 0.008*\"mozart\" + 0.007*\"herr\" + 0.007*\"scully\" + 0.006*\"rowan\" + 0.005*\"ripley\" + 0.005*\"sie\"\n",
      "2019-05-10 17:30:01,194 : INFO : topic #0 (0.203): 0.061*\"harry\" + 0.058*\"scene\" + 0.028*\"nick\" + 0.021*\"darcy\" + 0.019*\"day\" + 0.017*\"ms\" + 0.016*\"int\" + 0.013*\"ext\" + 0.011*\"dan\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:01,197 : INFO : topic diff=0.029668, rho=0.094033\n",
      "2019-05-10 17:30:01,199 : INFO : PROGRESS: pass 7, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:01,476 : INFO : PROGRESS: pass 7, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:01,818 : INFO : PROGRESS: pass 7, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:02,233 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:02,297 : INFO : topic #9 (0.053): 0.041*\"b\" + 0.021*\"g\" + 0.020*\"phillip\" + 0.016*\"harvey\" + 0.013*\"donnie\" + 0.013*\"kelly\" + 0.012*\"butch\" + 0.012*\"camera\" + 0.011*\"wendy\" + 0.008*\"dickson\"\n",
      "2019-05-10 17:30:02,303 : INFO : topic #8 (0.058): 0.045*\"alex\" + 0.041*\"ace\" + 0.023*\"nantz\" + 0.015*\"hector\" + 0.011*\"sergeant\" + 0.009*\"finkle\" + 0.009*\"melissa\" + 0.007*\"marines\" + 0.007*\"einhorn\" + 0.007*\"imlay\"\n",
      "2019-05-10 17:30:02,306 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:30:02,310 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.012*\"monsieur\" + 0.010*\"de\" + 0.010*\"la\" + 0.009*\"revision\" + 0.007*\"mozart\" + 0.007*\"herr\" + 0.006*\"scully\" + 0.006*\"pink\" + 0.005*\"rowan\"\n",
      "2019-05-10 17:30:02,318 : INFO : topic #0 (0.203): 0.064*\"harry\" + 0.055*\"scene\" + 0.026*\"nick\" + 0.019*\"darcy\" + 0.018*\"day\" + 0.018*\"int\" + 0.017*\"ms\" + 0.014*\"ext\" + 0.011*\"dan\" + 0.010*\"s\"\n",
      "2019-05-10 17:30:02,324 : INFO : topic diff=0.030653, rho=0.094033\n",
      "2019-05-10 17:30:02,328 : INFO : PROGRESS: pass 7, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:02,523 : INFO : PROGRESS: pass 7, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:03,055 : INFO : PROGRESS: pass 7, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:03,235 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:03,329 : INFO : topic #9 (0.053): 0.038*\"b\" + 0.020*\"g\" + 0.019*\"tao\" + 0.019*\"phillip\" + 0.015*\"harvey\" + 0.012*\"kelly\" + 0.012*\"donnie\" + 0.011*\"butch\" + 0.011*\"camera\" + 0.010*\"wendy\"\n",
      "2019-05-10 17:30:03,331 : INFO : topic #8 (0.058): 0.046*\"alex\" + 0.038*\"ace\" + 0.021*\"nantz\" + 0.014*\"hector\" + 0.010*\"sergeant\" + 0.009*\"finkle\" + 0.008*\"melissa\" + 0.006*\"marines\" + 0.006*\"einhorn\" + 0.006*\"imlay\"\n",
      "2019-05-10 17:30:03,334 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:30:03,336 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.011*\"monsieur\" + 0.010*\"de\" + 0.009*\"la\" + 0.009*\"revision\" + 0.007*\"mozart\" + 0.007*\"herr\" + 0.006*\"scully\" + 0.006*\"pink\" + 0.005*\"rowan\"\n",
      "2019-05-10 17:30:03,338 : INFO : topic #0 (0.203): 0.084*\"harry\" + 0.042*\"scene\" + 0.021*\"day\" + 0.019*\"nick\" + 0.019*\"ext\" + 0.016*\"int\" + 0.016*\"trench\" + 0.014*\"darcy\" + 0.013*\"ms\" + 0.012*\"blaze\"\n",
      "2019-05-10 17:30:03,341 : INFO : topic diff=0.047110, rho=0.094033\n",
      "2019-05-10 17:30:03,343 : INFO : PROGRESS: pass 7, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:03,950 : INFO : PROGRESS: pass 7, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:04,058 : INFO : PROGRESS: pass 7, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:04,381 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:04,447 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.028*\"g\" + 0.021*\"camera\" + 0.018*\"wendy\" + 0.016*\"phillip\" + 0.015*\"wichita\" + 0.014*\"tao\" + 0.014*\"kelly\" + 0.014*\"harvey\" + 0.014*\"elwood\"\n",
      "2019-05-10 17:30:04,456 : INFO : topic #8 (0.058): 0.048*\"alex\" + 0.035*\"ace\" + 0.019*\"nantz\" + 0.013*\"hector\" + 0.012*\"kase\" + 0.010*\"sergeant\" + 0.008*\"melissa\" + 0.008*\"finkle\" + 0.006*\"marines\" + 0.006*\"einhorn\"\n",
      "2019-05-10 17:30:04,460 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.013*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:04,463 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.012*\"monsieur\" + 0.009*\"de\" + 0.009*\"la\" + 0.008*\"revision\" + 0.006*\"herr\" + 0.006*\"marylin\" + 0.006*\"mozart\" + 0.006*\"pink\" + 0.006*\"scully\"\n",
      "2019-05-10 17:30:04,465 : INFO : topic #0 (0.203): 0.083*\"harry\" + 0.041*\"scene\" + 0.020*\"day\" + 0.019*\"nick\" + 0.018*\"ext\" + 0.015*\"int\" + 0.015*\"trench\" + 0.014*\"darcy\" + 0.013*\"ms\" + 0.011*\"blaze\"\n",
      "2019-05-10 17:30:04,468 : INFO : topic diff=0.050841, rho=0.094033\n",
      "2019-05-10 17:30:04,471 : INFO : PROGRESS: pass 7, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:04,975 : INFO : PROGRESS: pass 7, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:05,280 : INFO : PROGRESS: pass 7, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:05,466 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:05,529 : INFO : topic #9 (0.053): 0.038*\"b\" + 0.028*\"g\" + 0.020*\"camera\" + 0.017*\"wendy\" + 0.017*\"harvey\" + 0.015*\"phillip\" + 0.014*\"kelly\" + 0.014*\"wichita\" + 0.013*\"tao\" + 0.013*\"elwood\"\n",
      "2019-05-10 17:30:05,532 : INFO : topic #8 (0.058): 0.046*\"alex\" + 0.033*\"ace\" + 0.018*\"nantz\" + 0.015*\"hector\" + 0.011*\"kase\" + 0.011*\"sergeant\" + 0.008*\"melissa\" + 0.007*\"finkle\" + 0.005*\"marines\" + 0.005*\"einhorn\"\n",
      "2019-05-10 17:30:05,534 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.013*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:05,536 : INFO : topic #1 (0.155): 0.014*\"papa\" + 0.011*\"monsieur\" + 0.010*\"la\" + 0.010*\"de\" + 0.007*\"revision\" + 0.007*\"pink\" + 0.006*\"herr\" + 0.006*\"marylin\" + 0.006*\"mozart\" + 0.005*\"scully\"\n",
      "2019-05-10 17:30:05,538 : INFO : topic #0 (0.203): 0.079*\"harry\" + 0.040*\"scene\" + 0.019*\"nick\" + 0.019*\"day\" + 0.018*\"ext\" + 0.015*\"int\" + 0.014*\"frodo\" + 0.013*\"trench\" + 0.013*\"darcy\" + 0.012*\"ms\"\n",
      "2019-05-10 17:30:05,540 : INFO : topic diff=0.028615, rho=0.094033\n",
      "2019-05-10 17:30:05,543 : INFO : PROGRESS: pass 7, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:06,051 : INFO : PROGRESS: pass 7, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:06,212 : INFO : PROGRESS: pass 7, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:06,532 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:06,594 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.028*\"harvey\" + 0.027*\"g\" + 0.020*\"camera\" + 0.016*\"wendy\" + 0.014*\"phillip\" + 0.014*\"kelly\" + 0.013*\"wichita\" + 0.012*\"tao\" + 0.012*\"elwood\"\n",
      "2019-05-10 17:30:06,597 : INFO : topic #8 (0.058): 0.043*\"alex\" + 0.031*\"ace\" + 0.016*\"nantz\" + 0.014*\"hector\" + 0.010*\"sergeant\" + 0.010*\"kase\" + 0.007*\"melissa\" + 0.007*\"finkle\" + 0.005*\"marines\" + 0.005*\"einhorn\"\n",
      "2019-05-10 17:30:06,599 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:30:06,601 : INFO : topic #1 (0.155): 0.013*\"monsieur\" + 0.013*\"papa\" + 0.010*\"de\" + 0.010*\"la\" + 0.007*\"revision\" + 0.007*\"pink\" + 0.005*\"herr\" + 0.005*\"marylin\" + 0.005*\"mozart\" + 0.005*\"scully\"\n",
      "2019-05-10 17:30:06,603 : INFO : topic #0 (0.203): 0.075*\"harry\" + 0.038*\"scene\" + 0.018*\"day\" + 0.018*\"nick\" + 0.017*\"ext\" + 0.015*\"int\" + 0.013*\"frodo\" + 0.012*\"ms\" + 0.012*\"trench\" + 0.012*\"darcy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:06,606 : INFO : topic diff=0.027931, rho=0.094033\n",
      "2019-05-10 17:30:06,608 : INFO : PROGRESS: pass 7, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:07,089 : INFO : PROGRESS: pass 7, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:07,261 : INFO : PROGRESS: pass 7, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:07,620 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:07,721 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.027*\"harvey\" + 0.026*\"g\" + 0.019*\"camera\" + 0.016*\"wendy\" + 0.014*\"phillip\" + 0.014*\"kelly\" + 0.012*\"wichita\" + 0.011*\"tao\" + 0.011*\"elwood\"\n",
      "2019-05-10 17:30:07,724 : INFO : topic #8 (0.058): 0.043*\"alex\" + 0.030*\"ace\" + 0.015*\"nantz\" + 0.013*\"hector\" + 0.010*\"sergeant\" + 0.009*\"kase\" + 0.007*\"melissa\" + 0.006*\"harris\" + 0.006*\"finkle\" + 0.005*\"marines\"\n",
      "2019-05-10 17:30:07,726 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:07,728 : INFO : topic #1 (0.155): 0.013*\"monsieur\" + 0.013*\"papa\" + 0.012*\"de\" + 0.009*\"la\" + 0.006*\"revision\" + 0.006*\"pink\" + 0.005*\"herr\" + 0.005*\"marylin\" + 0.005*\"mozart\" + 0.005*\"comrade\"\n",
      "2019-05-10 17:30:07,730 : INFO : topic #0 (0.203): 0.073*\"harry\" + 0.038*\"scene\" + 0.020*\"nick\" + 0.018*\"day\" + 0.016*\"ext\" + 0.014*\"int\" + 0.012*\"ms\" + 0.012*\"frodo\" + 0.012*\"trench\" + 0.011*\"darcy\"\n",
      "2019-05-10 17:30:07,733 : INFO : topic diff=0.030654, rho=0.094033\n",
      "2019-05-10 17:30:07,735 : INFO : PROGRESS: pass 7, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:08,227 : INFO : PROGRESS: pass 7, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:08,460 : INFO : PROGRESS: pass 7, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:08,758 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:08,826 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.024*\"g\" + 0.024*\"harvey\" + 0.021*\"phillip\" + 0.017*\"camera\" + 0.016*\"wendy\" + 0.013*\"terrance\" + 0.012*\"kelly\" + 0.011*\"wichita\" + 0.010*\"tao\"\n",
      "2019-05-10 17:30:08,828 : INFO : topic #8 (0.058): 0.040*\"alex\" + 0.029*\"ace\" + 0.014*\"nantz\" + 0.012*\"hector\" + 0.010*\"sergeant\" + 0.009*\"kase\" + 0.006*\"harris\" + 0.006*\"melissa\" + 0.006*\"finkle\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:30:08,831 : INFO : topic #2 (0.125): 0.077*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:30:08,833 : INFO : topic #1 (0.155): 0.012*\"monsieur\" + 0.012*\"papa\" + 0.010*\"de\" + 0.009*\"la\" + 0.009*\"jamal\" + 0.006*\"pink\" + 0.006*\"revision\" + 0.005*\"rupees\" + 0.005*\"herr\" + 0.004*\"ze\"\n",
      "2019-05-10 17:30:08,834 : INFO : topic #0 (0.203): 0.081*\"harry\" + 0.037*\"scene\" + 0.020*\"nick\" + 0.017*\"day\" + 0.016*\"ext\" + 0.013*\"int\" + 0.012*\"ms\" + 0.011*\"frodo\" + 0.011*\"trench\" + 0.010*\"darcy\"\n",
      "2019-05-10 17:30:08,838 : INFO : topic diff=0.040423, rho=0.094033\n",
      "2019-05-10 17:30:08,840 : INFO : PROGRESS: pass 7, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:09,250 : INFO : PROGRESS: pass 7, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:09,494 : INFO : PROGRESS: pass 7, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:09,819 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:09,884 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.024*\"g\" + 0.023*\"harvey\" + 0.020*\"phillip\" + 0.017*\"camera\" + 0.015*\"wendy\" + 0.013*\"kelly\" + 0.012*\"terrance\" + 0.010*\"wichita\" + 0.010*\"tao\"\n",
      "2019-05-10 17:30:09,886 : INFO : topic #8 (0.058): 0.039*\"alex\" + 0.029*\"ace\" + 0.013*\"nantz\" + 0.011*\"hector\" + 0.009*\"sergeant\" + 0.008*\"kase\" + 0.006*\"harris\" + 0.006*\"melissa\" + 0.005*\"finkle\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:30:09,889 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:09,892 : INFO : topic #1 (0.155): 0.011*\"monsieur\" + 0.011*\"de\" + 0.011*\"papa\" + 0.010*\"la\" + 0.008*\"dickie\" + 0.008*\"jamal\" + 0.005*\"ripley\" + 0.005*\"der\" + 0.005*\"pink\" + 0.005*\"herr\"\n",
      "2019-05-10 17:30:09,895 : INFO : topic #0 (0.203): 0.077*\"harry\" + 0.035*\"scene\" + 0.021*\"nick\" + 0.016*\"day\" + 0.014*\"ext\" + 0.012*\"int\" + 0.011*\"ms\" + 0.010*\"trench\" + 0.010*\"frodo\" + 0.010*\"darcy\"\n",
      "2019-05-10 17:30:09,897 : INFO : topic diff=0.034784, rho=0.094033\n",
      "2019-05-10 17:30:09,900 : INFO : PROGRESS: pass 7, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:10,342 : INFO : PROGRESS: pass 7, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:10,869 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:10,960 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.024*\"g\" + 0.021*\"harvey\" + 0.019*\"phillip\" + 0.016*\"camera\" + 0.015*\"wendy\" + 0.014*\"kelly\" + 0.011*\"terrance\" + 0.010*\"wichita\" + 0.009*\"tao\"\n",
      "2019-05-10 17:30:10,963 : INFO : topic #8 (0.058): 0.037*\"alex\" + 0.027*\"ace\" + 0.012*\"nantz\" + 0.010*\"hector\" + 0.009*\"sergeant\" + 0.007*\"kase\" + 0.007*\"melissa\" + 0.006*\"wigand\" + 0.006*\"harris\" + 0.005*\"finkle\"\n",
      "2019-05-10 17:30:10,965 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:10,967 : INFO : topic #1 (0.155): 0.013*\"de\" + 0.012*\"papa\" + 0.012*\"monsieur\" + 0.011*\"la\" + 0.008*\"dickie\" + 0.007*\"jamal\" + 0.005*\"ze\" + 0.005*\"pink\" + 0.005*\"ja\" + 0.005*\"ripley\"\n",
      "2019-05-10 17:30:10,970 : INFO : topic #0 (0.203): 0.076*\"harry\" + 0.034*\"scene\" + 0.020*\"nick\" + 0.017*\"frodo\" + 0.015*\"day\" + 0.013*\"ext\" + 0.012*\"int\" + 0.011*\"ms\" + 0.010*\"trench\" + 0.009*\"darcy\"\n",
      "2019-05-10 17:30:10,972 : INFO : topic diff=0.027867, rho=0.094033\n",
      "2019-05-10 17:30:11,866 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:11,927 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.023*\"g\" + 0.020*\"harvey\" + 0.018*\"phillip\" + 0.016*\"wendy\" + 0.015*\"camera\" + 0.013*\"kelly\" + 0.011*\"terrance\" + 0.009*\"wichita\" + 0.008*\"tao\"\n",
      "2019-05-10 17:30:11,930 : INFO : topic #8 (0.058): 0.038*\"alex\" + 0.025*\"ace\" + 0.011*\"nantz\" + 0.009*\"hector\" + 0.009*\"sergeant\" + 0.007*\"melissa\" + 0.007*\"kase\" + 0.006*\"wigand\" + 0.005*\"harris\" + 0.004*\"finkle\"\n",
      "2019-05-10 17:30:11,932 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.033*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.017*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:30:11,934 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.011*\"de\" + 0.011*\"la\" + 0.010*\"monsieur\" + 0.009*\"scully\" + 0.008*\"rowan\" + 0.006*\"dickie\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.006*\"ja\"\n",
      "2019-05-10 17:30:11,936 : INFO : topic #0 (0.203): 0.078*\"harry\" + 0.033*\"scene\" + 0.020*\"nick\" + 0.016*\"frodo\" + 0.015*\"day\" + 0.013*\"ext\" + 0.011*\"int\" + 0.010*\"ms\" + 0.009*\"trench\" + 0.008*\"dan\"\n",
      "2019-05-10 17:30:11,939 : INFO : topic diff=0.036197, rho=0.094033\n",
      "2019-05-10 17:30:12,758 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:12,802 : INFO : topic #9 (0.053): 0.032*\"b\" + 0.022*\"g\" + 0.019*\"harvey\" + 0.017*\"phillip\" + 0.016*\"kelly\" + 0.014*\"camera\" + 0.014*\"wendy\" + 0.011*\"madame\" + 0.010*\"terrance\" + 0.009*\"wichita\"\n",
      "2019-05-10 17:30:12,804 : INFO : topic #8 (0.058): 0.054*\"alex\" + 0.023*\"ace\" + 0.012*\"hector\" + 0.010*\"nantz\" + 0.009*\"sergeant\" + 0.006*\"melissa\" + 0.006*\"kase\" + 0.005*\"harris\" + 0.005*\"wigand\" + 0.004*\"finkle\"\n",
      "2019-05-10 17:30:12,806 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"her\"\n",
      "2019-05-10 17:30:12,808 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.011*\"de\" + 0.011*\"la\" + 0.010*\"monsieur\" + 0.009*\"scully\" + 0.007*\"rowan\" + 0.006*\"dickie\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.005*\"ja\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:12,809 : INFO : topic #0 (0.203): 0.069*\"scene\" + 0.067*\"nick\" + 0.062*\"harry\" + 0.024*\"darcy\" + 0.020*\"ms\" + 0.015*\"dan\" + 0.014*\"day\" + 0.012*\"mcu\" + 0.012*\"frodo\" + 0.011*\"ext\"\n",
      "2019-05-10 17:30:12,811 : INFO : topic diff=0.039225, rho=0.094033\n",
      "2019-05-10 17:30:12,814 : INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:30:12,894 : INFO : PROGRESS: pass 8, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:30:12,895 : INFO : PROGRESS: pass 8, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:30:12,896 : INFO : PROGRESS: pass 8, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:30:12,898 : INFO : PROGRESS: pass 8, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:30:12,900 : INFO : PROGRESS: pass 8, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:30:13,000 : INFO : PROGRESS: pass 8, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:30:13,115 : INFO : PROGRESS: pass 8, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:30:13,261 : INFO : PROGRESS: pass 8, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:13,951 : INFO : PROGRESS: pass 8, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 8\n",
      "2019-05-10 17:30:13,966 : INFO : PROGRESS: pass 8, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:14,332 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:14,393 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.025*\"phillip\" + 0.022*\"harvey\" + 0.021*\"g\" + 0.016*\"kelly\" + 0.014*\"butch\" + 0.013*\"wendy\" + 0.013*\"camera\" + 0.011*\"dickson\" + 0.010*\"madame\"\n",
      "2019-05-10 17:30:14,396 : INFO : topic #8 (0.058): 0.063*\"alex\" + 0.051*\"ace\" + 0.013*\"finkle\" + 0.010*\"melissa\" + 0.009*\"einhorn\" + 0.009*\"hector\" + 0.008*\"sergeant\" + 0.008*\"snowflake\" + 0.008*\"nantz\" + 0.006*\"marino\"\n",
      "2019-05-10 17:30:14,398 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:14,400 : INFO : topic #1 (0.155): 0.017*\"papa\" + 0.011*\"de\" + 0.010*\"la\" + 0.009*\"mozart\" + 0.009*\"monsieur\" + 0.008*\"scully\" + 0.008*\"herr\" + 0.007*\"ripley\" + 0.007*\"rowan\" + 0.005*\"dickie\"\n",
      "2019-05-10 17:30:14,402 : INFO : topic #0 (0.203): 0.067*\"scene\" + 0.065*\"nick\" + 0.060*\"harry\" + 0.022*\"darcy\" + 0.020*\"ms\" + 0.017*\"dan\" + 0.013*\"day\" + 0.012*\"mcu\" + 0.011*\"frodo\" + 0.011*\"ext\"\n",
      "2019-05-10 17:30:14,405 : INFO : topic diff=0.037590, rho=0.093620\n",
      "2019-05-10 17:30:14,408 : INFO : PROGRESS: pass 8, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:14,717 : INFO : PROGRESS: pass 8, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:15,016 : INFO : PROGRESS: pass 8, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:15,323 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:15,387 : INFO : topic #9 (0.053): 0.032*\"b\" + 0.022*\"phillip\" + 0.021*\"g\" + 0.020*\"harvey\" + 0.014*\"kelly\" + 0.013*\"butch\" + 0.013*\"camera\" + 0.012*\"wendy\" + 0.010*\"dickson\" + 0.009*\"madame\"\n",
      "2019-05-10 17:30:15,389 : INFO : topic #8 (0.058): 0.052*\"alex\" + 0.043*\"ace\" + 0.027*\"hector\" + 0.026*\"nantz\" + 0.012*\"sergeant\" + 0.011*\"finkle\" + 0.009*\"melissa\" + 0.008*\"marines\" + 0.008*\"einhorn\" + 0.008*\"imlay\"\n",
      "2019-05-10 17:30:15,394 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.033*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:15,396 : INFO : topic #1 (0.155): 0.017*\"papa\" + 0.011*\"de\" + 0.009*\"la\" + 0.009*\"monsieur\" + 0.008*\"mozart\" + 0.008*\"herr\" + 0.007*\"scully\" + 0.007*\"ripley\" + 0.006*\"rowan\" + 0.005*\"dickie\"\n",
      "2019-05-10 17:30:15,398 : INFO : topic #0 (0.203): 0.065*\"nick\" + 0.065*\"scene\" + 0.062*\"harry\" + 0.021*\"darcy\" + 0.019*\"ms\" + 0.016*\"dan\" + 0.013*\"day\" + 0.011*\"mcu\" + 0.011*\"frodo\" + 0.011*\"ext\"\n",
      "2019-05-10 17:30:15,401 : INFO : topic diff=0.052093, rho=0.093620\n",
      "2019-05-10 17:30:15,403 : INFO : PROGRESS: pass 8, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:15,806 : INFO : PROGRESS: pass 8, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:16,057 : INFO : PROGRESS: pass 8, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:16,367 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:16,434 : INFO : topic #9 (0.053): 0.032*\"b\" + 0.022*\"phillip\" + 0.022*\"g\" + 0.019*\"harvey\" + 0.015*\"kelly\" + 0.013*\"butch\" + 0.013*\"camera\" + 0.012*\"wendy\" + 0.010*\"madame\" + 0.009*\"dickson\"\n",
      "2019-05-10 17:30:16,437 : INFO : topic #8 (0.058): 0.051*\"alex\" + 0.044*\"ace\" + 0.025*\"hector\" + 0.024*\"nantz\" + 0.012*\"sergeant\" + 0.010*\"finkle\" + 0.008*\"melissa\" + 0.008*\"marines\" + 0.007*\"einhorn\" + 0.007*\"imlay\"\n",
      "2019-05-10 17:30:16,439 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:16,441 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.013*\"monsieur\" + 0.011*\"de\" + 0.010*\"la\" + 0.008*\"mozart\" + 0.007*\"herr\" + 0.007*\"scully\" + 0.006*\"ripley\" + 0.006*\"rowan\" + 0.005*\"dickie\"\n",
      "2019-05-10 17:30:16,442 : INFO : topic #0 (0.203): 0.060*\"scene\" + 0.060*\"nick\" + 0.060*\"harry\" + 0.019*\"darcy\" + 0.018*\"ms\" + 0.018*\"day\" + 0.015*\"int\" + 0.015*\"dan\" + 0.013*\"ext\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:16,445 : INFO : topic diff=0.029162, rho=0.093620\n",
      "2019-05-10 17:30:16,447 : INFO : PROGRESS: pass 8, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:16,708 : INFO : PROGRESS: pass 8, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:17,166 : INFO : PROGRESS: pass 8, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:17,452 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:17,516 : INFO : topic #9 (0.053): 0.039*\"b\" + 0.021*\"g\" + 0.020*\"phillip\" + 0.018*\"harvey\" + 0.015*\"donnie\" + 0.014*\"kelly\" + 0.012*\"camera\" + 0.012*\"butch\" + 0.011*\"wendy\" + 0.009*\"madame\"\n",
      "2019-05-10 17:30:17,518 : INFO : topic #8 (0.058): 0.048*\"alex\" + 0.041*\"ace\" + 0.024*\"hector\" + 0.022*\"nantz\" + 0.011*\"sergeant\" + 0.009*\"finkle\" + 0.009*\"melissa\" + 0.007*\"marines\" + 0.007*\"einhorn\" + 0.007*\"imlay\"\n",
      "2019-05-10 17:30:17,521 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:17,522 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.012*\"monsieur\" + 0.010*\"de\" + 0.010*\"la\" + 0.009*\"revision\" + 0.007*\"mozart\" + 0.007*\"herr\" + 0.006*\"scully\" + 0.006*\"pink\" + 0.006*\"ripley\"\n",
      "2019-05-10 17:30:17,526 : INFO : topic #0 (0.203): 0.062*\"harry\" + 0.057*\"scene\" + 0.055*\"nick\" + 0.018*\"ms\" + 0.018*\"darcy\" + 0.017*\"day\" + 0.017*\"int\" + 0.014*\"dan\" + 0.014*\"ext\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:17,529 : INFO : topic diff=0.029481, rho=0.093620\n",
      "2019-05-10 17:30:17,531 : INFO : PROGRESS: pass 8, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:17,712 : INFO : PROGRESS: pass 8, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:18,168 : INFO : PROGRESS: pass 8, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:18,453 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:18,546 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.020*\"g\" + 0.019*\"tao\" + 0.019*\"phillip\" + 0.017*\"harvey\" + 0.014*\"donnie\" + 0.013*\"kelly\" + 0.011*\"butch\" + 0.011*\"camera\" + 0.010*\"wendy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:18,549 : INFO : topic #8 (0.058): 0.049*\"alex\" + 0.038*\"ace\" + 0.023*\"hector\" + 0.021*\"nantz\" + 0.010*\"sergeant\" + 0.009*\"finkle\" + 0.008*\"melissa\" + 0.007*\"marines\" + 0.006*\"einhorn\" + 0.006*\"imlay\"\n",
      "2019-05-10 17:30:18,551 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:18,556 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.011*\"monsieur\" + 0.010*\"de\" + 0.009*\"la\" + 0.009*\"revision\" + 0.007*\"mozart\" + 0.007*\"herr\" + 0.006*\"scully\" + 0.006*\"pink\" + 0.006*\"ripley\"\n",
      "2019-05-10 17:30:18,557 : INFO : topic #0 (0.203): 0.082*\"harry\" + 0.044*\"scene\" + 0.042*\"nick\" + 0.020*\"day\" + 0.018*\"ext\" + 0.015*\"int\" + 0.015*\"trench\" + 0.014*\"ms\" + 0.014*\"darcy\" + 0.013*\"dan\"\n",
      "2019-05-10 17:30:18,562 : INFO : topic diff=0.045125, rho=0.093620\n",
      "2019-05-10 17:30:18,566 : INFO : PROGRESS: pass 8, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:19,095 : INFO : PROGRESS: pass 8, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:19,573 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:19,665 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.027*\"g\" + 0.021*\"camera\" + 0.018*\"wendy\" + 0.016*\"phillip\" + 0.015*\"harvey\" + 0.014*\"wichita\" + 0.014*\"kelly\" + 0.014*\"tao\" + 0.013*\"elwood\"\n",
      "2019-05-10 17:30:19,668 : INFO : topic #8 (0.058): 0.051*\"alex\" + 0.035*\"ace\" + 0.021*\"hector\" + 0.019*\"nantz\" + 0.013*\"kase\" + 0.010*\"sergeant\" + 0.008*\"melissa\" + 0.008*\"finkle\" + 0.006*\"marines\" + 0.006*\"einhorn\"\n",
      "2019-05-10 17:30:19,670 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:19,672 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.012*\"monsieur\" + 0.009*\"de\" + 0.009*\"la\" + 0.008*\"revision\" + 0.007*\"herr\" + 0.006*\"marylin\" + 0.006*\"mozart\" + 0.006*\"pink\" + 0.005*\"scully\"\n",
      "2019-05-10 17:30:19,673 : INFO : topic #0 (0.203): 0.080*\"harry\" + 0.043*\"scene\" + 0.042*\"nick\" + 0.020*\"day\" + 0.018*\"ext\" + 0.015*\"ms\" + 0.015*\"int\" + 0.014*\"trench\" + 0.013*\"darcy\" + 0.012*\"dan\"\n",
      "2019-05-10 17:30:19,676 : INFO : topic diff=0.048962, rho=0.093620\n",
      "2019-05-10 17:30:19,678 : INFO : PROGRESS: pass 8, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 8\n",
      "2019-05-10 17:30:19,679 : INFO : PROGRESS: pass 8, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:20,148 : INFO : PROGRESS: pass 8, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:20,309 : INFO : PROGRESS: pass 8, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:20,620 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:20,683 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.028*\"g\" + 0.020*\"camera\" + 0.019*\"harvey\" + 0.017*\"wendy\" + 0.015*\"phillip\" + 0.014*\"kelly\" + 0.014*\"wichita\" + 0.013*\"tao\" + 0.013*\"elwood\"\n",
      "2019-05-10 17:30:20,686 : INFO : topic #8 (0.058): 0.049*\"alex\" + 0.033*\"ace\" + 0.022*\"hector\" + 0.017*\"nantz\" + 0.012*\"kase\" + 0.011*\"sergeant\" + 0.008*\"melissa\" + 0.007*\"finkle\" + 0.005*\"marines\" + 0.005*\"einhorn\"\n",
      "2019-05-10 17:30:20,688 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:20,690 : INFO : topic #1 (0.155): 0.014*\"papa\" + 0.011*\"monsieur\" + 0.010*\"la\" + 0.010*\"de\" + 0.007*\"revision\" + 0.007*\"pink\" + 0.006*\"herr\" + 0.006*\"marylin\" + 0.006*\"mozart\" + 0.005*\"scully\"\n",
      "2019-05-10 17:30:20,692 : INFO : topic #0 (0.203): 0.076*\"harry\" + 0.042*\"nick\" + 0.042*\"scene\" + 0.019*\"day\" + 0.017*\"ext\" + 0.015*\"int\" + 0.014*\"ms\" + 0.013*\"frodo\" + 0.013*\"trench\" + 0.012*\"darcy\"\n",
      "2019-05-10 17:30:20,695 : INFO : topic diff=0.027475, rho=0.093620\n",
      "2019-05-10 17:30:20,697 : INFO : PROGRESS: pass 8, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:21,301 : INFO : PROGRESS: pass 8, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:21,334 : INFO : PROGRESS: pass 8, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:21,643 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:21,735 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.030*\"harvey\" + 0.027*\"g\" + 0.019*\"camera\" + 0.016*\"wendy\" + 0.014*\"phillip\" + 0.014*\"kelly\" + 0.013*\"wichita\" + 0.012*\"tao\" + 0.012*\"elwood\"\n",
      "2019-05-10 17:30:21,737 : INFO : topic #8 (0.058): 0.046*\"alex\" + 0.031*\"ace\" + 0.021*\"hector\" + 0.016*\"nantz\" + 0.011*\"kase\" + 0.010*\"sergeant\" + 0.007*\"melissa\" + 0.007*\"finkle\" + 0.005*\"marines\" + 0.005*\"harris\"\n",
      "2019-05-10 17:30:21,739 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:21,741 : INFO : topic #1 (0.155): 0.014*\"monsieur\" + 0.013*\"papa\" + 0.011*\"de\" + 0.010*\"la\" + 0.007*\"revision\" + 0.006*\"pink\" + 0.006*\"herr\" + 0.005*\"mozart\" + 0.005*\"marylin\" + 0.005*\"comrade\"\n",
      "2019-05-10 17:30:21,744 : INFO : topic #0 (0.203): 0.073*\"harry\" + 0.040*\"scene\" + 0.040*\"nick\" + 0.018*\"day\" + 0.016*\"ext\" + 0.014*\"int\" + 0.014*\"ms\" + 0.013*\"dan\" + 0.012*\"frodo\" + 0.012*\"trench\"\n",
      "2019-05-10 17:30:21,746 : INFO : topic diff=0.026955, rho=0.093620\n",
      "2019-05-10 17:30:21,749 : INFO : PROGRESS: pass 8, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:22,352 : INFO : PROGRESS: pass 8, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:22,385 : INFO : PROGRESS: pass 8, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:22,770 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:22,835 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.029*\"harvey\" + 0.026*\"g\" + 0.019*\"camera\" + 0.016*\"wendy\" + 0.014*\"kelly\" + 0.013*\"phillip\" + 0.012*\"wichita\" + 0.011*\"tao\" + 0.011*\"elwood\"\n",
      "2019-05-10 17:30:22,837 : INFO : topic #8 (0.058): 0.046*\"alex\" + 0.030*\"ace\" + 0.020*\"hector\" + 0.015*\"nantz\" + 0.010*\"kase\" + 0.010*\"sergeant\" + 0.007*\"harris\" + 0.007*\"melissa\" + 0.006*\"finkle\" + 0.005*\"marines\"\n",
      "2019-05-10 17:30:22,840 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:22,843 : INFO : topic #1 (0.155): 0.014*\"monsieur\" + 0.013*\"papa\" + 0.012*\"de\" + 0.010*\"la\" + 0.006*\"revision\" + 0.006*\"pink\" + 0.005*\"herr\" + 0.005*\"mozart\" + 0.005*\"comrade\" + 0.005*\"marylin\"\n",
      "2019-05-10 17:30:22,845 : INFO : topic #0 (0.203): 0.071*\"harry\" + 0.042*\"nick\" + 0.040*\"scene\" + 0.018*\"day\" + 0.016*\"ext\" + 0.013*\"int\" + 0.013*\"ms\" + 0.012*\"dan\" + 0.012*\"frodo\" + 0.011*\"trench\"\n",
      "2019-05-10 17:30:22,847 : INFO : topic diff=0.030060, rho=0.093620\n",
      "2019-05-10 17:30:22,850 : INFO : PROGRESS: pass 8, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:23,412 : INFO : PROGRESS: pass 8, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:23,447 : INFO : PROGRESS: pass 8, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:23,818 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:23,883 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.025*\"harvey\" + 0.024*\"g\" + 0.021*\"phillip\" + 0.017*\"camera\" + 0.016*\"wendy\" + 0.013*\"kelly\" + 0.013*\"terrance\" + 0.011*\"wichita\" + 0.010*\"tao\"\n",
      "2019-05-10 17:30:23,886 : INFO : topic #8 (0.058): 0.043*\"alex\" + 0.029*\"ace\" + 0.018*\"hector\" + 0.014*\"nantz\" + 0.010*\"sergeant\" + 0.010*\"kase\" + 0.007*\"harris\" + 0.006*\"melissa\" + 0.006*\"finkle\" + 0.004*\"marines\"\n",
      "2019-05-10 17:30:23,888 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.012*\"s\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:23,890 : INFO : topic #1 (0.155): 0.012*\"monsieur\" + 0.012*\"papa\" + 0.011*\"de\" + 0.009*\"la\" + 0.009*\"jamal\" + 0.006*\"pink\" + 0.005*\"revision\" + 0.005*\"rupees\" + 0.005*\"herr\" + 0.005*\"comrade\"\n",
      "2019-05-10 17:30:23,892 : INFO : topic #0 (0.203): 0.079*\"harry\" + 0.041*\"nick\" + 0.039*\"scene\" + 0.017*\"day\" + 0.015*\"ext\" + 0.013*\"ms\" + 0.013*\"int\" + 0.012*\"dan\" + 0.011*\"frodo\" + 0.011*\"trench\"\n",
      "2019-05-10 17:30:23,894 : INFO : topic diff=0.038649, rho=0.093620\n",
      "2019-05-10 17:30:23,897 : INFO : PROGRESS: pass 8, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:24,350 : INFO : PROGRESS: pass 8, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:24,670 : INFO : PROGRESS: pass 8, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:24,884 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:24,948 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.024*\"harvey\" + 0.023*\"g\" + 0.020*\"phillip\" + 0.016*\"camera\" + 0.015*\"wendy\" + 0.014*\"kelly\" + 0.012*\"terrance\" + 0.010*\"wichita\" + 0.009*\"tao\"\n",
      "2019-05-10 17:30:24,951 : INFO : topic #8 (0.058): 0.042*\"alex\" + 0.029*\"ace\" + 0.017*\"hector\" + 0.013*\"nantz\" + 0.009*\"sergeant\" + 0.009*\"kase\" + 0.006*\"harris\" + 0.006*\"melissa\" + 0.005*\"finkle\" + 0.004*\"martinez\"\n",
      "2019-05-10 17:30:24,954 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:24,956 : INFO : topic #1 (0.155): 0.012*\"de\" + 0.012*\"monsieur\" + 0.011*\"papa\" + 0.010*\"la\" + 0.009*\"dickie\" + 0.007*\"jamal\" + 0.006*\"ripley\" + 0.005*\"der\" + 0.005*\"pink\" + 0.005*\"herr\"\n",
      "2019-05-10 17:30:24,958 : INFO : topic #0 (0.203): 0.075*\"harry\" + 0.041*\"nick\" + 0.037*\"scene\" + 0.016*\"day\" + 0.014*\"ext\" + 0.012*\"ms\" + 0.012*\"dan\" + 0.012*\"int\" + 0.010*\"trench\" + 0.010*\"frodo\"\n",
      "2019-05-10 17:30:24,962 : INFO : topic diff=0.033794, rho=0.093620\n",
      "2019-05-10 17:30:24,964 : INFO : PROGRESS: pass 8, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:25,430 : INFO : PROGRESS: pass 8, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:25,895 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:25,959 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.023*\"g\" + 0.023*\"harvey\" + 0.019*\"phillip\" + 0.015*\"camera\" + 0.015*\"wendy\" + 0.014*\"kelly\" + 0.011*\"terrance\" + 0.010*\"wichita\" + 0.009*\"tao\"\n",
      "2019-05-10 17:30:25,961 : INFO : topic #8 (0.058): 0.040*\"alex\" + 0.027*\"ace\" + 0.015*\"hector\" + 0.012*\"nantz\" + 0.009*\"sergeant\" + 0.008*\"kase\" + 0.007*\"melissa\" + 0.007*\"wigand\" + 0.006*\"harris\" + 0.005*\"finkle\"\n",
      "2019-05-10 17:30:25,964 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.024*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:25,966 : INFO : topic #1 (0.155): 0.013*\"de\" + 0.013*\"papa\" + 0.012*\"monsieur\" + 0.011*\"la\" + 0.008*\"dickie\" + 0.007*\"jamal\" + 0.005*\"ze\" + 0.005*\"ripley\" + 0.005*\"der\" + 0.005*\"pink\"\n",
      "2019-05-10 17:30:25,968 : INFO : topic #0 (0.203): 0.074*\"harry\" + 0.039*\"nick\" + 0.036*\"scene\" + 0.016*\"frodo\" + 0.015*\"day\" + 0.013*\"ext\" + 0.012*\"ms\" + 0.012*\"dan\" + 0.011*\"int\" + 0.009*\"trench\"\n",
      "2019-05-10 17:30:25,971 : INFO : topic diff=0.026159, rho=0.093620\n",
      "2019-05-10 17:30:26,844 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:26,904 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.023*\"g\" + 0.021*\"harvey\" + 0.017*\"phillip\" + 0.015*\"wendy\" + 0.015*\"camera\" + 0.013*\"kelly\" + 0.010*\"terrance\" + 0.009*\"wichita\" + 0.008*\"tao\"\n",
      "2019-05-10 17:30:26,906 : INFO : topic #8 (0.058): 0.040*\"alex\" + 0.026*\"ace\" + 0.014*\"hector\" + 0.011*\"nantz\" + 0.009*\"sergeant\" + 0.008*\"kase\" + 0.007*\"melissa\" + 0.006*\"wigand\" + 0.005*\"harris\" + 0.004*\"finkle\"\n",
      "2019-05-10 17:30:26,908 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.033*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:26,910 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.012*\"de\" + 0.011*\"la\" + 0.011*\"monsieur\" + 0.009*\"scully\" + 0.008*\"rowan\" + 0.007*\"dickie\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.006*\"der\"\n",
      "2019-05-10 17:30:26,912 : INFO : topic #0 (0.203): 0.077*\"harry\" + 0.038*\"nick\" + 0.035*\"scene\" + 0.015*\"frodo\" + 0.015*\"day\" + 0.012*\"ext\" + 0.012*\"ms\" + 0.011*\"dan\" + 0.011*\"int\" + 0.009*\"trench\"\n",
      "2019-05-10 17:30:26,914 : INFO : topic diff=0.035675, rho=0.093620\n",
      "2019-05-10 17:30:27,736 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:27,779 : INFO : topic #9 (0.053): 0.031*\"b\" + 0.022*\"g\" + 0.020*\"harvey\" + 0.017*\"kelly\" + 0.016*\"phillip\" + 0.014*\"camera\" + 0.014*\"wendy\" + 0.012*\"madame\" + 0.009*\"terrance\" + 0.009*\"wichita\"\n",
      "2019-05-10 17:30:27,780 : INFO : topic #8 (0.058): 0.056*\"alex\" + 0.023*\"ace\" + 0.017*\"hector\" + 0.010*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"kase\" + 0.006*\"melissa\" + 0.006*\"harris\" + 0.005*\"wigand\" + 0.004*\"finkle\"\n",
      "2019-05-10 17:30:27,782 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.033*\"and\" + 0.031*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.014*\"is\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:27,784 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.011*\"de\" + 0.011*\"la\" + 0.010*\"monsieur\" + 0.009*\"scully\" + 0.007*\"rowan\" + 0.006*\"dickie\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.005*\"ja\"\n",
      "2019-05-10 17:30:27,785 : INFO : topic #0 (0.203): 0.082*\"nick\" + 0.069*\"scene\" + 0.061*\"harry\" + 0.023*\"darcy\" + 0.021*\"ms\" + 0.019*\"dan\" + 0.013*\"day\" + 0.012*\"mcu\" + 0.012*\"frodo\" + 0.011*\"ext\"\n",
      "2019-05-10 17:30:27,787 : INFO : topic diff=0.037420, rho=0.093620\n",
      "2019-05-10 17:30:27,790 : INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #1000/42094, outstanding queue size 1\n",
      "2019-05-10 17:30:27,868 : INFO : PROGRESS: pass 9, dispatched chunk #1 = documents up to #2000/42094, outstanding queue size 2\n",
      "2019-05-10 17:30:27,869 : INFO : PROGRESS: pass 9, dispatched chunk #2 = documents up to #3000/42094, outstanding queue size 3\n",
      "2019-05-10 17:30:27,870 : INFO : PROGRESS: pass 9, dispatched chunk #3 = documents up to #4000/42094, outstanding queue size 4\n",
      "2019-05-10 17:30:27,871 : INFO : PROGRESS: pass 9, dispatched chunk #4 = documents up to #5000/42094, outstanding queue size 5\n",
      "2019-05-10 17:30:27,873 : INFO : PROGRESS: pass 9, dispatched chunk #5 = documents up to #6000/42094, outstanding queue size 6\n",
      "2019-05-10 17:30:27,973 : INFO : PROGRESS: pass 9, dispatched chunk #6 = documents up to #7000/42094, outstanding queue size 7\n",
      "2019-05-10 17:30:28,070 : INFO : PROGRESS: pass 9, dispatched chunk #7 = documents up to #8000/42094, outstanding queue size 8\n",
      "2019-05-10 17:30:28,147 : INFO : PROGRESS: pass 9, dispatched chunk #8 = documents up to #9000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:28,909 : INFO : PROGRESS: pass 9, dispatched chunk #9 = documents up to #10000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:28,950 : INFO : PROGRESS: pass 9, dispatched chunk #10 = documents up to #11000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:29,276 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:29,338 : INFO : topic #9 (0.053): 0.032*\"b\" + 0.024*\"phillip\" + 0.023*\"harvey\" + 0.021*\"g\" + 0.016*\"kelly\" + 0.014*\"butch\" + 0.013*\"wendy\" + 0.013*\"camera\" + 0.011*\"madame\" + 0.010*\"dickson\"\n",
      "2019-05-10 17:30:29,340 : INFO : topic #8 (0.058): 0.065*\"alex\" + 0.051*\"ace\" + 0.013*\"hector\" + 0.013*\"finkle\" + 0.010*\"melissa\" + 0.009*\"einhorn\" + 0.008*\"sergeant\" + 0.008*\"snowflake\" + 0.008*\"nantz\" + 0.006*\"marino\"\n",
      "2019-05-10 17:30:29,343 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.033*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.014*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:29,345 : INFO : topic #1 (0.155): 0.017*\"papa\" + 0.011*\"de\" + 0.010*\"la\" + 0.009*\"monsieur\" + 0.009*\"mozart\" + 0.008*\"ripley\" + 0.008*\"scully\" + 0.008*\"herr\" + 0.007*\"rowan\" + 0.006*\"dickie\"\n",
      "2019-05-10 17:30:29,347 : INFO : topic #0 (0.203): 0.079*\"nick\" + 0.067*\"scene\" + 0.059*\"harry\" + 0.022*\"darcy\" + 0.020*\"ms\" + 0.020*\"dan\" + 0.013*\"day\" + 0.011*\"mcu\" + 0.011*\"frodo\" + 0.011*\"ext\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:29,349 : INFO : topic diff=0.036413, rho=0.093212\n",
      "2019-05-10 17:30:29,352 : INFO : PROGRESS: pass 9, dispatched chunk #11 = documents up to #12000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:29,716 : INFO : PROGRESS: pass 9, dispatched chunk #12 = documents up to #13000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:30,118 : INFO : PROGRESS: pass 9, dispatched chunk #13 = documents up to #14000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:30,315 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:30,378 : INFO : topic #9 (0.053): 0.030*\"b\" + 0.022*\"phillip\" + 0.022*\"harvey\" + 0.021*\"g\" + 0.015*\"kelly\" + 0.013*\"camera\" + 0.013*\"butch\" + 0.013*\"u\" + 0.012*\"wendy\" + 0.010*\"madame\"\n",
      "2019-05-10 17:30:30,381 : INFO : topic #8 (0.058): 0.054*\"alex\" + 0.042*\"ace\" + 0.034*\"hector\" + 0.026*\"nantz\" + 0.012*\"sergeant\" + 0.011*\"finkle\" + 0.009*\"melissa\" + 0.008*\"marines\" + 0.008*\"einhorn\" + 0.008*\"imlay\"\n",
      "2019-05-10 17:30:30,383 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.033*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.014*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:30,385 : INFO : topic #1 (0.155): 0.017*\"papa\" + 0.011*\"de\" + 0.010*\"monsieur\" + 0.010*\"la\" + 0.008*\"mozart\" + 0.008*\"herr\" + 0.007*\"ripley\" + 0.007*\"scully\" + 0.006*\"rowan\" + 0.006*\"dickie\"\n",
      "2019-05-10 17:30:30,386 : INFO : topic #0 (0.203): 0.079*\"nick\" + 0.065*\"scene\" + 0.061*\"harry\" + 0.020*\"darcy\" + 0.020*\"ms\" + 0.019*\"dan\" + 0.013*\"day\" + 0.011*\"mcu\" + 0.010*\"frodo\" + 0.010*\"ext\"\n",
      "2019-05-10 17:30:30,390 : INFO : topic diff=0.051909, rho=0.093212\n",
      "2019-05-10 17:30:30,392 : INFO : PROGRESS: pass 9, dispatched chunk #14 = documents up to #15000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:30,696 : INFO : PROGRESS: pass 9, dispatched chunk #15 = documents up to #16000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:31,339 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:31,451 : INFO : topic #9 (0.053): 0.031*\"b\" + 0.021*\"phillip\" + 0.021*\"g\" + 0.020*\"harvey\" + 0.015*\"kelly\" + 0.013*\"butch\" + 0.013*\"camera\" + 0.012*\"u\" + 0.012*\"wendy\" + 0.010*\"madame\"\n",
      "2019-05-10 17:30:31,454 : INFO : topic #8 (0.058): 0.052*\"alex\" + 0.043*\"ace\" + 0.032*\"hector\" + 0.024*\"nantz\" + 0.012*\"sergeant\" + 0.010*\"finkle\" + 0.008*\"melissa\" + 0.008*\"marines\" + 0.007*\"einhorn\" + 0.007*\"imlay\"\n",
      "2019-05-10 17:30:31,457 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"in\" + 0.018*\"his\" + 0.017*\"he\" + 0.014*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:31,459 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.013*\"monsieur\" + 0.011*\"de\" + 0.010*\"la\" + 0.008*\"mozart\" + 0.007*\"herr\" + 0.007*\"ripley\" + 0.007*\"scully\" + 0.006*\"rowan\" + 0.005*\"dickie\"\n",
      "2019-05-10 17:30:31,460 : INFO : topic #0 (0.203): 0.072*\"nick\" + 0.060*\"scene\" + 0.059*\"harry\" + 0.019*\"darcy\" + 0.019*\"ms\" + 0.018*\"dan\" + 0.018*\"day\" + 0.015*\"int\" + 0.012*\"ext\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:31,463 : INFO : topic diff=0.028901, rho=0.093212\n",
      "2019-05-10 17:30:31,465 : INFO : PROGRESS: pass 9, dispatched chunk #16 = documents up to #17000/42094, outstanding queue size 8\n",
      "2019-05-10 17:30:31,467 : INFO : PROGRESS: pass 9, dispatched chunk #17 = documents up to #18000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:31,598 : INFO : PROGRESS: pass 9, dispatched chunk #18 = documents up to #19000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:32,122 : INFO : PROGRESS: pass 9, dispatched chunk #19 = documents up to #20000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:32,593 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:32,694 : INFO : topic #9 (0.053): 0.037*\"b\" + 0.020*\"g\" + 0.020*\"harvey\" + 0.020*\"phillip\" + 0.015*\"donnie\" + 0.014*\"kelly\" + 0.012*\"jerry\" + 0.012*\"camera\" + 0.012*\"butch\" + 0.012*\"u\"\n",
      "2019-05-10 17:30:32,696 : INFO : topic #8 (0.058): 0.050*\"alex\" + 0.041*\"ace\" + 0.031*\"hector\" + 0.022*\"nantz\" + 0.011*\"sergeant\" + 0.009*\"finkle\" + 0.009*\"melissa\" + 0.007*\"marines\" + 0.007*\"einhorn\" + 0.007*\"imlay\"\n",
      "2019-05-10 17:30:32,699 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"a\" + 0.032*\"and\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:32,701 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.012*\"monsieur\" + 0.011*\"de\" + 0.010*\"la\" + 0.009*\"revision\" + 0.007*\"mozart\" + 0.007*\"herr\" + 0.006*\"ripley\" + 0.006*\"scully\" + 0.006*\"pink\"\n",
      "2019-05-10 17:30:32,703 : INFO : topic #0 (0.203): 0.068*\"nick\" + 0.061*\"harry\" + 0.058*\"scene\" + 0.019*\"ms\" + 0.017*\"darcy\" + 0.017*\"day\" + 0.017*\"dan\" + 0.016*\"int\" + 0.013*\"ext\" + 0.011*\"s\"\n",
      "2019-05-10 17:30:32,705 : INFO : topic diff=0.028645, rho=0.093212\n",
      "2019-05-10 17:30:32,708 : INFO : PROGRESS: pass 9, dispatched chunk #20 = documents up to #21000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:32,744 : INFO : PROGRESS: pass 9, dispatched chunk #21 = documents up to #22000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:33,330 : INFO : PROGRESS: pass 9, dispatched chunk #22 = documents up to #23000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:33,643 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:33,707 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.019*\"g\" + 0.018*\"tao\" + 0.018*\"phillip\" + 0.018*\"harvey\" + 0.015*\"donnie\" + 0.013*\"kelly\" + 0.012*\"jerry\" + 0.011*\"camera\" + 0.011*\"butch\"\n",
      "2019-05-10 17:30:33,709 : INFO : topic #8 (0.058): 0.050*\"alex\" + 0.038*\"ace\" + 0.029*\"hector\" + 0.021*\"nantz\" + 0.010*\"sergeant\" + 0.008*\"finkle\" + 0.008*\"melissa\" + 0.007*\"marines\" + 0.006*\"einhorn\" + 0.006*\"imlay\"\n",
      "2019-05-10 17:30:33,712 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"a\" + 0.032*\"and\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.017*\"he\" + 0.017*\"in\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:33,714 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.012*\"monsieur\" + 0.011*\"de\" + 0.010*\"la\" + 0.009*\"revision\" + 0.007*\"mozart\" + 0.007*\"herr\" + 0.006*\"ripley\" + 0.006*\"scully\" + 0.006*\"pink\"\n",
      "2019-05-10 17:30:33,716 : INFO : topic #0 (0.203): 0.080*\"harry\" + 0.052*\"nick\" + 0.045*\"scene\" + 0.020*\"day\" + 0.018*\"ext\" + 0.016*\"dan\" + 0.015*\"ms\" + 0.015*\"int\" + 0.015*\"trench\" + 0.013*\"darcy\"\n",
      "2019-05-10 17:30:33,720 : INFO : topic diff=0.044180, rho=0.093212\n",
      "2019-05-10 17:30:33,722 : INFO : PROGRESS: pass 9, dispatched chunk #23 = documents up to #24000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:34,045 : INFO : PROGRESS: pass 9, dispatched chunk #24 = documents up to #25000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:34,295 : INFO : PROGRESS: pass 9, dispatched chunk #25 = documents up to #26000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:34,707 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:34,770 : INFO : topic #9 (0.053): 0.035*\"b\" + 0.027*\"g\" + 0.020*\"camera\" + 0.018*\"wendy\" + 0.016*\"harvey\" + 0.016*\"phillip\" + 0.015*\"kelly\" + 0.014*\"wichita\" + 0.013*\"tao\" + 0.013*\"elwood\"\n",
      "2019-05-10 17:30:34,773 : INFO : topic #8 (0.058): 0.053*\"alex\" + 0.035*\"ace\" + 0.027*\"hector\" + 0.019*\"nantz\" + 0.014*\"kase\" + 0.010*\"sergeant\" + 0.008*\"melissa\" + 0.008*\"finkle\" + 0.006*\"marines\" + 0.006*\"einhorn\"\n",
      "2019-05-10 17:30:34,775 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:34,777 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.013*\"monsieur\" + 0.010*\"de\" + 0.009*\"la\" + 0.008*\"revision\" + 0.007*\"herr\" + 0.006*\"marylin\" + 0.006*\"mozart\" + 0.006*\"pink\" + 0.005*\"ripley\"\n",
      "2019-05-10 17:30:34,779 : INFO : topic #0 (0.203): 0.079*\"harry\" + 0.052*\"nick\" + 0.044*\"scene\" + 0.019*\"day\" + 0.017*\"ext\" + 0.015*\"ms\" + 0.015*\"dan\" + 0.014*\"int\" + 0.014*\"trench\" + 0.013*\"darcy\"\n",
      "2019-05-10 17:30:34,782 : INFO : topic diff=0.047630, rho=0.093212\n",
      "2019-05-10 17:30:34,785 : INFO : PROGRESS: pass 9, dispatched chunk #26 = documents up to #27000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:35,295 : INFO : PROGRESS: pass 9, dispatched chunk #27 = documents up to #28000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:35,413 : INFO : PROGRESS: pass 9, dispatched chunk #28 = documents up to #29000/42094, outstanding queue size 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:35,729 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:35,793 : INFO : topic #9 (0.053): 0.036*\"b\" + 0.027*\"g\" + 0.020*\"camera\" + 0.019*\"harvey\" + 0.017*\"wendy\" + 0.015*\"phillip\" + 0.015*\"kelly\" + 0.013*\"wichita\" + 0.013*\"tao\" + 0.012*\"elwood\"\n",
      "2019-05-10 17:30:35,797 : INFO : topic #8 (0.058): 0.050*\"alex\" + 0.033*\"ace\" + 0.028*\"hector\" + 0.017*\"nantz\" + 0.013*\"kase\" + 0.011*\"sergeant\" + 0.008*\"melissa\" + 0.007*\"finkle\" + 0.005*\"marines\" + 0.005*\"harris\"\n",
      "2019-05-10 17:30:35,799 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:35,801 : INFO : topic #1 (0.155): 0.014*\"papa\" + 0.012*\"monsieur\" + 0.011*\"de\" + 0.010*\"la\" + 0.007*\"revision\" + 0.007*\"pink\" + 0.006*\"herr\" + 0.006*\"marylin\" + 0.006*\"mozart\" + 0.005*\"ripley\"\n",
      "2019-05-10 17:30:35,802 : INFO : topic #0 (0.203): 0.075*\"harry\" + 0.051*\"nick\" + 0.043*\"scene\" + 0.018*\"day\" + 0.017*\"ext\" + 0.014*\"ms\" + 0.014*\"int\" + 0.014*\"dan\" + 0.013*\"frodo\" + 0.012*\"trench\"\n",
      "2019-05-10 17:30:35,805 : INFO : topic diff=0.026570, rho=0.093212\n",
      "2019-05-10 17:30:35,808 : INFO : PROGRESS: pass 9, dispatched chunk #29 = documents up to #30000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:36,332 : INFO : PROGRESS: pass 9, dispatched chunk #30 = documents up to #31000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:36,412 : INFO : PROGRESS: pass 9, dispatched chunk #31 = documents up to #32000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:36,772 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:36,839 : INFO : topic #9 (0.053): 0.034*\"b\" + 0.031*\"harvey\" + 0.026*\"g\" + 0.019*\"camera\" + 0.016*\"wendy\" + 0.015*\"kelly\" + 0.014*\"phillip\" + 0.013*\"wichita\" + 0.012*\"tao\" + 0.011*\"elwood\"\n",
      "2019-05-10 17:30:36,841 : INFO : topic #8 (0.058): 0.047*\"alex\" + 0.031*\"ace\" + 0.026*\"hector\" + 0.016*\"nantz\" + 0.012*\"kase\" + 0.010*\"sergeant\" + 0.007*\"melissa\" + 0.007*\"finkle\" + 0.005*\"marines\" + 0.005*\"harris\"\n",
      "2019-05-10 17:30:36,843 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:36,846 : INFO : topic #1 (0.155): 0.014*\"monsieur\" + 0.013*\"papa\" + 0.011*\"de\" + 0.010*\"la\" + 0.007*\"revision\" + 0.006*\"pink\" + 0.006*\"herr\" + 0.005*\"mozart\" + 0.005*\"ripley\" + 0.005*\"comrade\"\n",
      "2019-05-10 17:30:36,848 : INFO : topic #0 (0.203): 0.072*\"harry\" + 0.048*\"nick\" + 0.041*\"scene\" + 0.018*\"day\" + 0.016*\"dan\" + 0.016*\"ext\" + 0.014*\"ms\" + 0.014*\"int\" + 0.012*\"frodo\" + 0.012*\"trench\"\n",
      "2019-05-10 17:30:36,851 : INFO : topic diff=0.026553, rho=0.093212\n",
      "2019-05-10 17:30:36,854 : INFO : PROGRESS: pass 9, dispatched chunk #32 = documents up to #33000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:37,285 : INFO : PROGRESS: pass 9, dispatched chunk #33 = documents up to #34000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:37,437 : INFO : PROGRESS: pass 9, dispatched chunk #34 = documents up to #35000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:37,855 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:37,920 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.029*\"harvey\" + 0.025*\"g\" + 0.018*\"camera\" + 0.015*\"wendy\" + 0.015*\"kelly\" + 0.013*\"phillip\" + 0.012*\"wichita\" + 0.011*\"tao\" + 0.011*\"elwood\"\n",
      "2019-05-10 17:30:37,923 : INFO : topic #8 (0.058): 0.047*\"alex\" + 0.030*\"ace\" + 0.025*\"hector\" + 0.015*\"nantz\" + 0.011*\"kase\" + 0.010*\"sergeant\" + 0.007*\"harris\" + 0.007*\"melissa\" + 0.006*\"finkle\" + 0.005*\"marines\"\n",
      "2019-05-10 17:30:37,925 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:37,926 : INFO : topic #1 (0.155): 0.014*\"monsieur\" + 0.013*\"papa\" + 0.013*\"de\" + 0.010*\"la\" + 0.006*\"revision\" + 0.006*\"pink\" + 0.005*\"herr\" + 0.005*\"comrade\" + 0.005*\"mozart\" + 0.005*\"ripley\"\n",
      "2019-05-10 17:30:37,929 : INFO : topic #0 (0.203): 0.070*\"harry\" + 0.051*\"nick\" + 0.041*\"scene\" + 0.017*\"day\" + 0.015*\"ext\" + 0.015*\"dan\" + 0.014*\"ms\" + 0.013*\"int\" + 0.011*\"frodo\" + 0.011*\"trench\"\n",
      "2019-05-10 17:30:37,931 : INFO : topic diff=0.029854, rho=0.093212\n",
      "2019-05-10 17:30:37,934 : INFO : PROGRESS: pass 9, dispatched chunk #35 = documents up to #36000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:38,392 : INFO : PROGRESS: pass 9, dispatched chunk #36 = documents up to #37000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:38,546 : INFO : PROGRESS: pass 9, dispatched chunk #37 = documents up to #38000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:38,893 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:38,984 : INFO : topic #9 (0.053): 0.033*\"b\" + 0.026*\"harvey\" + 0.023*\"g\" + 0.021*\"phillip\" + 0.017*\"camera\" + 0.016*\"wendy\" + 0.014*\"kelly\" + 0.012*\"terrance\" + 0.011*\"wichita\" + 0.010*\"tao\"\n",
      "2019-05-10 17:30:38,987 : INFO : topic #8 (0.058): 0.044*\"alex\" + 0.029*\"ace\" + 0.023*\"hector\" + 0.014*\"nantz\" + 0.010*\"kase\" + 0.010*\"sergeant\" + 0.007*\"harris\" + 0.006*\"melissa\" + 0.006*\"finkle\" + 0.004*\"marines\"\n",
      "2019-05-10 17:30:38,990 : INFO : topic #2 (0.125): 0.078*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:38,992 : INFO : topic #1 (0.155): 0.012*\"monsieur\" + 0.012*\"papa\" + 0.011*\"de\" + 0.009*\"la\" + 0.009*\"jamal\" + 0.005*\"pink\" + 0.005*\"revision\" + 0.005*\"rupees\" + 0.005*\"herr\" + 0.005*\"comrade\"\n",
      "2019-05-10 17:30:38,994 : INFO : topic #0 (0.203): 0.078*\"harry\" + 0.049*\"nick\" + 0.040*\"scene\" + 0.017*\"day\" + 0.015*\"ext\" + 0.015*\"dan\" + 0.013*\"ms\" + 0.013*\"int\" + 0.011*\"frodo\" + 0.011*\"trench\"\n",
      "2019-05-10 17:30:38,996 : INFO : topic diff=0.037653, rho=0.093212\n",
      "2019-05-10 17:30:38,999 : INFO : PROGRESS: pass 9, dispatched chunk #38 = documents up to #39000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:39,472 : INFO : PROGRESS: pass 9, dispatched chunk #39 = documents up to #40000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:39,694 : INFO : PROGRESS: pass 9, dispatched chunk #40 = documents up to #41000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:39,960 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:40,055 : INFO : topic #9 (0.053): 0.032*\"b\" + 0.025*\"harvey\" + 0.023*\"g\" + 0.019*\"phillip\" + 0.016*\"camera\" + 0.015*\"wendy\" + 0.014*\"kelly\" + 0.012*\"terrance\" + 0.010*\"wichita\" + 0.010*\"u\"\n",
      "2019-05-10 17:30:40,061 : INFO : topic #8 (0.058): 0.043*\"alex\" + 0.030*\"ace\" + 0.021*\"hector\" + 0.013*\"nantz\" + 0.010*\"kase\" + 0.009*\"sergeant\" + 0.006*\"harris\" + 0.006*\"melissa\" + 0.005*\"finkle\" + 0.004*\"emilio\"\n",
      "2019-05-10 17:30:40,064 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"a\" + 0.032*\"and\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:40,071 : INFO : topic #1 (0.155): 0.012*\"de\" + 0.012*\"monsieur\" + 0.011*\"papa\" + 0.010*\"la\" + 0.009*\"dickie\" + 0.007*\"jamal\" + 0.006*\"ripley\" + 0.006*\"von\" + 0.005*\"der\" + 0.005*\"herr\"\n",
      "2019-05-10 17:30:40,073 : INFO : topic #0 (0.203): 0.074*\"harry\" + 0.049*\"nick\" + 0.038*\"scene\" + 0.016*\"day\" + 0.015*\"dan\" + 0.014*\"ext\" + 0.013*\"ms\" + 0.012*\"int\" + 0.010*\"trench\" + 0.010*\"frodo\"\n",
      "2019-05-10 17:30:40,076 : INFO : topic diff=0.032713, rho=0.093212\n",
      "2019-05-10 17:30:40,079 : INFO : PROGRESS: pass 9, dispatched chunk #41 = documents up to #42000/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:40,598 : INFO : PROGRESS: pass 9, dispatched chunk #42 = documents up to #42094/42094, outstanding queue size 9\n",
      "2019-05-10 17:30:41,051 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:41,114 : INFO : topic #9 (0.053): 0.032*\"b\" + 0.023*\"harvey\" + 0.023*\"g\" + 0.018*\"phillip\" + 0.015*\"camera\" + 0.015*\"wendy\" + 0.014*\"kelly\" + 0.013*\"jerry\" + 0.011*\"terrance\" + 0.010*\"wichita\"\n",
      "2019-05-10 17:30:41,117 : INFO : topic #8 (0.058): 0.041*\"alex\" + 0.027*\"ace\" + 0.019*\"hector\" + 0.012*\"nantz\" + 0.009*\"sergeant\" + 0.009*\"kase\" + 0.007*\"melissa\" + 0.007*\"wigand\" + 0.006*\"harris\" + 0.005*\"finkle\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:41,119 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"a\" + 0.032*\"and\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.013*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:41,123 : INFO : topic #1 (0.155): 0.014*\"de\" + 0.013*\"papa\" + 0.012*\"monsieur\" + 0.011*\"la\" + 0.008*\"dickie\" + 0.007*\"jamal\" + 0.006*\"ripley\" + 0.006*\"von\" + 0.005*\"ze\" + 0.005*\"der\"\n",
      "2019-05-10 17:30:41,142 : INFO : topic #0 (0.203): 0.073*\"harry\" + 0.046*\"nick\" + 0.037*\"scene\" + 0.016*\"frodo\" + 0.015*\"day\" + 0.014*\"dan\" + 0.013*\"ext\" + 0.012*\"ms\" + 0.011*\"int\" + 0.009*\"trench\"\n",
      "2019-05-10 17:30:41,145 : INFO : topic diff=0.024703, rho=0.093212\n",
      "2019-05-10 17:30:42,211 : INFO : merging changes from 3000 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:42,294 : INFO : topic #9 (0.053): 0.032*\"b\" + 0.022*\"g\" + 0.022*\"harvey\" + 0.017*\"phillip\" + 0.015*\"wendy\" + 0.015*\"camera\" + 0.014*\"kelly\" + 0.012*\"jerry\" + 0.010*\"terrance\" + 0.009*\"wichita\"\n",
      "2019-05-10 17:30:42,304 : INFO : topic #8 (0.058): 0.041*\"alex\" + 0.026*\"ace\" + 0.018*\"hector\" + 0.011*\"nantz\" + 0.009*\"sergeant\" + 0.008*\"kase\" + 0.007*\"melissa\" + 0.006*\"wigand\" + 0.006*\"harris\" + 0.004*\"finkle\"\n",
      "2019-05-10 17:30:42,307 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.032*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"in\" + 0.017*\"he\" + 0.014*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:42,310 : INFO : topic #1 (0.155): 0.016*\"papa\" + 0.012*\"de\" + 0.011*\"la\" + 0.011*\"monsieur\" + 0.009*\"scully\" + 0.008*\"rowan\" + 0.007*\"dickie\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.006*\"der\"\n",
      "2019-05-10 17:30:42,314 : INFO : topic #0 (0.203): 0.076*\"harry\" + 0.046*\"nick\" + 0.037*\"scene\" + 0.015*\"frodo\" + 0.015*\"day\" + 0.014*\"dan\" + 0.012*\"ms\" + 0.012*\"ext\" + 0.011*\"int\" + 0.009*\"trench\"\n",
      "2019-05-10 17:30:42,320 : INFO : topic diff=0.035344, rho=0.093212\n",
      "2019-05-10 17:30:44,078 : INFO : merging changes from 3094 documents into a model of 42094 documents\n",
      "2019-05-10 17:30:44,154 : INFO : topic #9 (0.053): 0.030*\"b\" + 0.021*\"g\" + 0.020*\"harvey\" + 0.017*\"kelly\" + 0.016*\"u\" + 0.016*\"phillip\" + 0.014*\"camera\" + 0.014*\"wendy\" + 0.013*\"madame\" + 0.012*\"jerry\"\n",
      "2019-05-10 17:30:44,158 : INFO : topic #8 (0.058): 0.057*\"alex\" + 0.023*\"ace\" + 0.020*\"hector\" + 0.010*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"kase\" + 0.006*\"melissa\" + 0.006*\"harris\" + 0.006*\"wigand\" + 0.004*\"finkle\"\n",
      "2019-05-10 17:30:44,160 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.033*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"he\" + 0.018*\"in\" + 0.014*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:44,164 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.012*\"de\" + 0.011*\"la\" + 0.010*\"monsieur\" + 0.009*\"scully\" + 0.007*\"rowan\" + 0.006*\"dickie\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.005*\"der\"\n",
      "2019-05-10 17:30:44,166 : INFO : topic #0 (0.203): 0.086*\"nick\" + 0.069*\"scene\" + 0.060*\"harry\" + 0.022*\"darcy\" + 0.021*\"dan\" + 0.021*\"ms\" + 0.013*\"day\" + 0.012*\"mcu\" + 0.011*\"frodo\" + 0.011*\"ext\"\n",
      "2019-05-10 17:30:44,169 : INFO : topic diff=0.035875, rho=0.093212\n",
      "2019-05-10 17:30:44,779 : INFO : saving LdaState object under lda_model.model.state, separately None\n",
      "2019-05-10 17:30:44,843 : INFO : saved lda_model.model.state\n",
      "2019-05-10 17:30:44,906 : INFO : saving LdaMulticore object under lda_model.model, separately ['expElogbeta', 'sstats']\n",
      "2019-05-10 17:30:44,908 : INFO : storing np array 'expElogbeta' to lda_model.model.expElogbeta.npy\n",
      "2019-05-10 17:30:44,917 : INFO : not storing attribute state\n",
      "2019-05-10 17:30:44,918 : INFO : not storing attribute id2word\n",
      "2019-05-10 17:30:44,919 : INFO : not storing attribute dispatcher\n",
      "2019-05-10 17:30:44,922 : INFO : saved lda_model.model\n",
      "2019-05-10 17:30:44,926 : INFO : topic #0 (0.203): 0.086*\"nick\" + 0.069*\"scene\" + 0.060*\"harry\" + 0.022*\"darcy\" + 0.021*\"dan\" + 0.021*\"ms\" + 0.013*\"day\" + 0.012*\"mcu\" + 0.011*\"frodo\" + 0.011*\"ext\"\n",
      "2019-05-10 17:30:44,928 : INFO : topic #1 (0.155): 0.015*\"papa\" + 0.012*\"de\" + 0.011*\"la\" + 0.010*\"monsieur\" + 0.009*\"scully\" + 0.007*\"rowan\" + 0.006*\"dickie\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.005*\"der\"\n",
      "2019-05-10 17:30:44,931 : INFO : topic #2 (0.125): 0.079*\"the\" + 0.033*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"he\" + 0.018*\"in\" + 0.014*\"is\" + 0.012*\"s\"\n",
      "2019-05-10 17:30:44,933 : INFO : topic #3 (0.104): 0.033*\"o\" + 0.033*\"ya\" + 0.032*\"n\" + 0.032*\"e\" + 0.026*\"y\" + 0.024*\"ha\" + 0.017*\"l\" + 0.016*\"r\" + 0.014*\"h\" + 0.013*\"ma\"\n",
      "2019-05-10 17:30:44,935 : INFO : topic #4 (0.090): 0.072*\"the\" + 0.028*\"of\" + 0.025*\"to\" + 0.021*\"a\" + 0.018*\"we\" + 0.017*\"and\" + 0.016*\"in\" + 0.015*\"is\" + 0.012*\"s\" + 0.011*\"it\"\n",
      "2019-05-10 17:30:44,938 : INFO : topic #5 (0.079): 0.071*\"huh\" + 0.056*\"dude\" + 0.022*\"uh\" + 0.012*\"hes\" + 0.009*\"theres\" + 0.009*\"desmond\" + 0.008*\"luthor\" + 0.007*\"lebowski\" + 0.006*\"whoa\" + 0.006*\"theyre\"\n",
      "2019-05-10 17:30:44,940 : INFO : topic #6 (0.070): 0.049*\"j\" + 0.007*\"ni\" + 0.007*\"sera\" + 0.004*\"bennie\" + 0.004*\"foley\" + 0.003*\"chick\" + 0.003*\"lama\" + 0.003*\"watts\" + 0.002*\"tito\" + 0.002*\"galvin\"\n",
      "2019-05-10 17:30:44,942 : INFO : topic #7 (0.063): 0.048*\"you\" + 0.045*\"i\" + 0.025*\"the\" + 0.024*\"to\" + 0.021*\"s\" + 0.021*\"a\" + 0.020*\"it\" + 0.017*\"t\" + 0.015*\"that\" + 0.013*\"and\"\n",
      "2019-05-10 17:30:44,945 : INFO : topic #8 (0.058): 0.057*\"alex\" + 0.023*\"ace\" + 0.020*\"hector\" + 0.010*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"kase\" + 0.006*\"melissa\" + 0.006*\"harris\" + 0.006*\"wigand\" + 0.004*\"finkle\"\n",
      "2019-05-10 17:30:44,947 : INFO : topic #9 (0.053): 0.030*\"b\" + 0.021*\"g\" + 0.020*\"harvey\" + 0.017*\"kelly\" + 0.016*\"u\" + 0.016*\"phillip\" + 0.014*\"camera\" + 0.014*\"wendy\" + 0.013*\"madame\" + 0.012*\"jerry\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.086*\"nick\" + 0.069*\"scene\" + 0.060*\"harry\" + 0.022*\"darcy\" + 0.021*\"dan\" + 0.021*\"ms\" + 0.013*\"day\" + 0.012*\"mcu\" + 0.011*\"frodo\" + 0.011*\"ext\"'),\n",
       " (1,\n",
       "  '0.015*\"papa\" + 0.012*\"de\" + 0.011*\"la\" + 0.010*\"monsieur\" + 0.009*\"scully\" + 0.007*\"rowan\" + 0.006*\"dickie\" + 0.006*\"jamal\" + 0.006*\"sie\" + 0.005*\"der\"'),\n",
       " (2,\n",
       "  '0.079*\"the\" + 0.033*\"and\" + 0.032*\"a\" + 0.023*\"of\" + 0.020*\"to\" + 0.018*\"his\" + 0.018*\"he\" + 0.018*\"in\" + 0.014*\"is\" + 0.012*\"s\"'),\n",
       " (3,\n",
       "  '0.033*\"o\" + 0.033*\"ya\" + 0.032*\"n\" + 0.032*\"e\" + 0.026*\"y\" + 0.024*\"ha\" + 0.017*\"l\" + 0.016*\"r\" + 0.014*\"h\" + 0.013*\"ma\"'),\n",
       " (4,\n",
       "  '0.072*\"the\" + 0.028*\"of\" + 0.025*\"to\" + 0.021*\"a\" + 0.018*\"we\" + 0.017*\"and\" + 0.016*\"in\" + 0.015*\"is\" + 0.012*\"s\" + 0.011*\"it\"'),\n",
       " (5,\n",
       "  '0.071*\"huh\" + 0.056*\"dude\" + 0.022*\"uh\" + 0.012*\"hes\" + 0.009*\"theres\" + 0.009*\"desmond\" + 0.008*\"luthor\" + 0.007*\"lebowski\" + 0.006*\"whoa\" + 0.006*\"theyre\"'),\n",
       " (6,\n",
       "  '0.049*\"j\" + 0.007*\"ni\" + 0.007*\"sera\" + 0.004*\"bennie\" + 0.004*\"foley\" + 0.003*\"chick\" + 0.003*\"lama\" + 0.003*\"watts\" + 0.002*\"tito\" + 0.002*\"galvin\"'),\n",
       " (7,\n",
       "  '0.048*\"you\" + 0.045*\"i\" + 0.025*\"the\" + 0.024*\"to\" + 0.021*\"s\" + 0.021*\"a\" + 0.020*\"it\" + 0.017*\"t\" + 0.015*\"that\" + 0.013*\"and\"'),\n",
       " (8,\n",
       "  '0.057*\"alex\" + 0.023*\"ace\" + 0.020*\"hector\" + 0.010*\"nantz\" + 0.009*\"sergeant\" + 0.007*\"kase\" + 0.006*\"melissa\" + 0.006*\"harris\" + 0.006*\"wigand\" + 0.004*\"finkle\"'),\n",
       " (9,\n",
       "  '0.030*\"b\" + 0.021*\"g\" + 0.020*\"harvey\" + 0.017*\"kelly\" + 0.016*\"u\" + 0.016*\"phillip\" + 0.014*\"camera\" + 0.014*\"wendy\" + 0.013*\"madame\" + 0.012*\"jerry\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Train the LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         random_state=100,\n",
    "                         num_topics=10,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "# See the topics\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 17:30:44,976 : INFO : using serial LSI version on this node\n",
      "2019-05-10 17:30:44,978 : INFO : updating model with new documents\n",
      "2019-05-10 17:30:44,980 : INFO : preparing a new chunk of documents\n",
      "2019-05-10 17:30:46,868 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-05-10 17:30:46,874 : INFO : 1st phase: constructing (89588, 110) action matrix\n",
      "2019-05-10 17:30:47,171 : INFO : orthonormalizing (89588, 110) action matrix\n",
      "2019-05-10 17:30:49,715 : INFO : 2nd phase: running dense svd on (110, 20000) matrix\n",
      "2019-05-10 17:30:50,065 : INFO : computing the final decomposition\n",
      "2019-05-10 17:30:50,066 : INFO : keeping 10 factors (discarding 5.409% of energy spectrum)\n",
      "2019-05-10 17:30:50,131 : INFO : processed documents up to #20000\n",
      "2019-05-10 17:30:50,146 : INFO : topic #0(8766.349): 0.448*\"i\" + 0.412*\"you\" + 0.353*\"the\" + 0.260*\"to\" + 0.232*\"a\" + 0.205*\"s\" + 0.203*\"it\" + 0.172*\"and\" + 0.156*\"t\" + 0.141*\"that\"\n",
      "2019-05-10 17:30:50,156 : INFO : topic #1(2052.778): -0.665*\"the\" + 0.418*\"i\" + 0.326*\"you\" + -0.220*\"of\" + -0.184*\"and\" + -0.149*\"a\" + -0.126*\"his\" + 0.124*\"t\" + -0.117*\"in\" + -0.102*\"he\"\n",
      "2019-05-10 17:30:50,159 : INFO : topic #2(1128.158): 0.628*\"i\" + -0.598*\"you\" + 0.177*\"and\" + 0.164*\"was\" + 0.151*\"my\" + -0.149*\"s\" + -0.122*\"re\" + -0.109*\"we\" + -0.108*\"what\" + -0.098*\"your\"\n",
      "2019-05-10 17:30:50,161 : INFO : topic #3(814.557): 0.509*\"s\" + -0.375*\"you\" + 0.274*\"it\" + -0.241*\"of\" + -0.201*\"and\" + -0.194*\"to\" + 0.172*\"t\" + 0.162*\"he\" + -0.153*\"my\" + -0.136*\"your\"\n",
      "2019-05-10 17:30:50,163 : INFO : topic #4(683.203): 0.515*\"we\" + -0.409*\"he\" + 0.267*\"it\" + -0.241*\"his\" + -0.233*\"you\" + -0.172*\"her\" + -0.171*\"she\" + 0.156*\"this\" + -0.156*\"him\" + 0.135*\"is\"\n",
      "2019-05-10 17:30:50,165 : INFO : preparing a new chunk of documents\n",
      "2019-05-10 17:30:50,586 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-05-10 17:30:50,593 : INFO : 1st phase: constructing (89588, 110) action matrix\n",
      "2019-05-10 17:30:50,800 : INFO : orthonormalizing (89588, 110) action matrix\n",
      "2019-05-10 17:30:53,813 : INFO : 2nd phase: running dense svd on (110, 20000) matrix\n",
      "2019-05-10 17:30:54,155 : INFO : computing the final decomposition\n",
      "2019-05-10 17:30:54,156 : INFO : keeping 10 factors (discarding 5.384% of energy spectrum)\n",
      "2019-05-10 17:30:54,194 : INFO : merging projections: (89588, 10) + (89588, 10)\n",
      "2019-05-10 17:30:54,226 : INFO : keeping 10 factors (discarding 0.140% of energy spectrum)\n",
      "2019-05-10 17:30:54,239 : INFO : processed documents up to #40000\n",
      "2019-05-10 17:30:54,244 : INFO : topic #0(9411.275): 0.445*\"i\" + 0.422*\"you\" + 0.343*\"the\" + 0.261*\"to\" + 0.228*\"a\" + 0.205*\"s\" + 0.204*\"it\" + 0.167*\"and\" + 0.163*\"t\" + 0.142*\"that\"\n",
      "2019-05-10 17:30:54,247 : INFO : topic #1(2027.650): 0.674*\"the\" + -0.477*\"i\" + -0.275*\"you\" + 0.192*\"of\" + 0.179*\"and\" + 0.127*\"a\" + -0.123*\"t\" + 0.105*\"in\" + 0.103*\"his\" + 0.101*\"to\"\n",
      "2019-05-10 17:30:54,250 : INFO : topic #2(1194.090): -0.668*\"you\" + 0.612*\"i\" + -0.139*\"we\" + 0.117*\"and\" + -0.116*\"s\" + 0.115*\"the\" + -0.115*\"re\" + 0.113*\"my\" + -0.113*\"your\" + 0.111*\"was\"\n",
      "2019-05-10 17:30:54,253 : INFO : topic #3(933.805): 0.519*\"s\" + -0.328*\"you\" + 0.312*\"it\" + 0.250*\"t\" + -0.214*\"to\" + -0.206*\"of\" + 0.186*\"we\" + 0.156*\"he\" + -0.144*\"your\" + -0.130*\"is\"\n",
      "2019-05-10 17:30:54,256 : INFO : topic #4(711.478): -0.511*\"we\" + -0.346*\"to\" + 0.343*\"a\" + -0.219*\"it\" + 0.209*\"you\" + 0.207*\"he\" + 0.190*\"and\" + -0.164*\"have\" + -0.163*\"is\" + 0.131*\"his\"\n",
      "2019-05-10 17:30:54,259 : INFO : preparing a new chunk of documents\n",
      "2019-05-10 17:30:54,337 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-05-10 17:30:54,350 : INFO : 1st phase: constructing (89588, 110) action matrix\n",
      "2019-05-10 17:30:54,389 : INFO : orthonormalizing (89588, 110) action matrix\n",
      "2019-05-10 17:30:56,793 : INFO : 2nd phase: running dense svd on (110, 2094) matrix\n",
      "2019-05-10 17:30:56,849 : INFO : computing the final decomposition\n",
      "2019-05-10 17:30:56,849 : INFO : keeping 10 factors (discarding 6.082% of energy spectrum)\n",
      "2019-05-10 17:30:56,883 : INFO : merging projections: (89588, 10) + (89588, 10)\n",
      "2019-05-10 17:30:56,917 : INFO : keeping 10 factors (discarding 0.426% of energy spectrum)\n",
      "2019-05-10 17:30:56,932 : INFO : processed documents up to #42094\n",
      "2019-05-10 17:30:56,936 : INFO : topic #0(5609.670): 0.445*\"i\" + 0.425*\"you\" + 0.343*\"the\" + 0.261*\"to\" + 0.230*\"a\" + 0.204*\"s\" + 0.201*\"it\" + 0.165*\"and\" + 0.160*\"t\" + 0.141*\"that\"\n",
      "2019-05-10 17:30:56,941 : INFO : topic #1(1222.705): 0.676*\"the\" + -0.480*\"i\" + -0.266*\"you\" + 0.185*\"and\" + 0.181*\"of\" + 0.129*\"a\" + -0.129*\"t\" + 0.121*\"his\" + 0.115*\"he\" + 0.107*\"in\"\n",
      "2019-05-10 17:30:56,945 : INFO : topic #2(686.721): -0.670*\"you\" + 0.607*\"i\" + -0.132*\"we\" + 0.121*\"and\" + -0.117*\"re\" + 0.116*\"the\" + 0.115*\"my\" + 0.108*\"was\" + -0.106*\"s\" + -0.105*\"your\"\n",
      "2019-05-10 17:30:56,947 : INFO : topic #3(531.801): 0.550*\"s\" + -0.317*\"you\" + 0.265*\"it\" + 0.261*\"t\" + -0.209*\"to\" + -0.203*\"of\" + 0.188*\"we\" + 0.175*\"he\" + -0.146*\"and\" + -0.134*\"is\"\n",
      "2019-05-10 17:30:56,950 : INFO : topic #4(422.327): -0.513*\"we\" + 0.262*\"you\" + 0.260*\"he\" + -0.252*\"to\" + -0.212*\"is\" + -0.209*\"it\" + 0.194*\"his\" + -0.171*\"have\" + 0.168*\"and\" + 0.160*\"her\"\n",
      "2019-05-10 17:30:56,953 : INFO : topic #0(5609.670): 0.445*\"i\" + 0.425*\"you\" + 0.343*\"the\" + 0.261*\"to\" + 0.230*\"a\" + 0.204*\"s\" + 0.201*\"it\" + 0.165*\"and\" + 0.160*\"t\" + 0.141*\"that\"\n",
      "2019-05-10 17:30:56,955 : INFO : topic #1(1222.705): 0.676*\"the\" + -0.480*\"i\" + -0.266*\"you\" + 0.185*\"and\" + 0.181*\"of\" + 0.129*\"a\" + -0.129*\"t\" + 0.121*\"his\" + 0.115*\"he\" + 0.107*\"in\"\n",
      "2019-05-10 17:30:56,960 : INFO : topic #2(686.721): -0.670*\"you\" + 0.607*\"i\" + -0.132*\"we\" + 0.121*\"and\" + -0.117*\"re\" + 0.116*\"the\" + 0.115*\"my\" + 0.108*\"was\" + -0.106*\"s\" + -0.105*\"your\"\n",
      "2019-05-10 17:30:56,963 : INFO : topic #3(531.801): 0.550*\"s\" + -0.317*\"you\" + 0.265*\"it\" + 0.261*\"t\" + -0.209*\"to\" + -0.203*\"of\" + 0.188*\"we\" + 0.175*\"he\" + -0.146*\"and\" + -0.134*\"is\"\n",
      "2019-05-10 17:30:56,966 : INFO : topic #4(422.327): -0.513*\"we\" + 0.262*\"you\" + 0.260*\"he\" + -0.252*\"to\" + -0.212*\"is\" + -0.209*\"it\" + 0.194*\"his\" + -0.171*\"have\" + 0.168*\"and\" + 0.160*\"her\"\n",
      "2019-05-10 17:30:56,968 : INFO : topic #5(390.285): 0.610*\"a\" + -0.275*\"he\" + -0.269*\"the\" + -0.235*\"t\" + 0.230*\"and\" + -0.209*\"to\" + 0.193*\"that\" + 0.145*\"s\" + -0.129*\"him\" + 0.118*\"of\"\n",
      "2019-05-10 17:30:56,971 : INFO : topic #6(375.909): -0.461*\"the\" + 0.450*\"to\" + 0.325*\"and\" + 0.307*\"he\" + 0.257*\"she\" + 0.253*\"her\" + -0.182*\"i\" + 0.136*\"him\" + 0.112*\"his\" + -0.108*\"m\"\n",
      "2019-05-10 17:30:56,974 : INFO : topic #7(313.414): 0.414*\"he\" + -0.348*\"she\" + -0.322*\"her\" + 0.271*\"a\" + -0.239*\"s\" + -0.211*\"it\" + 0.201*\"him\" + 0.172*\"t\" + -0.170*\"and\" + 0.167*\"in\"\n",
      "2019-05-10 17:30:56,977 : INFO : topic #8(293.369): -0.412*\"t\" + -0.395*\"and\" + 0.372*\"he\" + 0.305*\"is\" + 0.292*\"s\" + -0.201*\"we\" + -0.188*\"don\" + 0.128*\"a\" + 0.124*\"my\" + 0.116*\"what\"\n",
      "2019-05-10 17:30:56,979 : INFO : topic #9(270.478): 0.505*\"it\" + -0.331*\"and\" + -0.318*\"we\" + 0.248*\"t\" + 0.222*\"of\" + 0.218*\"was\" + 0.194*\"a\" + -0.175*\"m\" + -0.124*\"re\" + -0.121*\"i\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.445*\"i\" + 0.425*\"you\" + 0.343*\"the\" + 0.261*\"to\" + 0.230*\"a\" + 0.204*\"s\" '\n",
      "  '+ 0.201*\"it\" + 0.165*\"and\" + 0.160*\"t\" + 0.141*\"that\"'),\n",
      " (1,\n",
      "  '0.676*\"the\" + -0.480*\"i\" + -0.266*\"you\" + 0.185*\"and\" + 0.181*\"of\" + '\n",
      "  '0.129*\"a\" + -0.129*\"t\" + 0.121*\"his\" + 0.115*\"he\" + 0.107*\"in\"'),\n",
      " (2,\n",
      "  '-0.670*\"you\" + 0.607*\"i\" + -0.132*\"we\" + 0.121*\"and\" + -0.117*\"re\" + '\n",
      "  '0.116*\"the\" + 0.115*\"my\" + 0.108*\"was\" + -0.106*\"s\" + -0.105*\"your\"'),\n",
      " (3,\n",
      "  '0.550*\"s\" + -0.317*\"you\" + 0.265*\"it\" + 0.261*\"t\" + -0.209*\"to\" + '\n",
      "  '-0.203*\"of\" + 0.188*\"we\" + 0.175*\"he\" + -0.146*\"and\" + -0.134*\"is\"'),\n",
      " (4,\n",
      "  '-0.513*\"we\" + 0.262*\"you\" + 0.260*\"he\" + -0.252*\"to\" + -0.212*\"is\" + '\n",
      "  '-0.209*\"it\" + 0.194*\"his\" + -0.171*\"have\" + 0.168*\"and\" + 0.160*\"her\"'),\n",
      " (5,\n",
      "  '0.610*\"a\" + -0.275*\"he\" + -0.269*\"the\" + -0.235*\"t\" + 0.230*\"and\" + '\n",
      "  '-0.209*\"to\" + 0.193*\"that\" + 0.145*\"s\" + -0.129*\"him\" + 0.118*\"of\"'),\n",
      " (6,\n",
      "  '-0.461*\"the\" + 0.450*\"to\" + 0.325*\"and\" + 0.307*\"he\" + 0.257*\"she\" + '\n",
      "  '0.253*\"her\" + -0.182*\"i\" + 0.136*\"him\" + 0.112*\"his\" + -0.108*\"m\"'),\n",
      " (7,\n",
      "  '0.414*\"he\" + -0.348*\"she\" + -0.322*\"her\" + 0.271*\"a\" + -0.239*\"s\" + '\n",
      "  '-0.211*\"it\" + 0.201*\"him\" + 0.172*\"t\" + -0.170*\"and\" + 0.167*\"in\"'),\n",
      " (8,\n",
      "  '-0.412*\"t\" + -0.395*\"and\" + 0.372*\"he\" + 0.305*\"is\" + 0.292*\"s\" + '\n",
      "  '-0.201*\"we\" + -0.188*\"don\" + 0.128*\"a\" + 0.124*\"my\" + 0.116*\"what\"'),\n",
      " (9,\n",
      "  '0.505*\"it\" + -0.331*\"and\" + -0.318*\"we\" + 0.248*\"t\" + 0.222*\"of\" + '\n",
      "  '0.218*\"was\" + 0.194*\"a\" + -0.175*\"m\" + -0.124*\"re\" + -0.121*\"i\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "# Build the LSI Model\n",
    "lsi_model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=10, decay=0.5)\n",
    "\n",
    "# View Topics\n",
    "pprint(lsi_model.print_topics(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOUGLAS' SVD METHOD\n",
    "###https://github.com/rexchang0424/Game-Recommender-Model/blob/master/02_game_recommend_EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined = pd.read_pickle(\"./data/combined_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10666348"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.word_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['word_count'] = combined['token_words'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42094"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23182"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# characters with over 50 words in their quotes/dialogues\n",
    "combined = combined[combined['word_count'] > 50]\n",
    "combined.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# 23182 characters\n",
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# WORD FREQUENCY ESSENTIALLY. DOES NOT PRESERVE ANY WORD ORDER\n",
    "tiffydiff = TfidfVectorizer(stop_words=stop_wordss, min_df=20, max_df=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['lines_clean'] = combined['lines_clean'].map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x)) # Only keeping letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "line_tiffy = tiffydiff.fit_transform(combined['lines_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23182, 12826)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 23182 characters\n",
    "# 12826s unique words\n",
    "line_tiffy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_tiffy = pd.SparseDataFrame(line_tiffy,columns=tiffydiff.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_tiffy.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23182, 2000)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "SVD = TruncatedSVD(n_components=2000)\n",
    "svd_matrix = SVD.fit_transform(line_tiffy)\n",
    "svd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/svd_matrix', 'wb') as fp:\n",
    "    pickle.dump(svd_matrix, fp)\n",
    "    \n",
    "with open('./data/svd_matrix', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.merge(line_tiffy, combined,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_explained_variance(svd_object, n_components):\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    plt.bar(np.array(range(n_components))+1, \n",
    "            svd_object.explained_variance_ratio_, \n",
    "            color='g', \n",
    "            label='explained variance')\n",
    "    plt.plot(np.array(range(n_components))+1, \n",
    "             np.cumsum(svd_object.explained_variance_ratio_), \n",
    "             label='cumulative explained variance')\n",
    "    plt.legend()\n",
    "    plt.xlabel('component')\n",
    "    plt.ylabel('variance ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfig = plt.figure(figsize=(20,10))\\nplt.bar(np.array(range(100))+1, \\n        SVD.explained_variance_ratio_, \\n        color='g', \\n        label='explained variance')\\nplt.plot(np.array(range(100))+1, \\n         np.cumsum(SVD.explained_variance_ratio_), \\n         label='cumulative explained variance')\\nplt.legend(fontsize=16);\\nplt.xlabel('component', fontsize=20);\\nplt.ylabel('variance ratio', fontsize=20);\\nplt.title('Explained variance by component', fontsize=36);\\n\""
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sm flex week 5\n",
    "'''\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.bar(np.array(range(100))+1, \n",
    "        SVD.explained_variance_ratio_, \n",
    "        color='g', \n",
    "        label='explained variance')\n",
    "plt.plot(np.array(range(100))+1, \n",
    "         np.cumsum(SVD.explained_variance_ratio_), \n",
    "         label='cumulative explained variance')\n",
    "plt.legend(fontsize=16);\n",
    "plt.xlabel('component', fontsize=20);\n",
    "plt.ylabel('variance ratio', fontsize=20);\n",
    "plt.title('Explained variance by component', fontsize=36);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_explained_variance(SVD, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23182, 23182)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_mat = cosine_similarity(svd_matrix, svd_matrix)\n",
    "cos_sim_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_char = dict(zip(combined.index, combined['title']))\n",
    "#combine_char = combined['title'] basically the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test\n",
    "game_index_title  = dict(zip(range(len(svd_matrix)), combined.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "#game_search_dict = dict(zip([combine_char[combine_char[x]] for x in combine_char], combine_char))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23182"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13789"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined.character.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = dict(combined['character'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23182"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cos_sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23182"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>character</th>\n",
       "      <th>lines</th>\n",
       "      <th>token_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>vader</th>\n",
       "      <th>test_token</th>\n",
       "      <th>lines_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>bianca</td>\n",
       "      <td>Did you change your hair? You might wanna thin...</td>\n",
       "      <td>[did, you, change, your, hair, you, might, wan...</td>\n",
       "      <td>1295</td>\n",
       "      <td>{'neg': 0.108, 'neu': 0.726, 'pos': 0.166, 'co...</td>\n",
       "      <td>did you change your hair you might wanna think...</td>\n",
       "      <td>did you change your hair you might wanna think...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title character  \\\n",
       "0  10 things i hate about you    bianca   \n",
       "\n",
       "                                               lines  \\\n",
       "0  Did you change your hair? You might wanna thin...   \n",
       "\n",
       "                                         token_words  word_count  \\\n",
       "0  [did, you, change, your, hair, you, might, wan...        1295   \n",
       "\n",
       "                                               vader  \\\n",
       "0  {'neg': 0.108, 'neu': 0.726, 'pos': 0.166, 'co...   \n",
       "\n",
       "                                          test_token  \\\n",
       "0  did you change your hair you might wanna think...   \n",
       "\n",
       "                                         lines_clean  \n",
       "0  did you change your hair you might wanna think...  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECOMMENDER FOR BIANCA, THEN MAKE A FUNCTION\n",
    "combined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([18384, 9035, 8969, 6, 7153, 15208, 16079, 1674, 3745, 5931], dtype='int64')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get bianca's index\n",
    "#combined[combined['character'] == 'bianca']['title'].index\n",
    "# for now it's just 0... there are multiple tho so watch out...\n",
    "\n",
    "#cos_sim_mat[1][:10]\n",
    "\n",
    "# go to bianca's similarities with others\n",
    "character_similar_bianca = pd.Series(cos_sim_mat[0])\n",
    "\n",
    "# grabbing top ten related characters (NOT COUNTING THEIR OWN)\n",
    "char_ind = character_similar_bianca.sort_values(ascending=False)[1:11].index\n",
    "\n",
    "char_ind\n",
    "#for character in char_ind:\n",
    "#    print(combined.at[character,'character'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>title</th>\n",
       "      <th>lines</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18384</th>\n",
       "      <td>grace</td>\n",
       "      <td>the horse whisperer</td>\n",
       "      <td>Yeah, fly boy... that's it... go... go... go.....</td>\n",
       "      <td>0.529341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9035</th>\n",
       "      <td>matt</td>\n",
       "      <td>i'll do anything</td>\n",
       "      <td>Good idea, let's not. We'll know all we need t...</td>\n",
       "      <td>0.525214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969</th>\n",
       "      <td>lucy</td>\n",
       "      <td>i am sam</td>\n",
       "      <td>Annie, Annie, Annie... Daddy, where does the s...</td>\n",
       "      <td>0.521713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kat</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>Leave it Why didn't we just read the Hardy Boy...</td>\n",
       "      <td>0.517613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7153</th>\n",
       "      <td>enid</td>\n",
       "      <td>ghost world</td>\n",
       "      <td>God, what a bunch of retards... I know, I like...</td>\n",
       "      <td>0.503196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15208</th>\n",
       "      <td>erica</td>\n",
       "      <td>something's gotta give</td>\n",
       "      <td>Okay, stay right where you are. We have a knif...</td>\n",
       "      <td>0.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16079</th>\n",
       "      <td>jackie</td>\n",
       "      <td>stepmom</td>\n",
       "      <td>...it's really not so bad Annabelle -- Red and...</td>\n",
       "      <td>0.498483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>carol</td>\n",
       "      <td>as good as it gets</td>\n",
       "      <td>Look at you, you're all better. You know all m...</td>\n",
       "      <td>0.488263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>jody</td>\n",
       "      <td>cherry falls</td>\n",
       "      <td>Cut it out. I was supposed to be home fifteen ...</td>\n",
       "      <td>0.488003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>joel</td>\n",
       "      <td>eternal sunshine of the spotless mind</td>\n",
       "      <td>Hi, Cindy. Joel. Listen, I'm not feeling well ...</td>\n",
       "      <td>0.484766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      character                                  title  \\\n",
       "18384     grace                    the horse whisperer   \n",
       "9035       matt                       i'll do anything   \n",
       "8969       lucy                               i am sam   \n",
       "6           kat             10 things i hate about you   \n",
       "7153       enid                            ghost world   \n",
       "15208     erica                 something's gotta give   \n",
       "16079    jackie                                stepmom   \n",
       "1674      carol                     as good as it gets   \n",
       "3745       jody                           cherry falls   \n",
       "5931       joel  eternal sunshine of the spotless mind   \n",
       "\n",
       "                                                   lines  similarity_score  \n",
       "18384  Yeah, fly boy... that's it... go... go... go.....          0.529341  \n",
       "9035   Good idea, let's not. We'll know all we need t...          0.525214  \n",
       "8969   Annie, Annie, Annie... Daddy, where does the s...          0.521713  \n",
       "6      Leave it Why didn't we just read the Hardy Boy...          0.517613  \n",
       "7153   God, what a bunch of retards... I know, I like...          0.503196  \n",
       "15208  Okay, stay right where you are. We have a knif...          0.500162  \n",
       "16079  ...it's really not so bad Annabelle -- Red and...          0.498483  \n",
       "1674   Look at you, you're all better. You know all m...          0.488263  \n",
       "3745   Cut it out. I was supposed to be home fifteen ...          0.488003  \n",
       "5931   Hi, Cindy. Joel. Listen, I'm not feeling well ...          0.484766  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a similarity dataframe\n",
    "similar_df = combined.loc[char_ind,['character','title','lines']]\n",
    "\n",
    "# grabbing the ssimilarity scores\n",
    "similar_df['similarity_score'] = character_similar_bianca.sort_values(ascending=False)[1:11].values\n",
    "similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_info = pd.read_csv('./data/movie_info_final.csv')\n",
    "#movie_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_df = similar_df.merge(movie_info, \n",
    "                              how='inner', \n",
    "                              left_on ='title', \n",
    "                              right_on = 'original_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>title</th>\n",
       "      <th>lines</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>original_title</th>\n",
       "      <th>imdb_title</th>\n",
       "      <th>imdb_url</th>\n",
       "      <th>genre</th>\n",
       "      <th>pic_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grace</td>\n",
       "      <td>the horse whisperer</td>\n",
       "      <td>Yeah, fly boy... that's it... go... go... go.....</td>\n",
       "      <td>0.529341</td>\n",
       "      <td>the horse whisperer</td>\n",
       "      <td>The Horse Whisperer (1998)</td>\n",
       "      <td>https://www.imdb.com/title/tt0119314/</td>\n",
       "      <td>Drama,Romance,Western</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BZTA5Nz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>matt</td>\n",
       "      <td>i'll do anything</td>\n",
       "      <td>Good idea, let's not. We'll know all we need t...</td>\n",
       "      <td>0.525214</td>\n",
       "      <td>i'll do anything</td>\n",
       "      <td>I'll Do Anything (1994)</td>\n",
       "      <td>https://www.imdb.com/title/tt0110097/</td>\n",
       "      <td>Comedy,Drama</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BYmZlM2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lucy</td>\n",
       "      <td>i am sam</td>\n",
       "      <td>Annie, Annie, Annie... Daddy, where does the s...</td>\n",
       "      <td>0.521713</td>\n",
       "      <td>i am sam</td>\n",
       "      <td>I Am Sam (2001)</td>\n",
       "      <td>https://www.imdb.com/title/tt0277027/</td>\n",
       "      <td>Drama</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BYzEyNz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kat</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>Leave it Why didn't we just read the Hardy Boy...</td>\n",
       "      <td>0.517613</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>10 Things I Hate About You (1999)</td>\n",
       "      <td>https://www.imdb.com/title/tt0147800/</td>\n",
       "      <td>Comedy,Drama,Romance</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMmVhZj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enid</td>\n",
       "      <td>ghost world</td>\n",
       "      <td>God, what a bunch of retards... I know, I like...</td>\n",
       "      <td>0.503196</td>\n",
       "      <td>ghost world</td>\n",
       "      <td>Ghost World (2001)</td>\n",
       "      <td>https://www.imdb.com/title/tt0162346/</td>\n",
       "      <td>Comedy,Drama</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BNWYwOD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>erica</td>\n",
       "      <td>something's gotta give</td>\n",
       "      <td>Okay, stay right where you are. We have a knif...</td>\n",
       "      <td>0.500162</td>\n",
       "      <td>something's gotta give</td>\n",
       "      <td>Something's Gotta Give (2003)</td>\n",
       "      <td>https://www.imdb.com/title/tt0337741/</td>\n",
       "      <td>Comedy,Drama,Romance</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BODYyNm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jackie</td>\n",
       "      <td>stepmom</td>\n",
       "      <td>...it's really not so bad Annabelle -- Red and...</td>\n",
       "      <td>0.498483</td>\n",
       "      <td>stepmom</td>\n",
       "      <td>Stepmom (1998)</td>\n",
       "      <td>https://www.imdb.com/title/tt0120686/</td>\n",
       "      <td>Comedy,Drama</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMGJlOW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>carol</td>\n",
       "      <td>as good as it gets</td>\n",
       "      <td>Look at you, you're all better. You know all m...</td>\n",
       "      <td>0.488263</td>\n",
       "      <td>as good as it gets</td>\n",
       "      <td>As Good as It Gets (1997)</td>\n",
       "      <td>https://www.imdb.com/title/tt0119822/</td>\n",
       "      <td>Comedy,Drama,Romance</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BNWMxZT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jody</td>\n",
       "      <td>cherry falls</td>\n",
       "      <td>Cut it out. I was supposed to be home fifteen ...</td>\n",
       "      <td>0.488003</td>\n",
       "      <td>cherry falls</td>\n",
       "      <td>Cherry Falls (2000)</td>\n",
       "      <td>https://www.imdb.com/title/tt0175526/</td>\n",
       "      <td>Horror,Mystery,Thriller</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BZGU4Yz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>joel</td>\n",
       "      <td>eternal sunshine of the spotless mind</td>\n",
       "      <td>Hi, Cindy. Joel. Listen, I'm not feeling well ...</td>\n",
       "      <td>0.484766</td>\n",
       "      <td>eternal sunshine of the spotless mind</td>\n",
       "      <td>Eternal Sunshine of the Spotless Mind (2004)</td>\n",
       "      <td>https://www.imdb.com/title/tt0338013/</td>\n",
       "      <td>Drama,Romance,Sci-Fi</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMTY4Nz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character                                  title  \\\n",
       "0     grace                    the horse whisperer   \n",
       "1      matt                       i'll do anything   \n",
       "2      lucy                               i am sam   \n",
       "3       kat             10 things i hate about you   \n",
       "4      enid                            ghost world   \n",
       "5     erica                 something's gotta give   \n",
       "6    jackie                                stepmom   \n",
       "7     carol                     as good as it gets   \n",
       "8      jody                           cherry falls   \n",
       "9      joel  eternal sunshine of the spotless mind   \n",
       "\n",
       "                                               lines  similarity_score  \\\n",
       "0  Yeah, fly boy... that's it... go... go... go.....          0.529341   \n",
       "1  Good idea, let's not. We'll know all we need t...          0.525214   \n",
       "2  Annie, Annie, Annie... Daddy, where does the s...          0.521713   \n",
       "3  Leave it Why didn't we just read the Hardy Boy...          0.517613   \n",
       "4  God, what a bunch of retards... I know, I like...          0.503196   \n",
       "5  Okay, stay right where you are. We have a knif...          0.500162   \n",
       "6  ...it's really not so bad Annabelle -- Red and...          0.498483   \n",
       "7  Look at you, you're all better. You know all m...          0.488263   \n",
       "8  Cut it out. I was supposed to be home fifteen ...          0.488003   \n",
       "9  Hi, Cindy. Joel. Listen, I'm not feeling well ...          0.484766   \n",
       "\n",
       "                          original_title  \\\n",
       "0                    the horse whisperer   \n",
       "1                       i'll do anything   \n",
       "2                               i am sam   \n",
       "3             10 things i hate about you   \n",
       "4                            ghost world   \n",
       "5                 something's gotta give   \n",
       "6                                stepmom   \n",
       "7                     as good as it gets   \n",
       "8                           cherry falls   \n",
       "9  eternal sunshine of the spotless mind   \n",
       "\n",
       "                                     imdb_title  \\\n",
       "0                    The Horse Whisperer (1998)   \n",
       "1                       I'll Do Anything (1994)   \n",
       "2                               I Am Sam (2001)   \n",
       "3             10 Things I Hate About You (1999)   \n",
       "4                            Ghost World (2001)   \n",
       "5                 Something's Gotta Give (2003)   \n",
       "6                                Stepmom (1998)   \n",
       "7                     As Good as It Gets (1997)   \n",
       "8                           Cherry Falls (2000)   \n",
       "9  Eternal Sunshine of the Spotless Mind (2004)   \n",
       "\n",
       "                                imdb_url                    genre  \\\n",
       "0  https://www.imdb.com/title/tt0119314/    Drama,Romance,Western   \n",
       "1  https://www.imdb.com/title/tt0110097/             Comedy,Drama   \n",
       "2  https://www.imdb.com/title/tt0277027/                    Drama   \n",
       "3  https://www.imdb.com/title/tt0147800/     Comedy,Drama,Romance   \n",
       "4  https://www.imdb.com/title/tt0162346/             Comedy,Drama   \n",
       "5  https://www.imdb.com/title/tt0337741/     Comedy,Drama,Romance   \n",
       "6  https://www.imdb.com/title/tt0120686/             Comedy,Drama   \n",
       "7  https://www.imdb.com/title/tt0119822/     Comedy,Drama,Romance   \n",
       "8  https://www.imdb.com/title/tt0175526/  Horror,Mystery,Thriller   \n",
       "9  https://www.imdb.com/title/tt0338013/     Drama,Romance,Sci-Fi   \n",
       "\n",
       "                                             pic_url  \n",
       "0  https://m.media-amazon.com/images/M/MV5BZTA5Nz...  \n",
       "1  https://m.media-amazon.com/images/M/MV5BYmZlM2...  \n",
       "2  https://m.media-amazon.com/images/M/MV5BYzEyNz...  \n",
       "3  https://m.media-amazon.com/images/M/MV5BMmVhZj...  \n",
       "4  https://m.media-amazon.com/images/M/MV5BNWYwOD...  \n",
       "5  https://m.media-amazon.com/images/M/MV5BODYyNm...  \n",
       "6  https://m.media-amazon.com/images/M/MV5BMGJlOW...  \n",
       "7  https://m.media-amazon.com/images/M/MV5BNWMxZT...  \n",
       "8  https://m.media-amazon.com/images/M/MV5BZGU4Yz...  \n",
       "9  https://m.media-amazon.com/images/M/MV5BMTY4Nz...  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original_title    0\n",
       "imdb_title        0\n",
       "imdb_url          0\n",
       "genre             1\n",
       "pic_url           5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_info.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], tags=['0']),\n",
       " TaggedDocument(words=['i', 'love', 'coding', 'in', 'python'], tags=['1']),\n",
       " TaggedDocument(words=['i', 'love', 'building', 'chatbots'], tags=['2']),\n",
       " TaggedDocument(words=['they', 'chat', 'amagingly', 'well'], tags=['3'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 5\n",
      "iteration 10\n",
      "iteration 15\n",
      "iteration 20\n",
      "iteration 25\n",
      "iteration 30\n",
      "iteration 35\n",
      "iteration 40\n",
      "iteration 45\n",
      "iteration 50\n",
      "iteration 55\n",
      "iteration 60\n",
      "iteration 65\n",
      "iteration 70\n",
      "iteration 75\n",
      "iteration 80\n",
      "iteration 85\n",
      "iteration 90\n",
      "iteration 95\n",
      "Model done\n"
     ]
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = [\"I love machine learning. Its awesome.\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if epoch % 5 == 0:\n",
    "        print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "print(\"Model done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 0.9951425790786743), ('2', 0.9879583716392517), ('3', 0.9813176989555359)]\n"
     ]
    }
   ],
   "source": [
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
